{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune MedSigLIP for Surgical Wound Assessment — Run 2\n",
    "\n",
    "**MamaGuard — Wound Assessment Component (v2: Expanded Unfreeze + Differential LR)**\n",
    "\n",
    "Fine-tunes `google/medsiglip-448` vision encoder on the [SurgWound dataset](https://huggingface.co/datasets/xuxuxuxuxu/SurgWound)\n",
    "for **6 binary clinical labels**: healing status, erythema, edema, infection risk, urgency, and exudate.\n",
    "\n",
    "The fine-tuned model serves as a structured clinical signal provider in a two-stage pipeline:\n",
    "```\n",
    "Wound photo → MedSigLIP (this model) → structured scores → Gemini/MedGemma orchestrator → empathetic response\n",
    "```\n",
    "\n",
    "### Changes from Run 1 → Run 2\n",
    "Run 1 (40 total optimizer steps, N_UNFREEZE=4) showed clear underfitting: inference scores clustered 0.45–0.59 and the model was still improving at epoch 5 with no plateau.\n",
    "\n",
    "| Parameter | Run 1 | Run 2 | Effect |\n",
    "|---|---|---|---|\n",
    "| `N_UNFREEZE` | 4 | **8** | ~2× trainable capacity (~14% → ~28%) |\n",
    "| `GRAD_ACCUM` | 16 | **4** | 8 → 30 optimizer steps/epoch |\n",
    "| `EPOCHS` | 5 | **10** | 40 → **300** total optimizer steps (7.5×) |\n",
    "| Learning rate | single 5e-5 | **differential** backbone=1.5e-5 / head=8e-5 | Preserves pretrained features, fast head convergence |\n",
    "| Threshold | fixed 0.5 | **per-label (Youden's J)** | Corrects miscalibration (e.g., healing sens=0.84/spec=0.26) |\n",
    "\n",
    "### Key Design Decisions\n",
    "- **Expanded selective freezing**: Last **8** encoder blocks + classification head are trainable (~28% of params); deeper layers give model more expressive capacity without saturating T4 VRAM\n",
    "- **Differential learning rate**: Backbone blocks update at `BACKBONE_LR=1.5e-5` (gentle, preserves pretrained SigLIP features); classifier head at `HEAD_LR=8e-5` (fast learning from random initialization)\n",
    "- **Masked BCE loss**: 3 of 6 labels have MISSING values — loss is zeroed out for those entries instead of dropping entire samples\n",
    "- **Light augmentation**: Horizontal flip + rotation + color jitter to compensate for small dataset (480 train images)\n",
    "- **eval_loss for model selection**: Val set has only 69 images — per-label AUC too noisy for checkpoint comparison\n",
    "- **Per-label threshold tuning**: Youden's J (J = sensitivity + specificity − 1) on validation set replaces a fixed threshold=0.5 after training\n",
    "\n",
    "### Dataset\n",
    "- **Source**: SurgWound (686 images: 480 train / 69 val / 137 test)\n",
    "- **Upload as Kaggle dataset** at `surgwound-dataset` for instant `/kaggle/input/` access\n",
    "\n",
    "### References\n",
    "- [MedSigLIP model](https://huggingface.co/google/medsiglip-448)\n",
    "- [Google's fine-tuning notebook](https://github.com/google-health/medsiglip/blob/main/notebooks/fine_tune_for_image_classification.ipynb)\n",
    "- [SurgWound dataset](https://huggingface.co/datasets/xuxuxuxuxu/SurgWound)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "### GPU Requirements\n",
    "This notebook is designed for **T4 (16GB)** on Kaggle free tier.\n",
    "It will also work on P100 (16GB), L4 (24GB), or A100 (40GB+).\n",
    "\n",
    "### Dataset Setup\n",
    "Before running, upload the `data/surgwound/` folder (containing `labels.csv` + `images/` with 686 JPGs)\n",
    "as a Kaggle dataset named `surgwound-dataset`. It will be accessible at `/kaggle/input/surgwound-dataset/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Install dependencies ──────────────────────────────────────────────────────\n",
    "# Pin transformers >= 4.46.0 for Trainer compatibility\n",
    "!pip install --upgrade --quiet \\\n",
    "    \"transformers>=4.46.0\" \\\n",
    "    accelerate \\\n",
    "    datasets \\\n",
    "    evaluate \\\n",
    "    tensorboard \\\n",
    "    scikit-learn \\\n",
    "    tqdm \\\n",
    "    pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Authenticate with Hugging Face ────────────────────────────────────────────\n",
    "# Required to download gated model: google/medsiglip-448\n",
    "#\n",
    "# On Kaggle: Add your HF token as a Kaggle Secret named \"HF_TOKEN\"\n",
    "# On Colab:  Add it to Colab Secrets, or it will prompt notebook_login()\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "if \"kaggle_secrets\" in dir() or os.path.exists(\"/kaggle\"):\n",
    "    # Running on Kaggle\n",
    "    try:\n",
    "        from kaggle_secrets import UserSecretsClient\n",
    "        secrets = UserSecretsClient()\n",
    "        os.environ[\"HF_TOKEN\"] = secrets.get_secret(\"HF_TOKEN\")\n",
    "        print(\"✓ HF_TOKEN loaded from Kaggle Secrets\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠ Could not load HF_TOKEN from Kaggle Secrets: {e}\")\n",
    "        print(\"  Falling back to huggingface_hub login...\")\n",
    "        from huggingface_hub import notebook_login\n",
    "        notebook_login()\n",
    "elif \"google.colab\" in sys.modules:\n",
    "    # Running on Colab\n",
    "    try:\n",
    "        from google.colab import userdata\n",
    "        os.environ[\"HF_TOKEN\"] = userdata.get(\"HF_TOKEN\")\n",
    "        print(\"✓ HF_TOKEN loaded from Colab Secrets\")\n",
    "    except Exception:\n",
    "        from huggingface_hub import notebook_login\n",
    "        notebook_login()\n",
    "else:\n",
    "    # Local / other environment\n",
    "    from huggingface_hub import get_token\n",
    "    if get_token() is None:\n",
    "        from huggingface_hub import notebook_login\n",
    "        notebook_login()\n",
    "    else:\n",
    "        print(\"✓ HF token already configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "All hyperparameters and label definitions in one place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Force single-GPU mode on multi-GPU Kaggle runtimes ──────────────────────\n",
    "# Must run BEFORE importing torch.\n",
    "import os\n",
    "\n",
    "if os.environ.get(\"CUDA_VISIBLE_DEVICES\") != \"0\":\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "print(f\"CUDA_VISIBLE_DEVICES={os.environ.get('CUDA_VISIBLE_DEVICES')}\")\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# ── Paths ─────────────────────────────────────────────────────────────────────\n",
    "# Adjust BASE_PATH depending on your environment:\n",
    "#   Kaggle:  /kaggle/input/surgwound-dataset\n",
    "#   Local:   ./data/surgwound\n",
    "\n",
    "if os.path.exists(\"/kaggle/input/datasets/kkfkmf/surgwound-dataset\"):\n",
    "    BASE_PATH = \"/kaggle/input/datasets/kkfkmf/surgwound-dataset\"  # confirmed working in Run 1\n",
    "elif os.path.exists(\"/kaggle/input/surgwound-dataset\"):\n",
    "    BASE_PATH = \"/kaggle/input/surgwound-dataset\"\n",
    "elif os.path.exists(\"/kaggle/input/surgwound\"):\n",
    "    BASE_PATH = \"/kaggle/input/surgwound\"\n",
    "else:\n",
    "    BASE_PATH = \"./data/surgwound\"  # Local development\n",
    "\n",
    "LABELS_CSV = os.path.join(BASE_PATH, \"labels.csv\")\n",
    "IMAGES_DIR = os.path.join(BASE_PATH, \"images\")\n",
    "\n",
    "# If Kaggle dataset was uploaded with directory mode=zip, images may arrive as images.zip\n",
    "IMAGES_ZIP = os.path.join(BASE_PATH, \"images.zip\")\n",
    "if not os.path.isdir(IMAGES_DIR) and os.path.isfile(IMAGES_ZIP):\n",
    "    import zipfile\n",
    "    print(f\"Extracting {IMAGES_ZIP} ...\")\n",
    "    with zipfile.ZipFile(IMAGES_ZIP, \"r\") as zf:\n",
    "        zf.extractall(BASE_PATH)\n",
    "    print(f\"✓ Extracted images to {IMAGES_DIR}\")\n",
    "\n",
    "# ── Model ─────────────────────────────────────────────────────────────────────\n",
    "MODEL_ID   = \"google/medsiglip-448\"\n",
    "OUTPUT_DIR = \"medsiglip-448-surgwound-v2\"   # separate dir so Run 1 checkpoint is preserved\n",
    "\n",
    "# ── Label definitions ─────────────────────────────────────────────────────────\n",
    "# 6 binary labels predicted from wound images\n",
    "LABEL_NAMES = [\n",
    "    \"healing_status\",   # 0: Healed, 1: Not Healed\n",
    "    \"erythema\",         # 0: Non-existent, 1: Existent       (has MISSING)\n",
    "    \"edema\",            # 0: Non-existent, 1: Existent       (has MISSING)\n",
    "    \"infection_risk\",   # 0: Low, 1: Medium+High\n",
    "    \"urgency\",          # 0: Green (home care), 1: Yellow+Red (needs attention)\n",
    "    \"exudate\",          # 0: Non-existent, 1: Any exudate    (has MISSING)\n",
    "]\n",
    "NUM_LABELS = len(LABEL_NAMES)\n",
    "\n",
    "id2label = {i: name for i, name in enumerate(LABEL_NAMES)}\n",
    "label2id = {name: i for i, name in enumerate(LABEL_NAMES)}\n",
    "\n",
    "# Precomputed from training split (non-MISSING samples only):\n",
    "#   healing:   neg=282, pos=198  → 282/198 = 1.42\n",
    "#   erythema:  neg=334, pos=129  → 334/129 = 2.59\n",
    "#   edema:     neg=328, pos=50   → 328/50  = 6.56\n",
    "#   infection: neg=402, pos=78   → 402/78  = 5.15\n",
    "#   urgency:   neg=423, pos=57   → 423/57  = 7.42\n",
    "#   exudate:   neg=367, pos=70   → 367/70  = 5.24\n",
    "POS_WEIGHT = torch.tensor([1.42, 2.59, 6.56, 5.15, 7.42, 5.24])\n",
    "\n",
    "# ── Freezing strategy ────────────────────────────────────────────────────────\n",
    "N_UNFREEZE = 8   # Unfreeze last N encoder blocks + classification head (Run 1: 4)\n",
    "                 # ~2× trainable capacity; differential LR mitigates overfit risk\n",
    "\n",
    "# ── Training hyperparameters ─────────────────────────────────────────────────\n",
    "BATCH_SIZE   = 4\n",
    "GRAD_ACCUM   = 4         # Effective batch = 4 × 4 = 16; gives 30 steps/epoch (Run 1: 16, 8 steps/epoch)\n",
    "EPOCHS       = 10        # 300 total optimizer steps vs Run 1's 40\n",
    "BACKBONE_LR  = 1.5e-5    # Gentle updates to preserve pretrained SigLIP features\n",
    "HEAD_LR      = 8e-5      # Fast learning for randomly initialized classifier head\n",
    "LR           = HEAD_LR   # TrainingArguments base LR — cosine scheduler scales both groups proportionally\n",
    "WARMUP_STEPS = 15        # 15/300 = 5% of total steps (Run 1: 10/40 = 25% — too aggressive)\n",
    "WEIGHT_DECAY = 0.015     # Slightly stronger regularisation for ~2× trainable capacity\n",
    "SCHEDULER    = \"cosine\"\n",
    "FP16         = True      # T4 doesn't support bf16; fp16 saves VRAM\n",
    "\n",
    "# ── Device ───────────────────────────────────────────────────────────────────\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    visible_gpu_count = torch.cuda.device_count()\n",
    "    print(f\"Visible GPU count: {visible_gpu_count}\")\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_mem  = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"✓ GPU: {gpu_name} ({gpu_mem:.1f} GB)\")\n",
    "    # For larger GPUs: increase BATCH_SIZE and reduce GRAD_ACCUM proportionally\n",
    "    # so that the effective batch size AND optimizer step count stay identical to T4.\n",
    "    # T4:  batch=4,  accum=4, eff=16, forward=120, steps/epoch=30\n",
    "    # A100: batch=16, accum=1, eff=16, forward=30,  steps/epoch=30 ← same!\n",
    "    if gpu_mem >= 30:  # A100 / L4\n",
    "        BATCH_SIZE = 16\n",
    "        GRAD_ACCUM = 1\n",
    "        print(f\"  → Adjusted: batch_size={BATCH_SIZE}, grad_accum={GRAD_ACCUM} (effective={BATCH_SIZE*GRAD_ACCUM}, same step count as T4)\")\n",
    "else:\n",
    "    print(\"⚠ No GPU detected — training will be extremely slow\")\n",
    "    FP16 = False\n",
    "\n",
    "# ── Step count verification (critical — print and sanity-check) ───────────────\n",
    "forward_passes  = (480 + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "optimizer_steps = (forward_passes + GRAD_ACCUM - 1) // GRAD_ACCUM\n",
    "total_steps     = EPOCHS * optimizer_steps\n",
    "warmup_pct      = 100 * WARMUP_STEPS / total_steps\n",
    "\n",
    "print(f\"\\nEffective batch size:    {BATCH_SIZE * GRAD_ACCUM}  (Run 1: 64)\")\n",
    "print(f\"Forward passes / epoch:  {forward_passes}\")\n",
    "print(f\"Optimizer steps / epoch: {optimizer_steps}  (Run 1: 8)\")\n",
    "print(f\"Total optimizer steps:   {total_steps}  (Run 1: 40)\")\n",
    "print(f\"Warmup: {WARMUP_STEPS} steps = {warmup_pct:.1f}% of total  (Run 1: 25.0%)\")\n",
    "print(f\"Backbone LR: {BACKBONE_LR}  |  Head LR: {HEAD_LR}\")\n",
    "print(f\"Dataset path: {BASE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load & Validate Dataset\n",
    "\n",
    "Read `labels.csv`, verify split counts (480/69/137), and print label distributions.\n",
    "This cell fails fast if the dataset is corrupted or missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ── Load CSV ──────────────────────────────────────────────────────────────────\n",
    "df = pd.read_csv(LABELS_CSV)\n",
    "print(f\"Loaded {len(df)} rows from {LABELS_CSV}\")\n",
    "print(f\"Columns: {list(df.columns)}\\n\")\n",
    "\n",
    "# ── Verify split counts ──────────────────────────────────────────────────────\n",
    "split_counts = df[\"split\"].value_counts().to_dict()\n",
    "print(\"Split counts:\", split_counts)\n",
    "\n",
    "assert split_counts.get(\"train\", 0) == 480, f\"Expected 480 train, got {split_counts.get('train', 0)}\"\n",
    "assert split_counts.get(\"validation\", 0) == 69, f\"Expected 69 val, got {split_counts.get('validation', 0)}\"\n",
    "assert split_counts.get(\"test\", 0) == 137, f\"Expected 137 test, got {split_counts.get('test', 0)}\"\n",
    "print(\"✓ Split counts verified: 480 train / 69 val / 137 test\\n\")\n",
    "\n",
    "# ── Verify all images exist on disk ──────────────────────────────────────────\n",
    "missing_images = []\n",
    "for _, row in df.iterrows():\n",
    "    img_path = os.path.join(BASE_PATH, row[\"image_path\"])\n",
    "    if not os.path.exists(img_path):\n",
    "        missing_images.append(img_path)\n",
    "\n",
    "if missing_images:\n",
    "    print(f\"✗ {len(missing_images)} images missing! First 5:\")\n",
    "    for p in missing_images[:5]:\n",
    "        print(f\"  {p}\")\n",
    "    raise FileNotFoundError(f\"{len(missing_images)} images not found on disk\")\n",
    "else:\n",
    "    print(f\"✓ All {len(df)} images verified on disk\\n\")\n",
    "\n",
    "# ── Print label distributions for training split ─────────────────────────────\n",
    "train_df = df[df[\"split\"] == \"train\"]\n",
    "print(\"=\" * 60)\n",
    "print(\"TRAINING SPLIT LABEL DISTRIBUTIONS\")\n",
    "print(\"=\" * 60)\n",
    "for col in [\"healing_status\", \"erythema\", \"edema\", \"infection_risk\", \"urgency_level\", \"exudate_type\"]:\n",
    "    counts = train_df[col].value_counts()\n",
    "    print(f\"\\n{col}:\")\n",
    "    for val, cnt in counts.items():\n",
    "        pct = 100 * cnt / len(train_df)\n",
    "        marker = \" ← MISSING\" if val == \"MISSING\" else \"\"\n",
    "        print(f\"  {val:55s} {cnt:4d} ({pct:5.1f}%){marker}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Label Encoding\n",
    "\n",
    "Convert raw CSV labels into a 6-dimensional binary vector per image.\n",
    "\n",
    "| # | Label | 0 (negative) | 1 (positive) | MISSING → -1 |\n",
    "|---|---|---|---|---|\n",
    "| 0 | healing_status | Healed | Not Healed | never |\n",
    "| 1 | erythema | Non-existent | Existent | yes (17 in train) |\n",
    "| 2 | edema | Non-existent | Existent | yes (102 in train) |\n",
    "| 3 | infection_risk | Low | Medium or High | never |\n",
    "| 4 | urgency | Home Care (Green) | Clinic Visit or Emergency | never |\n",
    "| 5 | exudate | Non-existent | Any type present | yes (43 in train) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_labels(row: pd.Series) -> list[float]:\n",
    "    \"\"\"\n",
    "    Convert a single CSV row into a 6-dim label vector.\n",
    "\n",
    "    Returns:\n",
    "        List of 6 floats: 0.0 (negative), 1.0 (positive), or -1.0 (MISSING).\n",
    "        The masked loss function will ignore -1.0 entries.\n",
    "    \"\"\"\n",
    "    labels = []\n",
    "\n",
    "    # 0. healing_status: \"Not Healed\" → 1 (positive), \"Healed\" → 0\n",
    "    labels.append(1.0 if row[\"healing_status\"] == \"Not Healed\" else 0.0)\n",
    "\n",
    "    # 1. erythema: \"Existent\" → 1, \"Non-existent\" → 0, \"MISSING\" → -1\n",
    "    if row[\"erythema\"] == \"MISSING\":\n",
    "        labels.append(-1.0)\n",
    "    else:\n",
    "        labels.append(1.0 if row[\"erythema\"] == \"Existent\" else 0.0)\n",
    "\n",
    "    # 2. edema: \"Existent\" → 1, \"Non-existent\" → 0, \"MISSING\" → -1\n",
    "    if row[\"edema\"] == \"MISSING\":\n",
    "        labels.append(-1.0)\n",
    "    else:\n",
    "        labels.append(1.0 if row[\"edema\"] == \"Existent\" else 0.0)\n",
    "\n",
    "    # 3. infection_risk: \"Medium\" or \"High\" → 1, \"Low\" → 0\n",
    "    labels.append(1.0 if row[\"infection_risk\"] in (\"Medium\", \"High\") else 0.0)\n",
    "\n",
    "    # 4. urgency: anything other than \"Home Care (Green)...\" → 1\n",
    "    labels.append(0.0 if row[\"urgency_level\"].startswith(\"Home Care\") else 1.0)\n",
    "\n",
    "    # 5. exudate: \"Non-existent\" → 0, \"MISSING\" → -1, anything else → 1\n",
    "    if row[\"exudate_type\"] == \"MISSING\":\n",
    "        labels.append(-1.0)\n",
    "    elif row[\"exudate_type\"] == \"Non-existent\":\n",
    "        labels.append(0.0)\n",
    "    else:\n",
    "        labels.append(1.0)  # Serous, Sanguineous, Purulent, Seropurulent\n",
    "\n",
    "    return labels\n",
    "\n",
    "\n",
    "# ── Verify encoding on a few known examples ──────────────────────────────────\n",
    "sample_row = train_df.iloc[0]\n",
    "sample_labels = encode_labels(sample_row)\n",
    "print(f\"Sample row (img_id={sample_row['img_id']}):\")\n",
    "print(f\"  healing_status = {str(sample_row['healing_status']):20s} → {sample_labels[0]}\")\n",
    "print(f\"  erythema       = {str(sample_row['erythema']):20s} → {sample_labels[1]}\")\n",
    "print(f\"  edema          = {str(sample_row['edema']):20s} → {sample_labels[2]}\")\n",
    "print(f\"  infection_risk = {str(sample_row['infection_risk']):20s} → {sample_labels[3]}\")\n",
    "print(f\"  urgency_level  = {str(sample_row['urgency_level'])[:30]:30s} → {sample_labels[4]}\")\n",
    "print(f\"  exudate_type   = {str(sample_row['exudate_type']):20s} → {sample_labels[5]}\")\n",
    "print(f\"\\n  Encoded vector: {sample_labels}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create HuggingFace Datasets\n",
    "\n",
    "Build `Dataset` objects for train, validation, and test splits.\n",
    "Each sample has an `image` (PIL) and `label` (6-dim float list)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, Features, Value, Sequence, Image as HFImage\n",
    "import PIL.Image  # Ensure PIL.Image is in sys.modules for datasets internals\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "def build_dataset_from_split(split_df: pd.DataFrame, split_name: str) -> Dataset:\n",
    "    \"\"\"\n",
    "    Build a HuggingFace Dataset from a pandas DataFrame for one split.\n",
    "\n",
    "    Loads images from disk and encodes labels as 6-dim float vectors.\n",
    "    Uses explicit Features schema to avoid datasets library PIL type-inference bugs.\n",
    "    \"\"\"\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    skipped = 0\n",
    "\n",
    "    for _, row in tqdm(split_df.iterrows(), total=len(split_df), desc=f\"Loading {split_name}\"):\n",
    "        img_path = os.path.join(BASE_PATH, row[\"image_path\"])\n",
    "        if not os.path.exists(img_path):\n",
    "            skipped += 1\n",
    "            continue\n",
    "        image_paths.append(img_path)\n",
    "        labels.append(encode_labels(row))\n",
    "\n",
    "    if skipped > 0:\n",
    "        print(f\"  ⚠ Skipped {skipped} missing images in {split_name}\")\n",
    "\n",
    "    # Explicit schema avoids PIL.Image.Image isinstance check inside datasets\n",
    "    features = Features({\n",
    "        \"image\": HFImage(),\n",
    "        \"label\": Sequence(Value(\"float32\"), length=6),\n",
    "    })\n",
    "\n",
    "    ds = Dataset.from_dict(\n",
    "        {\"image\": image_paths, \"label\": labels},\n",
    "        features=features,\n",
    "    )\n",
    "\n",
    "    print(f\"  ✓ {split_name}: {len(ds)} samples\")\n",
    "    return ds\n",
    "\n",
    "\n",
    "# ── Build all three splits ────────────────────────────────────────────────────\n",
    "train_ds_raw = build_dataset_from_split(df[df[\"split\"] == \"train\"], \"train\")\n",
    "val_ds_raw   = build_dataset_from_split(df[df[\"split\"] == \"validation\"], \"validation\")\n",
    "test_ds_raw  = build_dataset_from_split(df[df[\"split\"] == \"test\"], \"test\")\n",
    "\n",
    "print(f\"\\nDataset sizes: train={len(train_ds_raw)}, val={len(val_ds_raw)}, test={len(test_ds_raw)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Image Preprocessing\n",
    "\n",
    "Following Google's MedSigLIP preprocessing exactly:\n",
    "1. **Zero-pad to square** — pads shorter dimension with black pixels to preserve aspect ratio\n",
    "2. **Resize** to 448×448 (bilinear)\n",
    "3. **Normalize** — scale to [-1, 1] with mean=0.5, std=0.5\n",
    "\n",
    "Training adds light augmentation (horizontal flip, rotation, color jitter) to compensate for the small dataset.\n",
    "\n",
    "> **Note**: Uses pure PIL + numpy instead of torchvision to avoid Kaggle's torch/torchvision version conflict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image as PILImage, ImageEnhance\n",
    "from transformers import AutoImageProcessor\n",
    "\n",
    "# ── Load processor to get canonical image size and normalization ──────────────\n",
    "image_processor = AutoImageProcessor.from_pretrained(MODEL_ID)\n",
    "IMG_SIZE = image_processor.size[\"height\"]   # 448\n",
    "IMG_MEAN = image_processor.image_mean       # [0.5, 0.5, 0.5]\n",
    "IMG_STD  = image_processor.image_std        # [0.5, 0.5, 0.5]\n",
    "print(f\"✓ Image size: {IMG_SIZE}, mean: {IMG_MEAN}, std: {IMG_STD}\")\n",
    "\n",
    "\n",
    "# ── Pure PIL/numpy transform functions (no torchvision dependency) ────────────\n",
    "\n",
    "def _pil_to_tensor(img: PILImage.Image) -> torch.Tensor:\n",
    "    \"\"\"Convert PIL RGB image → float tensor (C, H, W) in [-1, 1].\"\"\"\n",
    "    arr = np.array(img, dtype=np.float32) / 255.0                        # [0, 1]\n",
    "    arr = (arr - np.array(IMG_MEAN, dtype=np.float32)) / \\\n",
    "          np.array(IMG_STD, dtype=np.float32)                             # [-1, 1]\n",
    "    return torch.from_numpy(arr).permute(2, 0, 1)                        # (C, H, W)\n",
    "\n",
    "\n",
    "def _zero_pad_to_square(img: PILImage.Image) -> PILImage.Image:\n",
    "    \"\"\"\n",
    "    Zero-pad shorter dimension to match longer — replicates Google's\n",
    "    CenterCrop(max(image.size)) trick exactly.\n",
    "    \"\"\"\n",
    "    w, h = img.size\n",
    "    max_dim = max(w, h)\n",
    "    if w == h:\n",
    "        return img\n",
    "    padded = PILImage.new(\"RGB\", (max_dim, max_dim), (0, 0, 0))\n",
    "    padded.paste(img, ((max_dim - w) // 2, (max_dim - h) // 2))\n",
    "    return padded\n",
    "\n",
    "\n",
    "def _augment(img: PILImage.Image) -> PILImage.Image:\n",
    "    \"\"\"Light training augmentation: flip, rotation, brightness/contrast jitter.\"\"\"\n",
    "    if random.random() < 0.5:\n",
    "        img = img.transpose(PILImage.Transpose.FLIP_LEFT_RIGHT)\n",
    "    angle = random.uniform(-10, 10)\n",
    "    img = img.rotate(angle, resample=PILImage.Resampling.BILINEAR,\n",
    "                     fillcolor=(0, 0, 0))\n",
    "    img = ImageEnhance.Brightness(img).enhance(random.uniform(0.9, 1.1))\n",
    "    img = ImageEnhance.Contrast(img).enhance(random.uniform(0.9, 1.1))\n",
    "    return img\n",
    "\n",
    "\n",
    "def _process_image(img: PILImage.Image, augment: bool) -> torch.Tensor:\n",
    "    \"\"\"Full pipeline: RGB → pad-to-square → [augment] → resize → normalise.\"\"\"\n",
    "    img = img.convert(\"RGB\")\n",
    "    img = _zero_pad_to_square(img)\n",
    "    if augment:\n",
    "        img = _augment(img)\n",
    "    img = img.resize((IMG_SIZE, IMG_SIZE), PILImage.Resampling.BILINEAR)\n",
    "    return _pil_to_tensor(img)\n",
    "\n",
    "\n",
    "def preprocess_train(examples: dict) -> dict:\n",
    "    \"\"\"Preprocess training examples with augmentation.\"\"\"\n",
    "    examples[\"pixel_values\"] = [\n",
    "        _process_image(img, augment=True) for img in examples[\"image\"]\n",
    "    ]\n",
    "    return examples\n",
    "\n",
    "\n",
    "def preprocess_eval(examples: dict) -> dict:\n",
    "    \"\"\"Preprocess validation/test examples without augmentation.\"\"\"\n",
    "    examples[\"pixel_values\"] = [\n",
    "        _process_image(img, augment=False) for img in examples[\"image\"]\n",
    "    ]\n",
    "    return examples\n",
    "\n",
    "\n",
    "# ── Apply preprocessing ───────────────────────────────────────────────────────\n",
    "print(\"Preprocessing training data (with augmentation)...\")\n",
    "train_ds = train_ds_raw.map(preprocess_train, batched=True, remove_columns=[\"image\"])\n",
    "\n",
    "print(\"Preprocessing validation data...\")\n",
    "val_ds = val_ds_raw.map(preprocess_eval, batched=True, remove_columns=[\"image\"])\n",
    "\n",
    "print(\"Preprocessing test data...\")\n",
    "test_ds = test_ds_raw.map(preprocess_eval, batched=True, remove_columns=[\"image\"])\n",
    "\n",
    "# ── Sanity check ──────────────────────────────────────────────────────────────\n",
    "sample_pv = torch.tensor(train_ds[0][\"pixel_values\"])\n",
    "assert sample_pv.shape == torch.Size([3, IMG_SIZE, IMG_SIZE]), \\\n",
    "    f\"Unexpected shape: {sample_pv.shape}\"\n",
    "assert sample_pv.min() >= -1.5 and sample_pv.max() <= 1.5, \\\n",
    "    f\"Pixel range unexpected: [{sample_pv.min():.2f}, {sample_pv.max():.2f}]\"\n",
    "\n",
    "print(f\"\\n✓ Preprocessed: train={len(train_ds)}, val={len(val_ds)}, test={len(test_ds)}\")\n",
    "print(f\"  pixel_values shape : {list(sample_pv.shape)}\")\n",
    "print(f\"  pixel value range  : [{sample_pv.min():.3f}, {sample_pv.max():.3f}]\")\n",
    "print(f\"  label              : {train_ds[0]['label']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visual Sanity Check\n",
    "\n",
    "Display sample images with their encoded labels to verify the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def show_sample(ds_raw, ds_processed, idx, title_prefix=\"\"):\n",
    "    \"\"\"Display an image alongside its encoded label vector.\"\"\"\n",
    "    raw_img = ds_raw[idx][\"image\"]\n",
    "    label_vec = ds_processed[idx][\"label\"]\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(4, 4))\n",
    "    ax.imshow(raw_img)\n",
    "    ax.set_title(f\"{title_prefix} sample {idx}\", fontsize=10)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "    label_str = \"\\n\".join(\n",
    "        f\"  {LABEL_NAMES[i]}: {label_vec[i]:+.0f}\"\n",
    "        + (\" (MISSING)\" if label_vec[i] == -1.0 else \"\")\n",
    "        for i in range(NUM_LABELS)\n",
    "    )\n",
    "    ax.text(\n",
    "        1.05, 0.5, label_str,\n",
    "        transform=ax.transAxes, fontsize=9, verticalalignment=\"center\",\n",
    "        fontfamily=\"monospace\",\n",
    "        bbox=dict(boxstyle=\"round\", facecolor=\"lightyellow\", alpha=0.8),\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Show 4 samples: 2 train, 2 val\n",
    "for i in [0, 50]:\n",
    "    show_sample(train_ds_raw, train_ds, i, title_prefix=\"Train\")\n",
    "for i in [0, 30]:\n",
    "    show_sample(val_ds_raw, val_ds, i, title_prefix=\"Val\")\n",
    "\n",
    "# ── Verify MISSING encoding ──────────────────────────────────────────────────\n",
    "missing_found = False\n",
    "for idx in range(len(train_ds)):\n",
    "    label_vec = train_ds[idx][\"label\"]\n",
    "    if -1.0 in label_vec:\n",
    "        missing_indices = [i for i, v in enumerate(label_vec) if v == -1.0]\n",
    "        missing_names = [LABEL_NAMES[i] for i in missing_indices]\n",
    "        print(f\"✓ Found MISSING in train[{idx}]: {missing_names}\")\n",
    "        print(f\"  Full label vector: {label_vec}\")\n",
    "        missing_found = True\n",
    "        break\n",
    "\n",
    "if not missing_found:\n",
    "    print(\"⚠ No MISSING values found in training data — check encoding!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Load Model with Selective Freezing\n",
    "\n",
    "Load `google/medsiglip-448` with a 6-label classification head.\n",
    "Freeze all parameters except the last 4 encoder blocks + the new head.\n",
    "\n",
    "This reduces trainable params from ~400M to ~50-60M, fitting comfortably in T4 16GB VRAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForImageClassification\n",
    "\n",
    "# ── Load pretrained model with new classification head ────────────────────────\n",
    "model = AutoModelForImageClassification.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    problem_type=\"multi_label_classification\",\n",
    "    num_labels=NUM_LABELS,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    ignore_mismatched_sizes=True,  # Head size mismatch: pretrained != 6\n",
    ")\n",
    "\n",
    "# ── Freeze all parameters ────────────────────────────────────────────────────\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# ── Unfreeze classification head (randomly initialized — MUST be trainable) ──\n",
    "for param in model.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# ── Unfreeze last N encoder blocks ───────────────────────────────────────────\n",
    "encoder_layers = model.vision_model.encoder.layers\n",
    "total_layers = len(encoder_layers)\n",
    "print(f\"Encoder has {total_layers} layers. Unfreezing last {N_UNFREEZE}...\")\n",
    "\n",
    "for layer in encoder_layers[-N_UNFREEZE:]:\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "# ── Print parameter counts ───────────────────────────────────────────────────\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "frozen_params = total_params - trainable_params\n",
    "\n",
    "print(f\"\\nParameter summary:\")\n",
    "print(f\"  Total:     {total_params:>12,}\")\n",
    "print(f\"  Trainable: {trainable_params:>12,} ({100*trainable_params/total_params:.1f}%)\")\n",
    "print(f\"  Frozen:    {frozen_params:>12,} ({100*frozen_params/total_params:.1f}%)\")\n",
    "\n",
    "# ── Move to device and check VRAM ────────────────────────────────────────────\n",
    "model = model.to(device)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    vram_used = torch.cuda.memory_allocated() / 1e9\n",
    "    vram_total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"\\nVRAM after model load: {vram_used:.2f} / {vram_total:.1f} GB\")\n",
    "    if vram_used > vram_total * 0.7:\n",
    "        print(\"⚠ VRAM usage is high — consider reducing N_UNFREEZE or BATCH_SIZE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Masked BCE Loss Function\n",
    "\n",
    "Three labels (erythema, edema, exudate) have MISSING values encoded as `-1.0`.\n",
    "Instead of dropping entire rows (losing signal for other valid labels),\n",
    "the masked loss zeros out gradient contributions for MISSING entries.\n",
    "\n",
    "Every image contributes to every label it has data for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import BCEWithLogitsLoss\n",
    "\n",
    "\n",
    "def masked_bce_loss(\n",
    "    outputs: dict,\n",
    "    labels: torch.Tensor,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    BCE loss with per-element masking for MISSING values and class-imbalance weighting.\n",
    "\n",
    "    Args:\n",
    "        outputs: Model output dict containing 'logits' of shape (batch, 6).\n",
    "        labels:  Tensor of shape (batch, 6) with values in {0.0, 1.0, -1.0}.\n",
    "                 -1.0 indicates MISSING — loss for that entry is masked out.\n",
    "\n",
    "    Returns:\n",
    "        Scalar loss tensor (mean over valid entries only).\n",
    "    \"\"\"\n",
    "    logits = outputs.get(\"logits\")             # (batch, 6)\n",
    "    mask = (labels >= 0).float()                # 1 where valid, 0 where MISSING\n",
    "    safe_labels = labels.clamp(min=0.0)         # Replace -1 with 0 for BCE math\n",
    "\n",
    "    pos_weight = POS_WEIGHT.to(logits.device)\n",
    "    loss_fct = BCEWithLogitsLoss(pos_weight=pos_weight, reduction=\"none\")\n",
    "    per_element_loss = loss_fct(logits, safe_labels)  # (batch, 6)\n",
    "    masked_loss = per_element_loss * mask               # Zero out MISSING\n",
    "\n",
    "    # Mean over valid entries only (prevents bias from batch MISSING counts)\n",
    "    num_valid = mask.sum()\n",
    "    if num_valid == 0:\n",
    "        return torch.tensor(0.0, device=logits.device, requires_grad=True)\n",
    "\n",
    "    return masked_loss.sum() / num_valid\n",
    "\n",
    "\n",
    "# ── Unit test: verify MISSING entries are masked ─────────────────────────────\n",
    "print(\"Running masked loss unit test...\")\n",
    "\n",
    "# Fake logits and labels with known MISSING entries\n",
    "_test_logits = torch.tensor([[0.5, 0.3, -0.2, 0.1, 0.4, 0.6]])\n",
    "_test_labels_valid = torch.tensor([[1.0, 0.0, 1.0, 0.0, 1.0, 0.0]])   # All valid\n",
    "_test_labels_missing = torch.tensor([[1.0, 0.0, -1.0, 0.0, 1.0, -1.0]])  # Entries 2,5 MISSING\n",
    "\n",
    "# Compute loss with all valid\n",
    "_loss_valid = masked_bce_loss({\"logits\": _test_logits}, _test_labels_valid)\n",
    "\n",
    "# Compute loss with MISSING\n",
    "_loss_masked = masked_bce_loss({\"logits\": _test_logits}, _test_labels_missing)\n",
    "\n",
    "print(f\"  Loss (all valid):     {_loss_valid.item():.4f}  (6/6 entries contribute)\")\n",
    "print(f\"  Loss (2 MISSING):     {_loss_masked.item():.4f}  (4/6 entries contribute)\")\n",
    "print(f\"  Losses differ: {_loss_valid.item() != _loss_masked.item()} (expected: True)\")\n",
    "\n",
    "# Verify gradient flows for valid entries but not MISSING\n",
    "_test_logits_grad = torch.tensor([[0.5, 0.3, -0.2, 0.1, 0.4, 0.6]], requires_grad=True)\n",
    "_loss_for_grad = masked_bce_loss({\"logits\": _test_logits_grad}, _test_labels_missing)\n",
    "_loss_for_grad.backward()\n",
    "grad = _test_logits_grad.grad[0]\n",
    "print(f\"  Gradients: {[round(g, 4) for g in grad.tolist()]}\")\n",
    "print(f\"  MISSING positions (2,5) have zero grad: {grad[2].item() == 0.0 and grad[5].item() == 0.0}\")\n",
    "print(\"✓ Masked loss unit test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Evaluation Metrics\n",
    "\n",
    "Macro-averaged One-vs-Rest ROC AUC + per-label sensitivity/specificity.\n",
    "For labels with MISSING values in the eval set, metrics are computed only on non-MISSING samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Compute evaluation metrics, handling MISSING values (-1) in labels.\n",
    "\n",
    "    Returns:\n",
    "        Dict with 'roc_auc_macro' and per-label AUC.\n",
    "    \"\"\"\n",
    "    logits, labels = eval_pred\n",
    "    scores = sigmoid(logits)\n",
    "\n",
    "    results = {}\n",
    "    per_label_auc = []\n",
    "\n",
    "    for i, name in enumerate(LABEL_NAMES):\n",
    "        # Mask out MISSING entries for this label\n",
    "        valid_mask = labels[:, i] >= 0\n",
    "        if valid_mask.sum() == 0:\n",
    "            continue\n",
    "\n",
    "        y_true = labels[valid_mask, i]\n",
    "        y_score = scores[valid_mask, i]\n",
    "\n",
    "        # Need at least one sample of each class for AUC\n",
    "        if len(np.unique(y_true)) < 2:\n",
    "            results[f\"auc_{name}\"] = float(\"nan\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            auc = roc_auc_score(y_true, y_score)\n",
    "            per_label_auc.append(auc)\n",
    "            results[f\"auc_{name}\"] = auc\n",
    "        except ValueError:\n",
    "            results[f\"auc_{name}\"] = float(\"nan\")\n",
    "\n",
    "    # Macro-averaged AUC (only over labels with valid AUC)\n",
    "    if per_label_auc:\n",
    "        results[\"roc_auc_macro\"] = np.mean(per_label_auc)\n",
    "    else:\n",
    "        results[\"roc_auc_macro\"] = float(\"nan\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def compute_full_metrics(logits, labels, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Compute detailed metrics including sensitivity/specificity per label.\n",
    "    Used for final test evaluation (not during training).\n",
    "    \"\"\"\n",
    "    scores = sigmoid(logits)\n",
    "    results = {}\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(f\"{'Label':<20} {'AUC':>8} {'Sens':>8} {'Spec':>8} {'N valid':>8}\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    all_aucs = []\n",
    "    for i, name in enumerate(LABEL_NAMES):\n",
    "        valid_mask = labels[:, i] >= 0\n",
    "        n_valid = valid_mask.sum()\n",
    "        y_true = labels[valid_mask, i]\n",
    "        y_score = scores[valid_mask, i]\n",
    "        y_pred = (y_score > threshold).astype(int)\n",
    "\n",
    "        # AUC\n",
    "        try:\n",
    "            auc = roc_auc_score(y_true, y_score) if len(np.unique(y_true)) >= 2 else float(\"nan\")\n",
    "        except ValueError:\n",
    "            auc = float(\"nan\")\n",
    "\n",
    "        # Sensitivity and specificity\n",
    "        tp = ((y_pred == 1) & (y_true == 1)).sum()\n",
    "        tn = ((y_pred == 0) & (y_true == 0)).sum()\n",
    "        fn = ((y_pred == 0) & (y_true == 1)).sum()\n",
    "        fp = ((y_pred == 1) & (y_true == 0)).sum()\n",
    "        sens = tp / (tp + fn) if (tp + fn) > 0 else float(\"nan\")\n",
    "        spec = tn / (tn + fp) if (tn + fp) > 0 else float(\"nan\")\n",
    "\n",
    "        if not np.isnan(auc):\n",
    "            all_aucs.append(auc)\n",
    "\n",
    "        results[f\"auc_{name}\"] = auc\n",
    "        results[f\"sens_{name}\"] = sens\n",
    "        results[f\"spec_{name}\"] = spec\n",
    "\n",
    "        print(f\"{name:<20} {auc:>8.4f} {sens:>8.4f} {spec:>8.4f} {n_valid:>8d}\")\n",
    "\n",
    "    macro_auc = np.mean(all_aucs) if all_aucs else float(\"nan\")\n",
    "    results[\"roc_auc_macro\"] = macro_auc\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"{'Macro AUC':<20} {macro_auc:>8.4f}\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "print(\"✓ Metrics functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(examples):\n",
    "    \"\"\"\n",
    "    Collate function for Trainer.\n",
    "\n",
    "    Stacks pixel_values into a (batch, 3, 448, 448) tensor\n",
    "    and labels into a (batch, 6) float tensor (with -1.0 for MISSING).\n",
    "    \"\"\"\n",
    "    pixel_values = torch.stack([torch.tensor(ex[\"pixel_values\"]) for ex in examples])\n",
    "    labels = torch.tensor([ex[\"label\"] for ex in examples], dtype=torch.float)\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
    "\n",
    "\n",
    "# ── Quick verification ────────────────────────────────────────────────────────\n",
    "_test_batch = collate_fn([train_ds[0], train_ds[1]])\n",
    "print(f\"Collated batch shapes:\")\n",
    "print(f\"  pixel_values: {_test_batch['pixel_values'].shape}\")   # (2, 3, 448, 448)\n",
    "print(f\"  labels:       {_test_batch['labels'].shape}\")          # (2, 6)\n",
    "print(f\"  labels dtype: {_test_batch['labels'].dtype}\")          # float32\n",
    "print(f\"  labels[0]:    {_test_batch['labels'][0].tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Trainer Setup\n",
    "\n",
    "Uses a **subclassed Trainer** to inject the masked BCE loss and **differential learning rates** via `create_optimizer`.\n",
    "This approach works across all `transformers` versions (safer than `compute_loss_func`\n",
    "parameter which was introduced in ~4.46.0 and may be unavailable on some Kaggle kernels).\n",
    "\n",
    "### Training Configuration Summary\n",
    "| Parameter | Value | Rationale |\n",
    "|---|---|---|\n",
    "| Effective batch size | 16 (4×4) | More optimizer steps per epoch (Run 1 used 64) |\n",
    "| Epochs | 10 | 300 total steps; `load_best_model_at_end` guards against overfit |\n",
    "| Backbone LR | 1.5e-5 | Gentle updates to preserve pretrained SigLIP features |\n",
    "| Head LR | 8e-5 | Fast learning for randomly initialized classifier head |\n",
    "| Warmup steps | 15 | 5% of 300 total steps (Run 1: 25% — severely under-warmed) |\n",
    "| Scheduler | Cosine | Scales both param groups proportionally |\n",
    "| fp16 | True | T4 VRAM optimization |\n",
    "| Model selection | eval_loss | Val set too small (69) for reliable AUC |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "\n",
    "class WoundClassificationTrainer(Trainer):\n",
    "    \"\"\"\n",
    "    Custom Trainer with two enhancements over a plain HF Trainer:\n",
    "      1. Masked BCE loss for MISSING label handling (compute_loss).\n",
    "      2. Differential learning rates (create_optimizer):\n",
    "           - backbone encoder blocks (unfrozen): BACKBONE_LR = 1.5e-5\n",
    "           - classifier head (random-init):      HEAD_LR     = 8e-5\n",
    "    \"\"\"\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        loss = masked_bce_loss(outputs, labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "    def create_optimizer(self):\n",
    "        \"\"\"\n",
    "        Build AdamW with 3 param groups for differential learning rates.\n",
    "\n",
    "        Groups:\n",
    "          1. backbone trainable params that receive weight decay  → BACKBONE_LR + WEIGHT_DECAY\n",
    "          2. backbone trainable params exempt from weight decay   → BACKBONE_LR + 0.0\n",
    "          3. classifier head params                               → HEAD_LR + WEIGHT_DECAY\n",
    "\n",
    "        The HF Trainer's cosine scheduler uses LambdaLR, which multiplies each\n",
    "        group's stored base_lr by the same schedule factor → warmup and cosine\n",
    "        decay apply proportionally to both backbone and head groups.\n",
    "        \"\"\"\n",
    "        no_decay  = {\"bias\", \"layer_norm.weight\", \"LayerNorm.weight\"}\n",
    "        head_params = list(self.model.classifier.parameters())\n",
    "        head_ids    = {id(p) for p in head_params}\n",
    "\n",
    "        backbone_decay, backbone_nodecay = [], []\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if not param.requires_grad or id(param) in head_ids:\n",
    "                continue\n",
    "            if any(nd in name for nd in no_decay):\n",
    "                backbone_nodecay.append(param)\n",
    "            else:\n",
    "                backbone_decay.append(param)\n",
    "\n",
    "        param_groups = [\n",
    "            {\"params\": backbone_decay,   \"lr\": BACKBONE_LR, \"weight_decay\": WEIGHT_DECAY},\n",
    "            {\"params\": backbone_nodecay, \"lr\": BACKBONE_LR, \"weight_decay\": 0.0},\n",
    "            {\"params\": head_params,      \"lr\": HEAD_LR,     \"weight_decay\": WEIGHT_DECAY},\n",
    "        ]\n",
    "\n",
    "        self.optimizer = torch.optim.AdamW(param_groups)\n",
    "\n",
    "        print(f\"  AdamW param groups:\")\n",
    "        print(f\"    backbone (w/ decay):  {len(backbone_decay):>4} tensors @ lr={BACKBONE_LR:.1e}\")\n",
    "        print(f\"    backbone (no decay):  {len(backbone_nodecay):>4} tensors @ lr={BACKBONE_LR:.1e}\")\n",
    "        print(f\"    classifier head:      {len(head_params):>4} tensors @ lr={HEAD_LR:.1e}\")\n",
    "        return self.optimizer\n",
    "\n",
    "\n",
    "# ── Training arguments ────────────────────────────────────────────────────────\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE * 2,  # Larger batch for eval (no grad)\n",
    "    gradient_accumulation_steps=GRAD_ACCUM,\n",
    "    learning_rate=LR,                           # = HEAD_LR; scheduler scales both groups proportionally\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    "    lr_scheduler_type=SCHEDULER,\n",
    "    fp16=FP16,\n",
    "    logging_steps=1,                    # 30 steps/epoch — log every step\n",
    "    save_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",              # renamed from evaluation_strategy in transformers >=4.46\n",
    "    metric_for_best_model=\"eval_loss\",  # Val set too small for reliable AUC\n",
    "    greater_is_better=False,            # Lower loss = better\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"tensorboard\",\n",
    "    push_to_hub=False,                  # Push manually after evaluation\n",
    "    remove_unused_columns=False,        # Keep our custom columns\n",
    "    dataloader_num_workers=2,\n",
    "    save_total_limit=3,                 # Keep only 3 best checkpoints\n",
    ")\n",
    "\n",
    "# ── Create Trainer ───────────────────────────────────────────────────────────\n",
    "trainer = WoundClassificationTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "forward_passes  = (len(train_ds) + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "optimizer_steps = (forward_passes + GRAD_ACCUM - 1) // GRAD_ACCUM\n",
    "print(f\"✓ Trainer initialized\")\n",
    "print(f\"  Effective batch size:    {BATCH_SIZE * GRAD_ACCUM}\")\n",
    "print(f\"  Forward passes / epoch:  {forward_passes}\")\n",
    "print(f\"  Optimizer steps / epoch: {optimizer_steps}\")\n",
    "print(f\"  Total optimizer steps:   {EPOCHS * optimizer_steps}\")\n",
    "print(f\"  Model selection: best eval_loss (lower is better)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Train\n",
    "\n",
    "Expected training time: **25–35 minutes on T4**.\n",
    "Increased from Run 1 (12.8 min) due to: (a) 10 epochs vs 5, (b) 8 unfrozen blocks vs 4 (heavier backward pass per step), (c) 30 optimizer steps/epoch vs 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "print(\"Starting training...\\n\")\n",
    "start_time = time.time()\n",
    "\n",
    "train_result = trainer.train()\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Training complete in {elapsed/60:.1f} minutes\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# ── Check for NaN (fp16 safety) ──────────────────────────────────────────────\n",
    "final_loss = train_result.training_loss\n",
    "if np.isnan(final_loss):\n",
    "    print(\"\\n⚠ WARNING: Training loss is NaN!\")\n",
    "    print(\"  This likely means fp16 caused numerical instability.\")\n",
    "    print(\"  Try re-running with FP16 = False in the config cell.\")\n",
    "else:\n",
    "    print(f\"Final training loss: {final_loss:.4f}\")\n",
    "    print(f\"Best model loaded from: {trainer.state.best_model_checkpoint}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Evaluate on Test Set\n",
    "\n",
    "Run inference on the 137 test images and compute per-label metrics:\n",
    "AUC, sensitivity, and specificity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Run prediction on test set ───────────────────────────────────────────────\n",
    "print(\"Running inference on test set (137 images)...\")\n",
    "predictions = trainer.predict(test_ds)\n",
    "\n",
    "test_logits = predictions.predictions  # (137, 6)\n",
    "test_labels = predictions.label_ids     # (137, 6)\n",
    "\n",
    "print(f\"Logits shape: {test_logits.shape}\")\n",
    "print(f\"Labels shape: {test_labels.shape}\")\n",
    "\n",
    "# ── Compute detailed metrics ─────────────────────────────────────────────────\n",
    "test_metrics = compute_full_metrics(test_logits, test_labels, threshold=0.5)\n",
    "\n",
    "# ── Summary ──────────────────────────────────────────────────────────────────\n",
    "print(f\"\\nTest set macro-averaged ROC AUC: {test_metrics['roc_auc_macro']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Per-Label Threshold Tuning\n",
    "\n",
    "Run 1 showed clear threshold miscalibration at the fixed 0.5 cutoff (all labels use the same decoder boundary):\n",
    "- `healing_status`: sens=0.84 / spec=0.26 → threshold too low (model over-predicts \"not healed\")\n",
    "- `infection_risk`: sens=0.35 / spec=0.68 → threshold too high (25 of 40 infection cases missed)\n",
    "\n",
    "**Youden's J statistic** (`J = sensitivity + specificity − 1`) finds the operating point that maximises true detection rate minus false alarm rate. It is equivalent to maximising the vertical distance from the ROC diagonal, and is computed independently per label.\n",
    "\n",
    "Thresholds are tuned on the **validation set** (69 images) and then applied to the test set and inference demo — no data leakage. Note that Macro AUC is threshold-independent (area under the full ROC curve), so it stays identical regardless of threshold choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# ── Get validation set predictions for threshold tuning ──────────────────────\n",
    "print(\"Running inference on validation set for threshold tuning (69 images)...\")\n",
    "val_preds        = trainer.predict(val_ds)\n",
    "val_logits_t     = val_preds.predictions    # (69, 6)\n",
    "val_labels_t     = val_preds.label_ids      # (69, 6)\n",
    "val_scores_t     = sigmoid(val_logits_t)\n",
    "\n",
    "# ── Sweep thresholds per label, maximise Youden's J = sens + spec - 1 ────────\n",
    "# Tuned on val set only — applied to test (no leakage).\n",
    "PRED_THRESHOLDS = {}\n",
    "threshold_grid  = np.arange(0.10, 0.91, 0.01)\n",
    "\n",
    "print(f\"\\n{'Label':<20} {'J@0.50':>7} {'Best-J':>7} {'Thresh':>7} {'Δ sens':>8} {'Δ spec':>8}\")\n",
    "print(\"-\" * 62)\n",
    "\n",
    "for i, name in enumerate(LABEL_NAMES):\n",
    "    valid_mask = val_labels_t[:, i] >= 0\n",
    "    y_true     = val_labels_t[valid_mask, i]\n",
    "    y_score    = val_scores_t[valid_mask, i]\n",
    "\n",
    "    # Baseline at threshold=0.5\n",
    "    y_pred05 = (y_score > 0.5).astype(int)\n",
    "    tp05 = ((y_pred05 == 1) & (y_true == 1)).sum()\n",
    "    tn05 = ((y_pred05 == 0) & (y_true == 0)).sum()\n",
    "    fn05 = ((y_pred05 == 0) & (y_true == 1)).sum()\n",
    "    fp05 = ((y_pred05 == 1) & (y_true == 0)).sum()\n",
    "    sens05 = tp05 / (tp05 + fn05) if (tp05 + fn05) > 0 else 0.0\n",
    "    spec05 = tn05 / (tn05 + fp05) if (tn05 + fp05) > 0 else 0.0\n",
    "    j05    = sens05 + spec05 - 1.0\n",
    "\n",
    "    # Sweep\n",
    "    best_j, best_thresh       = j05, 0.5\n",
    "    best_sens, best_spec      = sens05, spec05\n",
    "    for thresh in threshold_grid:\n",
    "        y_pred = (y_score > thresh).astype(int)\n",
    "        tp = ((y_pred == 1) & (y_true == 1)).sum()\n",
    "        tn = ((y_pred == 0) & (y_true == 0)).sum()\n",
    "        fn = ((y_pred == 0) & (y_true == 1)).sum()\n",
    "        fp = ((y_pred == 1) & (y_true == 0)).sum()\n",
    "        sens = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        spec = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
    "        j    = sens + spec - 1.0\n",
    "        if j > best_j:\n",
    "            best_j, best_thresh = j, thresh\n",
    "            best_sens, best_spec = sens, spec\n",
    "\n",
    "    PRED_THRESHOLDS[name] = float(round(best_thresh, 2))\n",
    "    delta_sens = best_sens - sens05\n",
    "    delta_spec = best_spec - spec05\n",
    "    print(f\"{name:<20} {j05:>7.3f} {best_j:>7.3f} {best_thresh:>7.2f} {delta_sens:>+8.3f} {delta_spec:>+8.3f}\")\n",
    "\n",
    "print(f\"\\nTuned thresholds: {PRED_THRESHOLDS}\")\n",
    "\n",
    "# ── Re-evaluate test set with tuned per-label thresholds ─────────────────────\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TEST SET — TUNED PER-LABEL THRESHOLDS (for comparison with fixed-0.5 above)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "test_scores_tuned = sigmoid(test_logits)   # test_logits from previous cell\n",
    "tuned_aucs        = []\n",
    "\n",
    "print(f\"\\n{'Label':<20} {'AUC':>8} {'Thresh':>7} {'Sens':>8} {'Spec':>8} {'N valid':>8}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for i, name in enumerate(LABEL_NAMES):\n",
    "    thresh     = PRED_THRESHOLDS[name]\n",
    "    valid_mask = test_labels[:, i] >= 0\n",
    "    n_valid    = valid_mask.sum()\n",
    "    y_true     = test_labels[valid_mask, i]\n",
    "    y_score    = test_scores_tuned[valid_mask, i]\n",
    "    y_pred     = (y_score > thresh).astype(int)\n",
    "\n",
    "    try:\n",
    "        auc = roc_auc_score(y_true, y_score) if len(np.unique(y_true)) >= 2 else float(\"nan\")\n",
    "    except ValueError:\n",
    "        auc = float(\"nan\")\n",
    "\n",
    "    tp = ((y_pred == 1) & (y_true == 1)).sum()\n",
    "    tn = ((y_pred == 0) & (y_true == 0)).sum()\n",
    "    fn = ((y_pred == 0) & (y_true == 1)).sum()\n",
    "    fp = ((y_pred == 1) & (y_true == 0)).sum()\n",
    "    sens = tp / (tp + fn) if (tp + fn) > 0 else float(\"nan\")\n",
    "    spec = tn / (tn + fp) if (tn + fp) > 0 else float(\"nan\")\n",
    "\n",
    "    if not np.isnan(auc):\n",
    "        tuned_aucs.append(auc)\n",
    "\n",
    "    print(f\"{name:<20} {auc:>8.4f} {thresh:>7.2f} {sens:>8.4f} {spec:>8.4f} {n_valid:>8d}\")\n",
    "\n",
    "macro_auc = np.mean(tuned_aucs) if tuned_aucs else float(\"nan\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Macro AUC':<20} {macro_auc:>8.4f}  (AUC is threshold-independent)\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nNote: Macro AUC identical to the fixed-threshold run above — thresholds only shift the sens/spec balance.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Save Model\n",
    "\n",
    "Save the fine-tuned model and image processor locally.\n",
    "Optionally push to Hugging Face Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Save locally ─────────────────────────────────────────────────────────────\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "image_processor.save_pretrained(OUTPUT_DIR)\n",
    "print(f\"✓ Model and image processor saved to {OUTPUT_DIR}/\")\n",
    "\n",
    "# ── Optionally push to Hugging Face Hub ──────────────────────────────────────\n",
    "# Uncomment the following lines to push to your HF account:\n",
    "#\n",
    "# HUB_MODEL_ID = \"<your-username>/medsiglip-448-mamaguard-wound\"\n",
    "# trainer.push_to_hub(HUB_MODEL_ID)\n",
    "# image_processor.push_to_hub(HUB_MODEL_ID)\n",
    "# print(f\"✓ Pushed to https://huggingface.co/{HUB_MODEL_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Inference Demo\n",
    "\n",
    "Load the saved model and run inference on 3 test images\n",
    "to show what the downstream Gemini orchestrator would receive.\n",
    "Per-label tuned thresholds from Section 15 are used by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Load the fine-tuned model ────────────────────────────────────────────────\n",
    "ft_model = AutoModelForImageClassification.from_pretrained(\n",
    "    OUTPUT_DIR,\n",
    "    problem_type=\"multi_label_classification\",\n",
    "    num_labels=NUM_LABELS,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "ft_model.eval()\n",
    "\n",
    "# ── Human-readable label mapping ─────────────────────────────────────────────\n",
    "LABEL_DISPLAY = {\n",
    "    \"healing_status\": (\"progressing\", \"not progressing\"),\n",
    "    \"erythema\":       (\"absent\", \"present (redness)\"),\n",
    "    \"edema\":          (\"absent\", \"present (swelling)\"),\n",
    "    \"infection_risk\": (\"low\", \"elevated\"),\n",
    "    \"urgency\":        (\"home care OK\", \"needs professional attention\"),\n",
    "    \"exudate\":        (\"absent\", \"present (drainage)\"),\n",
    "}\n",
    "\n",
    "# Fall back to 0.5 per label if threshold tuning cell was not run\n",
    "_default_thresholds = {name: 0.5 for name in LABEL_NAMES}\n",
    "_tuned = \"PRED_THRESHOLDS\" in dir() or \"PRED_THRESHOLDS\" in globals()\n",
    "_active_thresholds = PRED_THRESHOLDS if _tuned else _default_thresholds  # type: ignore[name-defined]\n",
    "print(f\"Using {'tuned per-label' if _tuned else 'default (0.5)'} thresholds: {_active_thresholds}\")\n",
    "\n",
    "\n",
    "def predict_wound(image, model, thresholds: dict | None = None):\n",
    "    \"\"\"\n",
    "    Run wound assessment on a single image using per-label thresholds.\n",
    "\n",
    "    Args:\n",
    "        image:      PIL image (any size/mode — will be preprocessed internally).\n",
    "        model:      Fine-tuned SiglipForImageClassification.\n",
    "        thresholds: Dict mapping label name → decision threshold.\n",
    "                    Defaults to per-label tuned thresholds (or 0.5 if not available).\n",
    "\n",
    "    Returns:\n",
    "        Dict: label name → {\"score\": float, \"prediction\": str}\n",
    "    \"\"\"\n",
    "    if thresholds is None:\n",
    "        thresholds = _active_thresholds\n",
    "\n",
    "    # Apply same preprocessing as eval (pure PIL pipeline, no augmentation)\n",
    "    pixel_values = _process_image(image, augment=False)\n",
    "    pixel_values = pixel_values.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    model_device = next(model.parameters()).device\n",
    "    pixel_values = pixel_values.to(model_device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(pixel_values=pixel_values)\n",
    "\n",
    "    probs = torch.sigmoid(outputs.logits[0]).cpu().numpy()\n",
    "\n",
    "    results = {}\n",
    "    for i, name in enumerate(LABEL_NAMES):\n",
    "        neg_label, pos_label = LABEL_DISPLAY[name]\n",
    "        is_positive = probs[i] > thresholds.get(name, 0.5)\n",
    "        results[name] = {\n",
    "            \"score\":      float(probs[i]),\n",
    "            \"threshold\":  thresholds.get(name, 0.5),\n",
    "            \"prediction\": pos_label if is_positive else neg_label,\n",
    "        }\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# ── Demo on 3 test images ────────────────────────────────────────────────────\n",
    "test_df     = df[df[\"split\"] == \"test\"]\n",
    "demo_indices = [0, 50, 100]\n",
    "\n",
    "for idx in demo_indices:\n",
    "    if idx >= len(test_df):\n",
    "        continue\n",
    "\n",
    "    row      = test_df.iloc[idx]\n",
    "    img_path = os.path.join(BASE_PATH, row[\"image_path\"])\n",
    "    img      = PILImage.open(img_path).convert(\"RGB\")\n",
    "\n",
    "    results  = predict_wound(img, ft_model)\n",
    "\n",
    "    print(f\"\\n{chr(9472)*65}\")\n",
    "    print(f\"Image: {row['image_path']} (img_id={row['img_id']})\")\n",
    "    print(f\"{chr(9472)*65}\")\n",
    "    print(f\"\\n{'Label':<20} {'Score bar':^22} {'Score':>6}  {'Thresh':>6}   Prediction\")\n",
    "    for name, info in results.items():\n",
    "        confidence = info[\"score\"]\n",
    "        thresh     = info[\"threshold\"]\n",
    "        prediction = info[\"prediction\"]\n",
    "        filled  = int(confidence * 20)\n",
    "        bar     = chr(9608) * filled + chr(9617) * (20 - filled)\n",
    "        marker  = \" ◀\" if confidence > thresh else \"\"\n",
    "        print(f\"  {name:<18} {bar} {confidence:.2f}   {thresh:.2f}   {prediction}{marker}\")\n",
    "\n",
    "    # Show the image\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(3, 3))\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(f\"img_id={row['img_id']}\")\n",
    "    ax.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\n✓ Inference demo complete\")\n",
    "print(\"  These structured scores feed into the Gemini/MedGemma orchestrator\")\n",
    "print(\"  to generate empathetic, contextual responses for MamaGuard users.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: Dynamic pos_weight Verification\n",
    "\n",
    "Recompute `pos_weight` from the actual training data to verify the hardcoded constants.\n",
    "Run this cell to double-check if you've modified the dataset or label encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Dynamically compute pos_weight from training labels ──────────────────────\n",
    "train_labels = torch.tensor(train_ds[\"label\"])  # (480, 6)\n",
    "\n",
    "print(f\"Training label tensor shape: {train_labels.shape}\")\n",
    "print(f\"\\n{'Label':<20s} {'Neg':>6} {'Pos':>6} {'Miss':>6} {'pos_weight':>10} {'Hardcoded':>10} {'Match':>6}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "computed_pw = []\n",
    "for i, name in enumerate(LABEL_NAMES):\n",
    "    col = train_labels[:, i]\n",
    "    n_missing = (col == -1).sum().item()\n",
    "    n_pos = (col == 1).sum().item()\n",
    "    n_neg = (col == 0).sum().item()\n",
    "    pw = n_neg / n_pos if n_pos > 0 else float(\"inf\")\n",
    "    computed_pw.append(pw)\n",
    "    hardcoded = POS_WEIGHT[i].item()\n",
    "    match = abs(pw - hardcoded) < 0.05\n",
    "    print(f\"{name:<20s} {n_neg:>6d} {n_pos:>6d} {n_missing:>6d} {pw:>10.2f} {hardcoded:>10.2f} {'✓' if match else '✗':>6}\")\n",
    "\n",
    "print(f\"\\nHardcoded POS_WEIGHT: {POS_WEIGHT.tolist()}\")\n",
    "print(f\"Computed POS_WEIGHT:  {[round(x, 2) for x in computed_pw]}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
