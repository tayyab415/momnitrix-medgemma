{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14879605,"sourceType":"datasetVersion","datasetId":9519693}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# MedGemma 1.5 LoRA Fine-Tuning for Maternal Health Risk Assessment\n\nThis notebook fine-tunes `google/medgemma-1.5-4b-it` on maternal health vitals data using LoRA and pure HuggingFace stack.\n\n**Hardware:** Kaggle T4 x2 GPUs (compute capability 7.5)  \n**Runtime:** ~45-60 minutes  \n**Output:** PEFT LoRA adapter (~50-100MB)\n\n## Key Constraints\n- T4 does NOT support bfloat16 → use `fp16=True`\n- MedGemma 1.5 is a multimodal VLM → use `AutoModelForImageTextToText`\n- Vision encoder is stripped for VRAM savings during training\n- LoRA adapter is restored onto full model at inference","metadata":{}},{"cell_type":"markdown","source":"## Cell 1: Install Dependencies\n\nInstall required packages with pinned versions for reproducibility.","metadata":{}},{"cell_type":"code","source":"# Install dependencies with versions validated for Kaggle T4\n!pip install -q \\\n    transformers>=4.50.0 \\\n    peft>=0.13.0 \\\n    trl>=0.12.0 \\\n    bitsandbytes>=0.44.0 \\\n    datasets \\\n    accelerate \\\n    huggingface_hub\n\nprint(\"\\n✅ Dependencies installed successfully\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-18T16:45:01.675476Z","iopub.execute_input":"2026-02-18T16:45:01.675678Z","iopub.status.idle":"2026-02-18T16:45:09.698874Z","shell.execute_reply.started":"2026-02-18T16:45:01.675658Z","shell.execute_reply":"2026-02-18T16:45:09.698005Z"}},"outputs":[{"name":"stdout","text":"\n✅ Dependencies installed successfully\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## Cell 2: Authentication & Configuration\n\nLoad HF_TOKEN from Kaggle secrets, configure GPU memory, and set hyperparameters.","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nfrom huggingface_hub import login\n\n# ============================================================================\n# KAGGLE SECRETS: Add your HuggingFace token in Kaggle Secrets panel\n# Secrets name: HF_TOKEN\n# ============================================================================\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nHF_TOKEN = user_secrets.get_secret(\"HF_TOKEN\")\n\n# Authenticate with HuggingFace\nlogin(token=HF_TOKEN)\n\n# GPU memory optimization for T4\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n\n# Print GPU info for verification\nprint(\"=\" * 60)\nprint(\"GPU INFORMATION\")\nprint(\"=\" * 60)\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nprint(f\"CUDA version: {torch.version.cuda}\")\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"GPU count: {torch.cuda.device_count()}\")\nfor i in range(torch.cuda.device_count()):\n    props = torch.cuda.get_device_properties(i)\n    print(f\"\\nGPU {i}: {torch.cuda.get_device_name(i)}\")\n    print(f\"  Compute Capability: {props.major}.{props.minor}\")\n    print(f\"  Total VRAM: {props.total_memory / 1e9:.2f} GB\")\n    print(f\"  Supports bf16: {props.major >= 8}\")  # T4 is 7.5 → NO bf16\n\n# ============================================================================\n# HYPERPARAMETERS - All in one place for easy tuning\n# ============================================================================\nCONFIG = {\n    # Model\n    \"model_name\": \"google/medgemma-1.5-4b-it\",\n    \"max_seq_length\": 2048,\n    \n    # Quantization (4-bit)\n    \"load_in_4bit\": True,\n    \"bnb_4bit_use_double_quant\": True,\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_compute_dtype\": \"float16\",  # NOT bfloat16 — T4 lacks bf16 support\n    \n    # LoRA\n    \"lora_r\": 16,\n    \"lora_alpha\": 16,\n    \"lora_dropout\": 0.05,\n    \"lora_target_modules\": \"all-linear\",\n    \n    # Training\n    \"num_train_epochs\": 3,\n    \"per_device_train_batch_size\": 2,\n    \"per_device_eval_batch_size\": 2,\n    \"gradient_accumulation_steps\": 4,  # effective batch = 8\n    \"learning_rate\": 2e-4,\n    \"lr_scheduler_type\": \"cosine\",\n    \"warmup_ratio\": 0.05,\n    \"weight_decay\": 0.01,\n    \"max_grad_norm\": 0.3,\n    \n    # Evaluation & Saving\n    \"eval_strategy\": \"steps\",\n    \"eval_steps\": 50,\n    \"save_strategy\": \"epoch\",\n    \"save_total_limit\": 2,\n    \"logging_steps\": 10,\n    \n    # Hub\n    \"hub_model_id\": \"tyb343/mamaguard-vitals-lora\",  # CHANGE THIS\n    \"output_dir\": \"medgemma-mamaguard-lora\",\n}\n\nprint(\"\\n=\" * 60)\nprint(\"CONFIGURATION\")\nprint(\"=\" * 60)\nfor key, value in CONFIG.items():\n    print(f\"  {key}: {value}\")\nprint(\"\\n⚠️  IMPORTANT: Change 'hub_model_id' to your HF username before running!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-18T16:45:17.030978Z","iopub.execute_input":"2026-02-18T16:45:17.031723Z","iopub.status.idle":"2026-02-18T16:45:24.613768Z","shell.execute_reply.started":"2026-02-18T16:45:17.031686Z","shell.execute_reply":"2026-02-18T16:45:24.612956Z"}},"outputs":[{"name":"stdout","text":"============================================================\nGPU INFORMATION\n============================================================\nCUDA available: True\nCUDA version: 12.6\nPyTorch version: 2.8.0+cu126\nGPU count: 2\n\nGPU 0: Tesla T4\n  Compute Capability: 7.5\n  Total VRAM: 15.64 GB\n  Supports bf16: False\n\nGPU 1: Tesla T4\n  Compute Capability: 7.5\n  Total VRAM: 15.64 GB\n  Supports bf16: False\n\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\nCONFIGURATION\n============================================================\n  model_name: google/medgemma-1.5-4b-it\n  max_seq_length: 2048\n  load_in_4bit: True\n  bnb_4bit_use_double_quant: True\n  bnb_4bit_quant_type: nf4\n  bnb_4bit_compute_dtype: float16\n  lora_r: 16\n  lora_alpha: 16\n  lora_dropout: 0.05\n  lora_target_modules: all-linear\n  num_train_epochs: 3\n  per_device_train_batch_size: 2\n  per_device_eval_batch_size: 2\n  gradient_accumulation_steps: 4\n  learning_rate: 0.0002\n  lr_scheduler_type: cosine\n  warmup_ratio: 0.05\n  weight_decay: 0.01\n  max_grad_norm: 0.3\n  eval_strategy: steps\n  eval_steps: 50\n  save_strategy: epoch\n  save_total_limit: 2\n  logging_steps: 10\n  hub_model_id: tyb343/mamaguard-vitals-lora\n  output_dir: medgemma-mamaguard-lora\n\n⚠️  IMPORTANT: Change 'hub_model_id' to your HF username before running!\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## Cell 3: Load Model with 4-bit Quantization\n\nLoad MedGemma 1.5 using `AutoModelForImageTextToText` with BitsAndBytes 4-bit quantization.  \nKey: Use `torch.float16` compute dtype (NOT bfloat16) — T4 doesn't support bf16.","metadata":{}},{"cell_type":"code","source":"import gc\nfrom transformers import (\n    AutoProcessor,\n    AutoModelForImageTextToText,\n    BitsAndBytesConfig,\n)\n\nprint(\"=\" * 60)\nprint(\"LOADING MODEL\")\nprint(\"=\" * 60)\n\n# Track VRAM before loading\ntorch.cuda.empty_cache()\ngc.collect()\nvram_before = torch.cuda.memory_allocated() / 1e9\nprint(f\"VRAM before model load: {vram_before:.2f} GB\")\n\n# 4-bit quantization config - CRITICAL: use float16 for T4\nquant_config = BitsAndBytesConfig(\n    load_in_4bit=CONFIG[\"load_in_4bit\"],\n    bnb_4bit_use_double_quant=CONFIG[\"bnb_4bit_use_double_quant\"],\n    bnb_4bit_quant_type=CONFIG[\"bnb_4bit_quant_type\"],\n    bnb_4bit_compute_dtype=torch.float16,  # MUST be float16 for T4\n)\n\n# Load model with AutoModelForImageTextToText (NOT AutoModelForCausalLM)\nmodel = AutoModelForImageTextToText.from_pretrained(\n    CONFIG[\"model_name\"],\n    quantization_config=quant_config,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n    attn_implementation=\"eager\",  # \"flash_attention_2\" can be tried but eager is safer\n    token=HF_TOKEN,\n)\n\n# Load processor (includes tokenizer + vision preprocessor)\nprocessor = AutoProcessor.from_pretrained(CONFIG[\"model_name\"], token=HF_TOKEN)\nprocessor.tokenizer.padding_side = \"right\"  # Required for training\n\n# Disable KV cache — required for gradient checkpointing compatibility\nmodel.config.use_cache = False\n\n# Print VRAM after loading\nvram_after_load = torch.cuda.memory_allocated() / 1e9\nprint(f\"VRAM after model load: {vram_after_load:.2f} GB\")\nprint(f\"VRAM used by model: {vram_after_load - vram_before:.2f} GB\")\n\nprint(\"\\n✅ Model and processor loaded successfully\")\nprint(f\"   Model type: {type(model).__name__}\")\nprint(f\"   Pad token: {processor.tokenizer.pad_token}\")\nprint(f\"   EOS token: {processor.tokenizer.eos_token}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-18T16:45:39.601132Z","iopub.execute_input":"2026-02-18T16:45:39.601942Z","iopub.status.idle":"2026-02-18T16:47:44.097463Z","shell.execute_reply.started":"2026-02-18T16:45:39.601913Z","shell.execute_reply":"2026-02-18T16:47:44.096654Z"}},"outputs":[{"name":"stderr","text":"2026-02-18 16:45:53.332803: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1771433153.770254      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1771433153.894800      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1771433154.832358      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1771433154.832397      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1771433154.832399      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1771433154.832401      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"name":"stdout","text":"============================================================\nLOADING MODEL\n============================================================\nVRAM before model load: 0.00 GB\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/2.55k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"75e6ee899f1a404bbebf359bdc5d1bf9"}},"metadata":{}},{"name":"stderr","text":"`torch_dtype` is deprecated! Use `dtype` instead!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/90.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f1d16189dd6435cb70da1dc27c8dd42"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c52f3e087cd496c9d7612c83a4dea2f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb9bdc44573d4c17919e930565a30972"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/3.64G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e64e4606545f4385849a8efc17cd2642"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"62a199093f8e4f9abd5bbfd818335ab7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/115 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2710be804e324466a5a3f9c2cc592399"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"processor_config.json:   0%|          | 0.00/70.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9900d51d95484a469b0a8d70bff9395b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"chat_template.jinja:   0%|          | 0.00/1.53k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f15a0fb35c0a45de830fd221f0da3e22"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d1a04bd5a864bdbb9ce3f968c5c2d9c"}},"metadata":{}},{"name":"stderr","text":"Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.16M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c74f4422be84ece839f95cbbc49c3e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.69M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f878bbfee0b24f9b941dc2cf3a0ad504"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f9e8a6904ba4fea81aec931e60b5908"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/35.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5165af2903c420cae5cf39c39f69ad7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/662 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e116bb126a02419092fc9b5063bcd29c"}},"metadata":{}},{"name":"stdout","text":"VRAM after model load: 0.15 GB\nVRAM used by model: 0.15 GB\n\n✅ Model and processor loaded successfully\n   Model type: Gemma3ForConditionalGeneration\n   Pad token: <pad>\n   EOS token: <eos>\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"### Strip Vision Encoder for VRAM Optimization\n\nSince we're training on text-only data, remove the vision encoder to save ~1-2GB VRAM.  \nThe vision encoder will be restored when loading the adapter onto the full base model at inference.","metadata":{}},{"cell_type":"code","source":"print(\"=\" * 60)\nprint(\"STRIPPING VISION ENCODER\")\nprint(\"=\" * 60)\n\nvram_before_strip = torch.cuda.memory_allocated() / 1e9\nprint(f\"VRAM before stripping: {vram_before_strip:.2f} GB\")\n\n# Set vision tower and multimodal projector to None to save VRAM\n# Note: setattr to None instead of delattr (properties may not have deleters)\nfor attr in [\"vision_tower\", \"multi_modal_projector\"]:\n    for parent in [model, getattr(model, \"model\", None)]:\n        if parent and hasattr(parent, attr):\n            try:\n                setattr(parent, attr, None)\n                print(f\"   Set {attr} to None\")\n            except AttributeError as e:\n                print(f\"   Note: {attr} could not be set to None ({e})\")\n# Force garbage collection\ngc.collect()\ntorch.cuda.empty_cache()\n\nvram_after_strip = torch.cuda.memory_allocated() / 1e9\nsaved = vram_before_strip - vram_after_strip\nprint(f\"\\nVRAM after stripping: {vram_after_strip:.2f} GB\")\nprint(f\"VRAM saved: {saved:.2f} GB\")\nprint(\"\\n✅ Vision encoder stripped for text-only training\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-18T16:47:53.127456Z","iopub.execute_input":"2026-02-18T16:47:53.128593Z","iopub.status.idle":"2026-02-18T16:47:53.521816Z","shell.execute_reply.started":"2026-02-18T16:47:53.128536Z","shell.execute_reply":"2026-02-18T16:47:53.521003Z"}},"outputs":[{"name":"stdout","text":"============================================================\nSTRIPPING VISION ENCODER\n============================================================\nVRAM before stripping: 0.15 GB\n   Note: vision_tower could not be set to None (property 'vision_tower' of 'Gemma3ForConditionalGeneration' object has no setter)\n   Set vision_tower to None\n   Note: multi_modal_projector could not be set to None (property 'multi_modal_projector' of 'Gemma3ForConditionalGeneration' object has no setter)\n   Set multi_modal_projector to None\n\nVRAM after stripping: 0.00 GB\nVRAM saved: 0.15 GB\n\n✅ Vision encoder stripped for text-only training\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"### Apply SiglipVisionTransformer Workaround\n\nMedGemma 1.5 has a known issue where `get_input_embeddings` on the SigLIP encoder causes errors.  \nApply this monkey-patch BEFORE creating the SFTTrainer.","metadata":{}},{"cell_type":"code","source":"from transformers.models.siglip.modeling_siglip import SiglipVisionTransformer\n\n# Monkey-patch to prevent get_input_embeddings error during training\nSiglipVisionTransformer.get_input_embeddings = lambda self: None\n\nprint(\"✅ Applied SiglipVisionTransformer workaround\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-18T16:47:58.399562Z","iopub.execute_input":"2026-02-18T16:47:58.400105Z","iopub.status.idle":"2026-02-18T16:47:58.404157Z","shell.execute_reply.started":"2026-02-18T16:47:58.400074Z","shell.execute_reply":"2026-02-18T16:47:58.403543Z"}},"outputs":[{"name":"stdout","text":"✅ Applied SiglipVisionTransformer workaround\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Force model config to FP16 to prevent any BF16 creep\nprint(\"=\" * 60)\nprint(\"ENSURING FP16 DType\")\nprint(\"=\" * 60)\n\n# Set model config dtype explicitly\nmodel.config.torch_dtype = torch.float16\n\n# Check for any BF16 params\nbf16_count = 0\nfor name, param in model.named_parameters():\n    if param.dtype == torch.bfloat16:\n        bf16_count += 1\n        print(f\"⚠️  Found BF16 param: {name}\")\n\nif bf16_count == 0:\n    print(\"✅ No BF16 parameters found\")\nelse:\n    print(f\"⚠️  Found {bf16_count} BF16 parameters - they should be handled by fp16=True\")\n\nprint(\"✅ Model dtype configuration complete\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-18T16:48:01.290187Z","iopub.execute_input":"2026-02-18T16:48:01.290762Z","iopub.status.idle":"2026-02-18T16:48:01.298336Z","shell.execute_reply.started":"2026-02-18T16:48:01.290732Z","shell.execute_reply":"2026-02-18T16:48:01.297554Z"}},"outputs":[{"name":"stdout","text":"============================================================\nENSURING FP16 DType\n============================================================\n✅ No BF16 parameters found\n✅ Model dtype configuration complete\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"## Cell 4: Load Dataset\n\nLoad the maternal health datasets from JSONL files. The data is already in `messages` format  \ncompatible with chat templating.","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\n\nprint(\"=\" * 60)\nprint(\"LOADING DATASETS\")\nprint(\"=\" * 60)\n\n# Load datasets from JSONL files\n# Assumes mamaguard_train.jsonl and mamaguard_eval.jsonl are uploaded as Kaggle datasets\ntrain_ds = load_dataset(\"json\", data_files=\"/kaggle/input/datasets/kkfkmf/momnitrix-finetuning-1/mamaguard_eval.jsonl\", split=\"train\")\neval_ds = load_dataset(\"json\", data_files=\"/kaggle/input/datasets/kkfkmf/momnitrix-finetuning-1/mamaguard_eval.jsonl\", split=\"train\")\n\nprint(f\"Train samples: {len(train_ds)}\")\nprint(f\"Eval samples: {len(eval_ds)}\")\n\n# Verify format\nprint(\"\\n=\" * 60)\nprint(\"SAMPLE VERIFICATION\")\nprint(\"=\" * 60)\nsample = train_ds[0]\nprint(f\"Keys: {list(sample.keys())}\")\nprint(f\"\\nMessages format: {type(sample['messages'])}\")\nprint(f\"Number of messages: {len(sample['messages'])}\")\nprint(f\"\\nFirst message role: {sample['messages'][0]['role']}\")\nprint(f\"First message content length: {len(sample['messages'][0]['content'])} chars\")\nprint(f\"\\nSecond message role: {sample['messages'][1]['role']}\")\nprint(f\"Second message content length: {len(sample['messages'][1]['content'])} chars\")\n\nprint(\"\\n✅ Datasets loaded and verified\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-18T16:48:09.389375Z","iopub.execute_input":"2026-02-18T16:48:09.390017Z","iopub.status.idle":"2026-02-18T16:48:11.612902Z","shell.execute_reply.started":"2026-02-18T16:48:09.389987Z","shell.execute_reply":"2026-02-18T16:48:11.612321Z"}},"outputs":[{"name":"stdout","text":"============================================================\nLOADING DATASETS\n============================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1de4355afe174c24b73cbacff5452a50"}},"metadata":{}},{"name":"stdout","text":"Train samples: 102\nEval samples: 102\n\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\n=\nSAMPLE VERIFICATION\n============================================================\nKeys: ['messages']\n\nMessages format: <class 'list'>\nNumber of messages: 2\n\nFirst message role: user\nFirst message content length: 585 chars\n\nSecond message role: assistant\nSecond message content length: 1717 chars\n\n✅ Datasets loaded and verified\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"## Cell 5: Define Custom Collate Function\n\nSince we're using `AutoModelForImageTextToText` (a VLM) with text-only data, we need a custom  \ncollate function that:\n1. Applies the chat template via `processor.apply_chat_template()`\n2. Processes text through the processor (no images, pass `text=` only)\n3. Creates labels from input_ids with proper masking (pad tokens = -100)","metadata":{}},{"cell_type":"code","source":"def collate_fn(examples):\n    \"\"\"\n    Custom collate function for text-only training with VLM processor.\n    \n    Args:\n        examples: List of examples from dataset (each has 'messages' key)\n    \n    Returns:\n        Batch dict with input_ids, attention_mask, and labels\n    \"\"\"\n    # Apply chat template to each example's messages\n    texts = []\n    for example in examples:\n        formatted = processor.apply_chat_template(\n            example[\"messages\"],\n            add_generation_prompt=False,  # False for training (assistant already responded)\n            tokenize=False\n        )\n        texts.append(formatted.strip())\n    \n    # Process through processor (text-only, no images)\n    batch = processor(\n        text=texts,\n        return_tensors=\"pt\",\n        padding=True,\n        truncation=True,\n        max_length=CONFIG[\"max_seq_length\"],\n    )\n    \n    # Create labels from input_ids\n    labels = batch[\"input_ids\"].clone()\n    \n    # Mask pad tokens with -100 (ignored in loss calculation)\n    labels[labels == processor.tokenizer.pad_token_id] = -100\n    \n    # Note: MedGemma uses specific image tokens - we don't need to mask them\n    # since we're training on text-only data (no images in the examples)\n    \n    batch[\"labels\"] = labels\n    return batch\n\n\n# Test the collate function\nprint(\"=\" * 60)\nprint(\"TESTING COLLATE FUNCTION\")\nprint(\"=\" * 60)\n\ntest_batch = collate_fn([train_ds[0], train_ds[1]])\nprint(f\"Batch keys: {list(test_batch.keys())}\")\nprint(f\"Input IDs shape: {test_batch['input_ids'].shape}\")\nprint(f\"Attention mask shape: {test_batch['attention_mask'].shape}\")\nprint(f\"Labels shape: {test_batch['labels'].shape}\")\n\n# Count non-pad tokens\nnon_pad_tokens = (test_batch[\"labels\"] != -100).sum().item()\ntotal_tokens = test_batch[\"labels\"].numel()\nprint(f\"\\nNon-pad tokens: {non_pad_tokens} / {total_tokens}\")\nprint(f\"Pad token percentage: {(total_tokens - non_pad_tokens) / total_tokens * 100:.1f}%\")\n\nprint(\"\\n✅ Collate function working correctly\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-18T16:48:25.067470Z","iopub.execute_input":"2026-02-18T16:48:25.068636Z","iopub.status.idle":"2026-02-18T16:48:25.108896Z","shell.execute_reply.started":"2026-02-18T16:48:25.068599Z","shell.execute_reply":"2026-02-18T16:48:25.108155Z"}},"outputs":[{"name":"stdout","text":"============================================================\nTESTING COLLATE FUNCTION\n============================================================\nBatch keys: ['input_ids', 'attention_mask', 'token_type_ids', 'labels']\nInput IDs shape: torch.Size([2, 530])\nAttention mask shape: torch.Size([2, 530])\nLabels shape: torch.Size([2, 530])\n\nNon-pad tokens: 1057 / 1060\nPad token percentage: 0.3%\n\n✅ Collate function working correctly\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"## Cell 6: Configure LoRA + SFTTrainer\n\nSet up LoRA configuration targeting all linear layers, then create the SFTTrainer.  \nNote: Do NOT call `prepare_model_for_kbit_training()` or `get_peft_model()` manually —  \npass `peft_config` directly to SFTTrainer and it handles everything.","metadata":{}},{"cell_type":"code","source":"from peft import LoraConfig\nfrom trl import SFTConfig, SFTTrainer\n\nprint(\"=\" * 60)\nprint(\"CONFIGURING LoRA\")\nprint(\"=\" * 60)\n\n# LoRA configuration\npeft_config = LoraConfig(\n    r=CONFIG[\"lora_r\"],\n    lora_alpha=CONFIG[\"lora_alpha\"],\n    lora_dropout=CONFIG[\"lora_dropout\"],\n    bias=\"none\",\n    target_modules=CONFIG[\"lora_target_modules\"],\n    task_type=\"CAUSAL_LM\",\n)\n\nprint(f\"LoRA rank (r): {peft_config.r}\")\nprint(f\"LoRA alpha: {peft_config.lora_alpha}\")\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"CONFIGURING SFTTrainer\")\nprint(\"=\" * 60)\n\n# Training args - use fp32 for LoRA (4-bit base model saves memory)\ntraining_args = SFTConfig(\n    output_dir=CONFIG[\"output_dir\"],\n    num_train_epochs=CONFIG[\"num_train_epochs\"],\n    per_device_train_batch_size=CONFIG[\"per_device_train_batch_size\"],\n    per_device_eval_batch_size=CONFIG[\"per_device_eval_batch_size\"],\n    gradient_accumulation_steps=CONFIG[\"gradient_accumulation_steps\"],\n    gradient_checkpointing=True,\n    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n    optim=\"adamw_torch_fused\",\n    learning_rate=CONFIG[\"learning_rate\"],\n    lr_scheduler_type=CONFIG[\"lr_scheduler_type\"],\n    warmup_ratio=CONFIG[\"warmup_ratio\"],\n    weight_decay=CONFIG[\"weight_decay\"],\n    # DISABLE fp16/bf16 - 4-bit quantization is enough for memory\n    # LoRA adapters are small and can train in FP32\n    fp16=False,\n    bf16=False,\n    dataloader_num_workers=0,\n    logging_steps=CONFIG[\"logging_steps\"],\n    eval_strategy=CONFIG[\"eval_strategy\"],\n    eval_steps=CONFIG[\"eval_steps\"],\n    save_strategy=CONFIG[\"save_strategy\"],\n    save_total_limit=CONFIG[\"save_total_limit\"],\n    max_grad_norm=CONFIG[\"max_grad_norm\"],\n    report_to=\"none\",\n    seed=42,\n    dataset_kwargs={\"skip_prepare_dataset\": True},\n    remove_unused_columns=False,\n)\n\nprint(f\"Training epochs: {training_args.num_train_epochs}\")\nprint(f\"Batch size: {training_args.per_device_train_batch_size}\")\nprint(f\"fp16: {training_args.fp16}\")\nprint(f\"bf16: {training_args.bf16}\")\nprint(\"Note: Training LoRA in FP32 (4-bit base model saves memory)\")\n\ntrainer = SFTTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_ds,\n    eval_dataset=eval_ds,\n    peft_config=peft_config,\n    processing_class=processor,\n    data_collator=collate_fn,\n)\n\nprint(\"\\n✅ SFTTrainer created successfully\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-18T16:48:29.439377Z","iopub.execute_input":"2026-02-18T16:48:29.439781Z","iopub.status.idle":"2026-02-18T16:48:35.309871Z","shell.execute_reply.started":"2026-02-18T16:48:29.439753Z","shell.execute_reply":"2026-02-18T16:48:35.309222Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"============================================================\nCONFIGURING LoRA\n============================================================\nLoRA rank (r): 16\nLoRA alpha: 16\n\n============================================================\nCONFIGURING SFTTrainer\n============================================================\nTraining epochs: 3\nBatch size: 2\nfp16: False\nbf16: False\nNote: Training LoRA in FP32 (4-bit base model saves memory)\n\n✅ SFTTrainer created successfully\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"## Cell 7: Train\n\nRun training for 3 epochs. Expected time: ~45-60 minutes on T4 x2.","metadata":{}},{"cell_type":"code","source":"# Cast all BF16 parameters to FP16 to prevent T4 crash\nprint(\"=\" * 60)\nprint(\"FIXING BF16 PARAMETERS FOR T4 COMPATIBILITY\")\nprint(\"=\" * 60)\n\nbf16_count = 0\nfor name, param in model.named_parameters():\n    if param.dtype == torch.bfloat16:\n        param.data = param.data.to(torch.float16)\n        bf16_count += 1\n\nprint(f\"Converted {bf16_count} BF16 parameters to FP16\")\n\n# Verify no BF16 remains\nremaining_bf16 = sum(1 for p in model.parameters() if p.dtype == torch.bfloat16)\nprint(f\"Remaining BF16 parameters: {remaining_bf16}\")\n\nif remaining_bf16 == 0:\n    print(\"✅ All parameters now FP16 - ready for T4 training\")\nelse:\n    print(f\"⚠️  Warning: {remaining_bf16} BF16 parameters still exist\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-18T16:48:38.649823Z","iopub.execute_input":"2026-02-18T16:48:38.650384Z","iopub.status.idle":"2026-02-18T16:48:38.679894Z","shell.execute_reply.started":"2026-02-18T16:48:38.650354Z","shell.execute_reply":"2026-02-18T16:48:38.679221Z"}},"outputs":[{"name":"stdout","text":"============================================================\nFIXING BF16 PARAMETERS FOR T4 COMPATIBILITY\n============================================================\nConverted 476 BF16 parameters to FP16\nRemaining BF16 parameters: 0\n✅ All parameters now FP16 - ready for T4 training\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"import time\n\nprint(\"=\" * 60)\nprint(\"STARTING TRAINING\")\nprint(\"=\" * 60)\n\n# Log VRAM before training\nvram_before_train = torch.cuda.memory_allocated() / 1e9\nprint(f\"VRAM before training: {vram_before_train:.2f} GB\")\nprint(f\"Estimated training time: 45-60 minutes on T4 x2\\n\")\n\nstart_time = time.time()\n\n# Train!\ntrainer.train()\n\n# Calculate elapsed time\nelapsed = time.time() - start_time\nminutes = int(elapsed // 60)\nseconds = int(elapsed % 60)\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"TRAINING COMPLETE\")\nprint(\"=\" * 60)\nprint(f\"Total training time: {minutes}m {seconds}s\")\n\n# Show final metrics\nif trainer.state.log_history:\n    final_loss = None\n    for entry in reversed(trainer.state.log_history):\n        if \"loss\" in entry:\n            final_loss = entry[\"loss\"]\n            break\n    if final_loss:\n        print(f\"Final training loss: {final_loss:.4f}\")\n    \n    final_eval_loss = None\n    for entry in reversed(trainer.state.log_history):\n        if \"eval_loss\" in entry:\n            final_eval_loss = entry[\"eval_loss\"]\n            break\n    if final_eval_loss:\n        print(f\"Final eval loss: {final_eval_loss:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-18T16:48:42.387800Z","iopub.execute_input":"2026-02-18T16:48:42.388600Z","iopub.status.idle":"2026-02-18T16:54:30.928242Z","shell.execute_reply.started":"2026-02-18T16:48:42.388543Z","shell.execute_reply":"2026-02-18T16:54:30.927634Z"}},"outputs":[{"name":"stderr","text":"The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 1, 'bos_token_id': 2, 'pad_token_id': 0}.\n","output_type":"stream"},{"name":"stdout","text":"============================================================\nSTARTING TRAINING\n============================================================\nVRAM before training: 0.00 GB\nEstimated training time: 45-60 minutes on T4 x2\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='39' max='39' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [39/39 05:38, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"\n============================================================\nTRAINING COMPLETE\n============================================================\nTotal training time: 5m 48s\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"## Cell 8: Save & Push to Hub\n\nSave the LoRA adapter locally and push to HuggingFace Hub. The adapter is only ~50-100MB  \n(not the full 4B model).","metadata":{}},{"cell_type":"code","source":"import shutil\n\nprint(\"=\" * 60)\nprint(\"SAVING ADAPTER\")\nprint(\"=\" * 60)\n\n# Save the LoRA adapter (not the full model)\ntrainer.save_model()\n\n# Save processor (tokenizer + config)\nprocessor.save_pretrained(CONFIG[\"output_dir\"])\n\n# Calculate adapter size\ntotal_size = 0\nfor dirpath, dirnames, filenames in os.walk(CONFIG[\"output_dir\"]):\n    for f in filenames:\n        fp = os.path.join(dirpath, f)\n        total_size += os.path.getsize(fp)\n\nsize_mb = total_size / (1024 * 1024)\nprint(f\"Adapter saved to: {CONFIG['output_dir']}/\")\nprint(f\"Total adapter size: {size_mb:.2f} MB\")\n\n# List saved files\nprint(\"\\nSaved files:\")\nfor f in sorted(os.listdir(CONFIG[\"output_dir\"])):\n    file_path = os.path.join(CONFIG[\"output_dir\"], f)\n    if os.path.isfile(file_path):\n        file_size = os.path.getsize(file_path) / 1024\n        print(f\"  {f}: {file_size:.1f} KB\")\n\nprint(\"\\n=\" * 60)\nprint(\"PUSHING TO HUB\")\nprint(\"=\" * 60)\n\n# Push to Hub (update hub_model_id in Cell 2 first!)\nif \"your-username\" not in CONFIG[\"hub_model_id\"]:\n    try:\n        from huggingface_hub import HfApi\n        api = HfApi()\n        api.create_repo(repo_id=CONFIG[\"hub_model_id\"], exist_ok=True)\n        api.upload_folder(\n            folder_path=CONFIG[\"output_dir\"],\n            repo_id=CONFIG[\"hub_model_id\"],\n            commit_message=\"Upload MamaGuard LoRA adapter\",\n        )\n        print(f\"✅ Adapter pushed to: https://huggingface.co/{CONFIG['hub_model_id']}\")\n    except Exception as e:\n        print(f\"❌ Failed to push to hub: {e}\")\nelse:\n    print(\"⚠️  Skipping hub push - please update 'hub_model_id' in CONFIG first!\")\n    print(f\"   Current value: {CONFIG['hub_model_id']}\")\n    print(f\"   Set to: your-actual-username/mamaguard-vitals-lora\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Cell 9: Test Inference (Base vs Fine-Tuned)\n\nCompare the base model (LoRA disabled) vs fine-tuned model (LoRA enabled) on a sample case.  \nThis demonstrates that the adapter is working and has learned maternal health patterns.","metadata":{}},{"cell_type":"code","source":"print(\"=\" * 60)\nprint(\"TEST INFERENCE: Base vs Fine-Tuned\")\nprint(\"=\" * 60)\n\n# Test case with high-risk vitals\ntest_messages = [\n    {\n        \"role\": \"user\",\n        \"content\": \"\"\"Evaluate the following pregnancy vitals and determine risk level:\n\nPatient profile:\n- 35 years old, G1P0\n- Gestation: week 32 (3rd trimester)\n- BMI group: overweight\n\nMonitoring data (smartwatch + app logs):\n- BP: 145/95 mmHg\n- Fasting glucose: 8.5 mmol/L\n- Body temp: 98.2°F\n- Resting pulse: 88 bpm\n\nPlease return:\n1) LOW/MID/HIGH risk classification\n2) Clinical interpretation tied to threshold values\n3) Likely maternal-fetal complications\n4) Week-32 appropriate management actions\n5) Immediate red-flag symptoms\"\"\"\n    }\n]\n\n# Format with chat template\ntest_text = processor.apply_chat_template(\n    test_messages,\n    add_generation_prompt=True,\n    tokenize=False\n).strip()\n\nprint(\"TEST PROMPT:\")\nprint(\"-\" * 60)\nprint(test_text[:500] + \"...\" if len(test_text) > 500 else test_text)\nprint(\"-\" * 60)\n\n# Tokenize\ninputs = processor(\n    text=[test_text],\n    return_tensors=\"pt\",\n    padding=True,\n).to(model.device)\n\n# Generation config\ngen_kwargs = {\n    \"max_new_tokens\": 512,\n    \"do_sample\": True,\n    \"temperature\": 0.7,\n    \"top_p\": 0.9,\n    \"eos_token_id\": processor.tokenizer.eos_token_id,\n}\n\n# ============================================================================\n# Test 1: WITH LoRA (fine-tuned)\n# ============================================================================\nprint(\"\\n\" + \"=\" * 60)\nprint(\"TEST 1: WITH LoRA ADAPTER (Fine-Tuned)\")\nprint(\"=\" * 60)\n\nmodel.eval()\nwith torch.no_grad():\n    outputs_ft = model.generate(**inputs, **gen_kwargs)\n\n# Decode only the new tokens\ngenerated_ft = outputs_ft[0][inputs[\"input_ids\"].shape[1]:]\nresponse_ft = processor.decode(generated_ft, skip_special_tokens=True)\n\nprint(\"Response:\")\nprint(response_ft[:800] + \"...\" if len(response_ft) > 800 else response_ft)\nprint(\"\\n\")\n\n# Try to extract risk level from response\nif \"HIGH\" in response_ft[:100]:\n    risk_pred = \"HIGH\"\nelif \"MID\" in response_ft[:100] or \"MEDIUM\" in response_ft[:100]:\n    risk_pred = \"MID\"\nelif \"LOW\" in response_ft[:100]:\n    risk_pred = \"LOW\"\nelse:\n    risk_pred = \"UNKNOWN\"\n\nprint(f\"Predicted Risk Level: {risk_pred}\")\n\n# ============================================================================\n# Summary\n# ============================================================================\nprint(\"\\n\" + \"=\" * 60)\nprint(\"SUMMARY\")\nprint(\"=\" * 60)\nprint(f\"With LoRA: {risk_pred} risk predicted\")\nprint(\"\\nExpected: HIGH risk (BP 145/95 > 140/90, Glucose 8.5 > 5.1)\")\nprint(\"\\n✅ Inference complete!\")\nprint(\"\\nNote: To compare with base model (LoRA disabled), reload the model\")\nprint(\"without the adapter or use: model.disable_adapter() if available.\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n\n## Export Strategy\n\nThe output is a **PEFT LoRA adapter** (~50-100MB), NOT a full model. At inference:\n\n```python\nfrom transformers import AutoModelForImageTextToText, AutoProcessor\nfrom peft import PeftModel\n\n# Load full base model (with vision encoder intact)\nbase = AutoModelForImageTextToText.from_pretrained(\n    \"google/medgemma-1.5-4b-it\", ...\n)\n\n# Apply LoRA adapter\nmodel = PeftModel.from_pretrained(base, \"your-username/mamaguard-vitals-lora\")\n\n# Now you have:\n# - Vision encoder intact → multimodal inference works\n# - LoRA adapter active → maternal health expertise applied\n```\n\n## Training Summary\n\n| Metric | Value |\n|--------|-------|\n| Model | google/medgemma-1.5-4b-it |\n| Method | LoRA (r=16, α=16) |\n| Quantization | 4-bit NF4 |\n| Train samples | 912 |\n| Eval samples | 102 |\n| Epochs | 3 |\n| Batch size | 2 (effective 8) |\n| Learning rate | 2e-4 |\n| Hardware | Kaggle T4 x2 |\n| Runtime | ~45-60 min |\n| Output size | ~50-100 MB |","metadata":{}}]}