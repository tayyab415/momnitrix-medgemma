{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MedGemma 1.5 LoRA Fine-Tuning for Maternal Health Risk Assessment\n",
    "\n",
    "This notebook fine-tunes `google/medgemma-1.5-4b-it` on maternal health vitals data using LoRA and pure HuggingFace stack.\n",
    "\n",
    "**Hardware:** Kaggle T4 x2 GPUs (compute capability 7.5)  \n",
    "**Runtime:** ~45-60 minutes  \n",
    "**Output:** PEFT LoRA adapter (~50-100MB)\n",
    "\n",
    "## Key Constraints\n",
    "- T4 does NOT support bfloat16 \u2192 use `fp16=True`\n",
    "- MedGemma 1.5 is a multimodal VLM \u2192 use `AutoModelForImageTextToText`\n",
    "- Vision encoder is stripped for VRAM savings during training\n",
    "- LoRA adapter is restored onto full model at inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Install Dependencies\n",
    "\n",
    "Install required packages with pinned versions for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies with versions validated for Kaggle T4\n",
    "!pip install -q \\\n",
    "    transformers>=4.50.0 \\\n",
    "    peft>=0.13.0 \\\n",
    "    trl>=0.12.0 \\\n",
    "    bitsandbytes>=0.44.0 \\\n",
    "    datasets \\\n",
    "    accelerate \\\n",
    "    huggingface_hub\n",
    "\n",
    "print(\"\\n\u2705 Dependencies installed successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Authentication & Configuration\n",
    "\n",
    "Load HF_TOKEN from Kaggle secrets, configure GPU memory, and set hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from huggingface_hub import login\n",
    "\n",
    "# ============================================================================\n",
    "# KAGGLE SECRETS: Add your HuggingFace token in Kaggle Secrets panel\n",
    "# Secrets name: HF_TOKEN\n",
    "# ============================================================================\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "HF_TOKEN = user_secrets.get_secret(\"HF_TOKEN\")\n",
    "\n",
    "# Authenticate with HuggingFace\n",
    "login(token=HF_TOKEN)\n",
    "\n",
    "# GPU memory optimization for T4\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# Print GPU info for verification\n",
    "print(\"=\" * 60)\n",
    "print(\"GPU INFORMATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"GPU count: {torch.cuda.device_count()}\")\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    props = torch.cuda.get_device_properties(i)\n",
    "    print(f\"\\nGPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "    print(f\"  Compute Capability: {props.major}.{props.minor}\")\n",
    "    print(f\"  Total VRAM: {props.total_memory / 1e9:.2f} GB\")\n",
    "    print(f\"  Supports bf16: {props.major >= 8}\")  # T4 is 7.5 \u2192 NO bf16\n",
    "\n",
    "# ============================================================================\n",
    "# HYPERPARAMETERS - All in one place for easy tuning\n",
    "# ============================================================================\n",
    "CONFIG = {\n",
    "    # Model\n",
    "    \"model_name\": \"google/medgemma-1.5-4b-it\",\n",
    "    \"max_seq_length\": 2048,\n",
    "    \n",
    "    # Quantization (4-bit)\n",
    "    \"load_in_4bit\": True,\n",
    "    \"bnb_4bit_use_double_quant\": True,\n",
    "    \"bnb_4bit_quant_type\": \"nf4\",\n",
    "    \"bnb_4bit_compute_dtype\": \"torch.float16\",  # NOT bfloat16 for T4!\n",
    "    \n",
    "    # LoRA\n",
    "    \"lora_r\": 16,\n",
    "    \"lora_alpha\": 16,\n",
    "    \"lora_dropout\": 0.05,\n",
    "    \"lora_target_modules\": \"all-linear\",\n",
    "    \n",
    "    # Training\n",
    "    \"num_train_epochs\": 3,\n",
    "    \"per_device_train_batch_size\": 2,\n",
    "    \"per_device_eval_batch_size\": 2,\n",
    "    \"gradient_accumulation_steps\": 4,  # effective batch = 8\n",
    "    \"learning_rate\": 2e-4,\n",
    "    \"lr_scheduler_type\": \"cosine\",\n",
    "    \"warmup_ratio\": 0.05,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"max_grad_norm\": 0.3,\n",
    "    \n",
    "    # Evaluation & Saving\n",
    "    \"eval_strategy\": \"steps\",\n",
    "    \"eval_steps\": 50,\n",
    "    \"save_strategy\": \"epoch\",\n",
    "    \"save_total_limit\": 2,\n",
    "    \"logging_steps\": 10,\n",
    "    \n",
    "    # Hub\n",
    "    \"hub_model_id\": \"your-username/mamaguard-vitals-lora\",  # CHANGE THIS\n",
    "    \"output_dir\": \"medgemma-mamaguard-lora\",\n",
    "}\n",
    "\n",
    "print(\"\\n=\" * 60)\n",
    "print(\"CONFIGURATION\")\n",
    "print(\"=\" * 60)\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "print(\"\\n\u26a0\ufe0f  IMPORTANT: Change 'hub_model_id' to your HF username before running!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Load Model with 4-bit Quantization\n",
    "\n",
    "Load MedGemma 1.5 using `AutoModelForImageTextToText` with BitsAndBytes 4-bit quantization.  \n",
    "Key: Use `torch.float16` compute dtype (NOT bfloat16) \u2014 T4 doesn't support bf16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from transformers import (\n",
    "    AutoProcessor,\n",
    "    AutoModelForImageTextToText,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"LOADING MODEL\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Track VRAM before loading\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "vram_before = torch.cuda.memory_allocated() / 1e9\n",
    "print(f\"VRAM before model load: {vram_before:.2f} GB\")\n",
    "\n",
    "# 4-bit quantization config - CRITICAL: use float16 for T4\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=CONFIG[\"load_in_4bit\"],\n",
    "    bnb_4bit_use_double_quant=CONFIG[\"bnb_4bit_use_double_quant\"],\n",
    "    bnb_4bit_quant_type=CONFIG[\"bnb_4bit_quant_type\"],\n",
    "    bnb_4bit_compute_dtype=torch.float16,  # MUST be float16 for T4\n",
    ")\n",
    "\n",
    "# Load model with AutoModelForImageTextToText (NOT AutoModelForCausalLM)\n",
    "model = AutoModelForImageTextToText.from_pretrained(\n",
    "    CONFIG[\"model_name\"],\n",
    "    quantization_config=quant_config,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"eager\",  # \"flash_attention_2\" can be tried but eager is safer\n",
    "    token=HF_TOKEN,\n",
    ")\n",
    "\n",
    "# Load processor (includes tokenizer + vision preprocessor)\n",
    "processor = AutoProcessor.from_pretrained(CONFIG[\"model_name\"], token=HF_TOKEN)\n",
    "processor.tokenizer.padding_side = \"right\"  # Required for training\n",
    "\n",
    "# Print VRAM after loading\n",
    "vram_after_load = torch.cuda.memory_allocated() / 1e9\n",
    "print(f\"VRAM after model load: {vram_after_load:.2f} GB\")\n",
    "print(f\"VRAM used by model: {vram_after_load - vram_before:.2f} GB\")\n",
    "\n",
    "print(\"\\n\u2705 Model and processor loaded successfully\")\n",
    "print(f\"   Model type: {type(model).__name__}\")\n",
    "print(f\"   Pad token: {processor.tokenizer.pad_token}\")\n",
    "print(f\"   EOS token: {processor.tokenizer.eos_token}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strip Vision Encoder for VRAM Optimization\n",
    "\n",
    "Since we're training on text-only data, remove the vision encoder to save ~1-2GB VRAM.  \n",
    "The vision encoder will be restored when loading the adapter onto the full base model at inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)",
    "print(\"STRIPPING VISION ENCODER\")",
    "print(\"=\" * 60)",
    "",
    "vram_before_strip = torch.cuda.memory_allocated() / 1e9",
    "print(f\"VRAM before stripping: {vram_before_strip:.2f} GB\")",
    "",
    "# Set vision tower and multimodal projector to None to save VRAM",
    "# Note: setattr to None instead of delattr (properties may not have deleters)",
    "for attr in [\"vision_tower\", \"multi_modal_projector\"]:",
    "    for parent in [model, getattr(model, \"model\", None)]:",
    "        if parent and hasattr(parent, attr):",
    "            try:",
    "                setattr(parent, attr, None)",
    "                print(f\"   Set {attr} to None\")",
    "            except AttributeError as e:",
    "                print(f\"   Note: {attr} could not be set to None ({e})\")",
    "",
    "# Force garbage collection",
    "gc.collect()",
    "torch.cuda.empty_cache()",
    "",
    "vram_after_strip = torch.cuda.memory_allocated() / 1e9",
    "saved = vram_before_strip - vram_after_strip",
    "print(f\"\\nVRAM after stripping: {vram_after_strip:.2f} GB\")",
    "print(f\"VRAM saved: {saved:.2f} GB\")",
    "print(\"\\n\u2705 Vision encoder stripped for text-only training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply SiglipVisionTransformer Workaround\n",
    "\n",
    "MedGemma 1.5 has a known issue where `get_input_embeddings` on the SigLIP encoder causes errors.  \n",
    "Apply this monkey-patch BEFORE creating the SFTTrainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.siglip.modeling_siglip import SiglipVisionTransformer\n",
    "\n",
    "# Monkey-patch to prevent get_input_embeddings error during training\n",
    "SiglipVisionTransformer.get_input_embeddings = lambda self: None\n",
    "\n",
    "print(\"\u2705 Applied SiglipVisionTransformer workaround\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: Load Dataset\n",
    "\n",
    "Load the maternal health datasets from JSONL files. The data is already in `messages` format  \n",
    "compatible with chat templating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"LOADING DATASETS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load datasets from JSONL files\n",
    "# Assumes mamaguard_train.jsonl and mamaguard_eval.jsonl are uploaded as Kaggle datasets\n",
    "train_ds = load_dataset(\"json\", data_files=\"/kaggle/input/mamaguard-dataset/mamaguard_train.jsonl\", split=\"train\")\n",
    "eval_ds = load_dataset(\"json\", data_files=\"/kaggle/input/mamaguard-dataset/mamaguard_eval.jsonl\", split=\"train\")\n",
    "\n",
    "print(f\"Train samples: {len(train_ds)}\")\n",
    "print(f\"Eval samples: {len(eval_ds)}\")\n",
    "\n",
    "# Verify format\n",
    "print(\"\\n=\" * 60)\n",
    "print(\"SAMPLE VERIFICATION\")\n",
    "print(\"=\" * 60)\n",
    "sample = train_ds[0]\n",
    "print(f\"Keys: {list(sample.keys())}\")\n",
    "print(f\"\\nMessages format: {type(sample[\"messages\"])}\")\n",
    "print(f\"Number of messages: {len(sample[\"messages\"])}\")\n",
    "print(f\"\\nFirst message role: {sample[\"messages\"][0][\"role\"]}\")\n",
    "print(f\"First message content length: {len(sample[\"messages\"][0][\"content\"])} chars\")\n",
    "print(f\"\\nSecond message role: {sample[\"messages\"][1][\"role\"]}\")\n",
    "print(f\"Second message content length: {len(sample[\"messages\"][1][\"content\"])} chars\")\n",
    "\n",
    "print(\"\\n\u2705 Datasets loaded and verified\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: Define Custom Collate Function\n",
    "\n",
    "Since we're using `AutoModelForImageTextToText` (a VLM) with text-only data, we need a custom  \n",
    "collate function that:\n",
    "1. Applies the chat template via `processor.apply_chat_template()`\n",
    "2. Processes text through the processor (no images, pass `text=` only)\n",
    "3. Creates labels from input_ids with proper masking (pad tokens = -100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(examples):\n",
    "    \"\"\"\n",
    "    Custom collate function for text-only training with VLM processor.\n",
    "    \n",
    "    Args:\n",
    "        examples: List of examples from dataset (each has 'messages' key)\n",
    "    \n",
    "    Returns:\n",
    "        Batch dict with input_ids, attention_mask, and labels\n",
    "    \"\"\"\n",
    "    # Apply chat template to each example's messages\n",
    "    texts = []\n",
    "    for example in examples:\n",
    "        formatted = processor.apply_chat_template(\n",
    "            example[\"messages\"],\n",
    "            add_generation_prompt=False,  # False for training (assistant already responded)\n",
    "            tokenize=False\n",
    "        )\n",
    "        texts.append(formatted.strip())\n",
    "    \n",
    "    # Process through processor (text-only, no images)\n",
    "    batch = processor(\n",
    "        text=texts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=CONFIG[\"max_seq_length\"],\n",
    "    )\n",
    "    \n",
    "    # Create labels from input_ids\n",
    "    labels = batch[\"input_ids\"].clone()\n",
    "    \n",
    "    # Mask pad tokens with -100 (ignored in loss calculation)\n",
    "    labels[labels == processor.tokenizer.pad_token_id] = -100\n",
    "    \n",
    "    # Note: MedGemma uses specific image tokens - we don't need to mask them\n",
    "    # since we're training on text-only data (no images in the examples)\n",
    "    \n",
    "    batch[\"labels\"] = labels\n",
    "    return batch\n",
    "\n",
    "\n",
    "# Test the collate function\n",
    "print(\"=\" * 60)\n",
    "print(\"TESTING COLLATE FUNCTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "test_batch = collate_fn([train_ds[0], train_ds[1]])\n",
    "print(f\"Batch keys: {list(test_batch.keys())}\")\n",
    "print(f\"Input IDs shape: {test_batch['input_ids'].shape}\")\n",
    "print(f\"Attention mask shape: {test_batch['attention_mask'].shape}\")\n",
    "print(f\"Labels shape: {test_batch['labels'].shape}\")\n",
    "\n",
    "# Count non-pad tokens\n",
    "non_pad_tokens = (test_batch[\"labels\"] != -100).sum().item()\n",
    "total_tokens = test_batch[\"labels\"].numel()\n",
    "print(f\"\\nNon-pad tokens: {non_pad_tokens} / {total_tokens}\")\n",
    "print(f\"Pad token percentage: {(total_tokens - non_pad_tokens) / total_tokens * 100:.1f}%\")\n",
    "\n",
    "print(\"\\n\u2705 Collate function working correctly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: Configure LoRA + SFTTrainer\n",
    "\n",
    "Set up LoRA configuration targeting all linear layers, then create the SFTTrainer.  \n",
    "Note: Do NOT call `prepare_model_for_kbit_training()` or `get_peft_model()` manually \u2014  \n",
    "pass `peft_config` directly to SFTTrainer and it handles everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CONFIGURING LoRA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# LoRA configuration targeting all linear layers\n",
    "# modules_to_save includes lm_head and embed_tokens per Google's Gemma guide\n",
    "peft_config = LoraConfig(\n",
    "    r=CONFIG[\"lora_r\"],\n",
    "    lora_alpha=CONFIG[\"lora_alpha\"],\n",
    "    lora_dropout=CONFIG[\"lora_dropout\"],\n",
    "    bias=\"none\",\n",
    "    target_modules=CONFIG[\"lora_target_modules\"],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    modules_to_save=[\"lm_head\", \"embed_tokens\"],\n",
    ")\n",
    "\n",
    "print(f\"LoRA rank (r): {peft_config.r}\")\n",
    "print(f\"LoRA alpha: {peft_config.lora_alpha}\")\n",
    "print(f\"Target modules: {peft_config.target_modules}\")\n",
    "print(f\"Modules to save: {peft_config.modules_to_save}\")\n",
    "\n",
    "print(\"\\n=\" * 60)\n",
    "print(\"CONFIGURING SFTTrainer\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# SFTConfig with T4-compatible settings\n",
    "# CRITICAL: fp16=True, bf16=False (T4 doesn't support bf16)\n",
    "training_args = SFTConfig(\n",
    "    output_dir=CONFIG[\"output_dir\"],\n",
    "    num_train_epochs=CONFIG[\"num_train_epochs\"],\n",
    "    per_device_train_batch_size=CONFIG[\"per_device_train_batch_size\"],\n",
    "    per_device_eval_batch_size=CONFIG[\"per_device_eval_batch_size\"],\n",
    "    gradient_accumulation_steps=CONFIG[\"gradient_accumulation_steps\"],\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    learning_rate=CONFIG[\"learning_rate\"],\n",
    "    lr_scheduler_type=CONFIG[\"lr_scheduler_type\"],\n",
    "    warmup_ratio=CONFIG[\"warmup_ratio\"],\n",
    "    weight_decay=CONFIG[\"weight_decay\"],\n",
    "    fp16=True,           # MUST be True for T4\n",
    "    bf16=False,          # MUST be False for T4\n",
    "    logging_steps=CONFIG[\"logging_steps\"],\n",
    "    eval_strategy=CONFIG[\"eval_strategy\"],\n",
    "    eval_steps=CONFIG[\"eval_steps\"],\n",
    "    save_strategy=CONFIG[\"save_strategy\"],\n",
    "    save_total_limit=CONFIG[\"save_total_limit\"],\n",
    "    max_grad_norm=CONFIG[\"max_grad_norm\"],\n",
    "    report_to=\"none\",   # Disable wandb/tensorboard for Kaggle\n",
    "    seed=42,\n",
    "    dataset_kwargs={\"skip_prepare_dataset\": True},  # We use custom collate_fn\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "print(f\"Training epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"Gradient accumulation: {training_args.gradient_accumulation_steps}\")\n",
    "print(f\"Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"fp16: {training_args.fp16}\")\n",
    "print(f\"bf16: {training_args.bf16}\")\n",
    "\n",
    "# Create SFTTrainer with peft_config passed directly\n",
    "# SFTTrainer handles: prepare_model_for_kbit_training, get_peft_model, etc.\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    peft_config=peft_config,\n",
    "    processing_class=processor,\n",
    "    data_collator=collate_fn,\n",
    ")\n",
    "\n",
    "print(\"\\n\u2705 SFTTrainer created successfully\")\n",
    "print(f\"   Trainable parameters will be shown after training starts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: Train\n",
    "\n",
    "Run training for 3 epochs. Expected time: ~45-60 minutes on T4 x2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Log VRAM before training\n",
    "vram_before_train = torch.cuda.memory_allocated() / 1e9\n",
    "print(f\"VRAM before training: {vram_before_train:.2f} GB\")\n",
    "print(f\"Estimated training time: 45-60 minutes on T4 x2\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Train!\n",
    "trainer.train()\n",
    "\n",
    "# Calculate elapsed time\n",
    "elapsed = time.time() - start_time\n",
    "minutes = int(elapsed // 60)\n",
    "seconds = int(elapsed % 60)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total training time: {minutes}m {seconds}s\")\n",
    "\n",
    "# Show final metrics\n",
    "if trainer.state.log_history:\n",
    "    final_loss = None\n",
    "    for entry in reversed(trainer.state.log_history):\n",
    "        if \"loss\" in entry:\n",
    "            final_loss = entry[\"loss\"]\n",
    "            break\n",
    "    if final_loss:\n",
    "        print(f\"Final training loss: {final_loss:.4f}\")\n",
    "    \n",
    "    final_eval_loss = None\n",
    "    for entry in reversed(trainer.state.log_history):\n",
    "        if \"eval_loss\" in entry:\n",
    "            final_eval_loss = entry[\"eval_loss\"]\n",
    "            break\n",
    "    if final_eval_loss:\n",
    "        print(f\"Final eval loss: {final_eval_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8: Save & Push to Hub\n",
    "\n",
    "Save the LoRA adapter locally and push to HuggingFace Hub. The adapter is only ~50-100MB  \n",
    "(not the full 4B model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SAVING ADAPTER\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Save the LoRA adapter (not the full model)\n",
    "trainer.save_model()\n",
    "\n",
    "# Save processor (tokenizer + config)\n",
    "processor.save_pretrained(CONFIG[\"output_dir\"])\n",
    "\n",
    "# Calculate adapter size\n",
    "total_size = 0\n",
    "for dirpath, dirnames, filenames in os.walk(CONFIG[\"output_dir\"]):\n",
    "    for f in filenames:\n",
    "        fp = os.path.join(dirpath, f)\n",
    "        total_size += os.path.getsize(fp)\n",
    "\n",
    "size_mb = total_size / (1024 * 1024)\n",
    "print(f\"Adapter saved to: {CONFIG['output_dir']}/\")\n",
    "print(f\"Total adapter size: {size_mb:.2f} MB\")\n",
    "\n",
    "# List saved files\n",
    "print(\"\\nSaved files:\")\n",
    "for f in sorted(os.listdir(CONFIG[\"output_dir\"])):\n",
    "    file_path = os.path.join(CONFIG[\"output_dir\"], f)\n",
    "    if os.path.isfile(file_path):\n",
    "        file_size = os.path.getsize(file_path) / 1024\n",
    "        print(f\"  {f}: {file_size:.1f} KB\")\n",
    "\n",
    "print(\"\\n=\" * 60)\n",
    "print(\"PUSHING TO HUB\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Push to Hub (update hub_model_id in Cell 2 first!)\n",
    "if \"your-username\" not in CONFIG[\"hub_model_id\"]:\n",
    "    try:\n",
    "        trainer.push_to_hub(CONFIG[\"hub_model_id\"])\n",
    "        print(f\"\u2705 Adapter pushed to: https://huggingface.co/{CONFIG['hub_model_id']}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\u274c Failed to push to hub: {e}\")\n",
    "else:\n",
    "    print(\"\u26a0\ufe0f  Skipping hub push - please update 'hub_model_id' in CONFIG first!\")\n",
    "    print(f\"   Current value: {CONFIG['hub_model_id']}\")\n",
    "    print(f\"   Set to: your-actual-username/mamaguard-vitals-lora\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 9: Test Inference (Base vs Fine-Tuned)\n",
    "\n",
    "Compare the base model (LoRA disabled) vs fine-tuned model (LoRA enabled) on a sample case.  \n",
    "This demonstrates that the adapter is working and has learned maternal health patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"TEST INFERENCE: Base vs Fine-Tuned\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test case with high-risk vitals\n",
    "test_messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"\"\"Evaluate the following pregnancy vitals and determine risk level:\n",
    "\n",
    "Patient profile:\n",
    "- 35 years old, G1P0\n",
    "- Gestation: week 32 (3rd trimester)\n",
    "- BMI group: overweight\n",
    "\n",
    "Monitoring data (smartwatch + app logs):\n",
    "- BP: 145/95 mmHg\n",
    "- Fasting glucose: 8.5 mmol/L\n",
    "- Body temp: 98.2\u00b0F\n",
    "- Resting pulse: 88 bpm\n",
    "\n",
    "Please return:\n",
    "1) LOW/MID/HIGH risk classification\n",
    "2) Clinical interpretation tied to threshold values\n",
    "3) Likely maternal-fetal complications\n",
    "4) Week-32 appropriate management actions\n",
    "5) Immediate red-flag symptoms\"\"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Format with chat template\n",
    "test_text = processor.apply_chat_template(\n",
    "    test_messages,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=False\n",
    ").strip()\n",
    "\n",
    "print(\"TEST PROMPT:\")\n",
    "print(\"-\" * 60)\n",
    "print(test_text[:500] + \"...\" if len(test_text) > 500 else test_text)\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Tokenize\n",
    "inputs = processor(\n",
    "    text=[test_text],\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    ").to(model.device)\n",
    "\n",
    "# Generation config\n",
    "gen_kwargs = {\n",
    "    \"max_new_tokens\": 512,\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_p\": 0.9,\n",
    "    \"eos_token_id\": processor.tokenizer.eos_token_id,\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# Test 1: WITH LoRA (fine-tuned)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TEST 1: WITH LoRA ADAPTER (Fine-Tuned)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs_ft = model.generate(**inputs, **gen_kwargs)\n",
    "\n",
    "# Decode only the new tokens\n",
    "generated_ft = outputs_ft[0][inputs[\"input_ids\"].shape[1]:]\n",
    "response_ft = processor.decode(generated_ft, skip_special_tokens=True)\n",
    "\n",
    "print(\"Response:\")\n",
    "print(response_ft[:800] + \"...\" if len(response_ft) > 800 else response_ft)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Try to extract risk level from response\n",
    "if \"HIGH\" in response_ft[:100]:\n",
    "    risk_pred = \"HIGH\"\n",
    "elif \"MID\" in response_ft[:100] or \"MEDIUM\" in response_ft[:100]:\n",
    "    risk_pred = \"MID\"\n",
    "elif \"LOW\" in response_ft[:100]:\n",
    "    risk_pred = \"LOW\"\n",
    "else:\n",
    "    risk_pred = \"UNKNOWN\"\n",
    "\n",
    "print(f\"Predicted Risk Level: {risk_pred}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Summary\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"With LoRA: {risk_pred} risk predicted\")\n",
    "print(\"\\nExpected: HIGH risk (BP 145/95 > 140/90, Glucose 8.5 > 5.1)\")\n",
    "print(\"\\n\u2705 Inference complete!\")\n",
    "print(\"\\nNote: To compare with base model (LoRA disabled), reload the model\")\n",
    "print(\"without the adapter or use: model.disable_adapter() if available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Export Strategy\n",
    "\n",
    "The output is a **PEFT LoRA adapter** (~50-100MB), NOT a full model. At inference:\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForImageTextToText, AutoProcessor\n",
    "from peft import PeftModel\n",
    "\n",
    "# Load full base model (with vision encoder intact)\n",
    "base = AutoModelForImageTextToText.from_pretrained(\n",
    "    \"google/medgemma-1.5-4b-it\", ...\n",
    ")\n",
    "\n",
    "# Apply LoRA adapter\n",
    "model = PeftModel.from_pretrained(base, \"your-username/mamaguard-vitals-lora\")\n",
    "\n",
    "# Now you have:\n",
    "# - Vision encoder intact \u2192 multimodal inference works\n",
    "# - LoRA adapter active \u2192 maternal health expertise applied\n",
    "```\n",
    "\n",
    "## Training Summary\n",
    "\n",
    "| Metric | Value |\n",
    "|--------|-------|\n",
    "| Model | google/medgemma-1.5-4b-it |\n",
    "| Method | LoRA (r=16, \u03b1=16) |\n",
    "| Quantization | 4-bit NF4 |\n",
    "| Train samples | 912 |\n",
    "| Eval samples | 102 |\n",
    "| Epochs | 3 |\n",
    "| Batch size | 2 (effective 8) |\n",
    "| Learning rate | 2e-4 |\n",
    "| Hardware | Kaggle T4 x2 |\n",
    "| Runtime | ~45-60 min |\n",
    "| Output size | ~50-100 MB |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "kaggle": {
   "accelerator": "gpu",
   "data_sources": [
    {
     "source_id": "mamaguard-dataset",
     "source_type": "dataset"
    }
   ],
   "is_gpu_enabled": true,
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}