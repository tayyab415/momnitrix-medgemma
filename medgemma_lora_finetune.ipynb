{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8050dea",
   "metadata": {},
   "source": [
    "# Experiment: MedGemma LoRA Fine-tuning for Maternal Health Risk Assessment\n",
    "\n",
    "**Objective:**\n",
    "- Fine-tune Google's MedGemma-4B-IT model using LoRA for maternal health risk classification\n",
    "- Train on structured clinical dialogue data (vitals → risk assessment)\n",
    "- Evaluate model's ability to generate clinically appropriate risk assessments\n",
    "\n",
    "**Success Criteria:**\n",
    "- Training loss decreases steadily without overfitting\n",
    "- Model generates responses with correct RISK LEVEL prefix (LOW/MID/HIGH)\n",
    "- Outputs contain required sections: Clinical Reasoning, Complications, Actions, Warning Signs\n",
    "- Validation perplexity < 5.0 after 3 epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ca88f3",
   "metadata": {},
   "source": [
    "## 1. Setup: Imports and Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83998bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports for MedGemma fine-tuning\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "# HuggingFace ecosystem\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    EarlyStoppingCallback,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType, PeftModel\n",
    "from datasets import Dataset, load_from_disk\n",
    "\n",
    "# Reproducibility config\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# Device setup\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d35111",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Define model, LoRA, and training hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f51d08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and data configuration\n",
    "MODEL_NAME = \"google/medgemma-4b-it\"  # 4B instruction-tuned MedGemma\n",
    "DATA_PATH = \"./mamaguard_train.jsonl\"  # Training data from prepare_training_data.py\n",
    "EVAL_PATH = \"./mamaguard_eval.jsonl\"   # Evaluation data\n",
    "OUTPUT_DIR = \"./medgemma-lora-maternal-health\"\n",
    "\n",
    "# LoRA hyperparameters\n",
    "LORA_CONFIG = {\n",
    "    \"r\": 16,              # LoRA rank (low = faster, high = more expressive)\n",
    "    \"lora_alpha\": 32,     # Scaling factor (typically 2*r)\n",
    "    \"lora_dropout\": 0.05, # Dropout for regularization\n",
    "    \"target_modules\": [   # Which layers to apply LoRA\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    "    \"bias\": \"none\",\n",
    "    \"task_type\": TaskType.CAUSAL_LM,\n",
    "}\n",
    "\n",
    "# Training hyperparameters\n",
    "TRAINING_CONFIG = {\n",
    "    \"num_train_epochs\": 3,\n",
    "    \"per_device_train_batch_size\": 1,\n",
    "    \"per_device_eval_batch_size\": 1,\n",
    "    \"gradient_accumulation_steps\": 4,\n",
    "    \"learning_rate\": 2e-4,\n",
    "    \"warmup_steps\": 100,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"logging_steps\": 10,\n",
    "    \"save_steps\": 200,\n",
    "    \"eval_steps\": 200,\n",
    "    \"save_total_limit\": 3,\n",
    "    \"load_best_model_at_end\": True,\n",
    "    \"metric_for_best_model\": \"eval_loss\",\n",
    "    \"greater_is_better\": False,\n",
    "    \"fp16\": torch.cuda.is_available(),  # Mixed precision training\n",
    "    \"optim\": \"paged_adamw_8bit\",  # Memory-efficient optimizer\n",
    "}\n",
    "\n",
    "# Tokenization config\n",
    "MAX_SEQ_LENGTH = 2048  # Max sequence length for training\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "print(f\"  LoRA rank: {LORA_CONFIG['r']}, alpha: {LORA_CONFIG['lora_alpha']}\")\n",
    "print(f\"  Learning rate: {TRAINING_CONFIG['learning_rate']}\")\n",
    "print(f\"  Batch size: {TRAINING_CONFIG['per_device_train_batch_size']} x {TRAINING_CONFIG['gradient_accumulation_steps']} grad accum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbc448e",
   "metadata": {},
   "source": [
    "## 3. Load and Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86480141",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonl(path: str) -> list[dict[str, Any]]:\n",
    "    \"\"\"Load JSONL file into list of dictionaries.\"\"\"\n",
    "    data = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line.strip()))\n",
    "    return data\n",
    "\n",
    "# Load training and evaluation data\n",
    "train_data = load_jsonl(DATA_PATH)\n",
    "eval_data = load_jsonl(EVAL_PATH)\n",
    "\n",
    "print(f\"Loaded {len(train_data)} training samples\")\n",
    "print(f\"Loaded {len(eval_data)} evaluation samples\")\n",
    "\n",
    "# Display a sample\n",
    "print(\"\\nSample training entry:\")\n",
    "print(train_data[0][\"text\"][:800] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc640510",
   "metadata": {},
   "source": [
    "## 4. Load Tokenizer and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf74d6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "\n",
    "# Set pad token if not present (Gemma uses eos as pad)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "print(f\"Tokenizer vocab size: {len(tokenizer)}\")\n",
    "print(f\"Pad token: {tokenizer.pad_token}\")\n",
    "print(f\"EOS token: {tokenizer.eos_token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc3de7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model with memory-efficient settings\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    device_map=\"auto\" if torch.cuda.is_available() else None,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Print model info\n",
    "print(f\"Model loaded: {MODEL_NAME}\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.1f}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea63f0f5",
   "metadata": {},
   "source": [
    "## 5. Apply LoRA Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bd36a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LoRA config\n",
    "lora_config = LoraConfig(\n",
    "    r=LORA_CONFIG[\"r\"],\n",
    "    lora_alpha=LORA_CONFIG[\"lora_alpha\"],\n",
    "    target_modules=LORA_CONFIG[\"target_modules\"],\n",
    "    lora_dropout=LORA_CONFIG[\"lora_dropout\"],\n",
    "    bias=LORA_CONFIG[\"bias\"],\n",
    "    task_type=LORA_CONFIG[\"task_type\"],\n",
    ")\n",
    "\n",
    "# Apply LoRA to model\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbaf4fb",
   "metadata": {},
   "source": [
    "## 6. Tokenize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02caac7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples: dict[str, list]) -> dict[str, list]:\n",
    "    \"\"\"Tokenize the text field for causal LM training.\"\"\"\n",
    "    outputs = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=MAX_SEQ_LENGTH,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=None,\n",
    "    )\n",
    "    # For causal LM, labels are same as input_ids\n",
    "    outputs[\"labels\"] = outputs[\"input_ids\"].copy()\n",
    "    return outputs\n",
    "\n",
    "# Convert to HF Dataset\n",
    "train_dataset = Dataset.from_list([{\"text\": d[\"text\"]} for d in train_data])\n",
    "eval_dataset = Dataset.from_list([{\"text\": d[\"text\"]} for d in eval_data])\n",
    "\n",
    "# Tokenize\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "eval_dataset = eval_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Eval dataset size: {len(eval_dataset)}\")\n",
    "print(f\"Example tokenized length: {len(train_dataset[0]['input_ids'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ca40f6",
   "metadata": {},
   "source": [
    "## 7. Setup Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0065979f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=TRAINING_CONFIG[\"num_train_epochs\"],\n",
    "    per_device_train_batch_size=TRAINING_CONFIG[\"per_device_train_batch_size\"],\n",
    "    per_device_eval_batch_size=TRAINING_CONFIG[\"per_device_eval_batch_size\"],\n",
    "    gradient_accumulation_steps=TRAINING_CONFIG[\"gradient_accumulation_steps\"],\n",
    "    learning_rate=TRAINING_CONFIG[\"learning_rate\"],\n",
    "    warmup_steps=TRAINING_CONFIG[\"warmup_steps\"],\n",
    "    weight_decay=TRAINING_CONFIG[\"weight_decay\"],\n",
    "    logging_steps=TRAINING_CONFIG[\"logging_steps\"],\n",
    "    save_steps=TRAINING_CONFIG[\"save_steps\"],\n",
    "    eval_steps=TRAINING_CONFIG[\"eval_steps\"],\n",
    "    save_total_limit=TRAINING_CONFIG[\"save_total_limit\"],\n",
    "    load_best_model_at_end=TRAINING_CONFIG[\"load_best_model_at_end\"],\n",
    "    metric_for_best_model=TRAINING_CONFIG[\"metric_for_best_model\"],\n",
    "    greater_is_better=TRAINING_CONFIG[\"greater_is_better\"],\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    logging_strategy=\"steps\",\n",
    "    fp16=TRAINING_CONFIG[\"fp16\"],\n",
    "    optim=TRAINING_CONFIG[\"optim\"],\n",
    "    report_to=\"none\",  # Disable wandb/tensorboard for local runs\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "print(f\"Training output dir: {OUTPUT_DIR}\")\n",
    "print(f\"Total training steps: ~{len(train_dataset) * TRAINING_CONFIG['num_train_epochs'] // (TRAINING_CONFIG['per_device_train_batch_size'] * TRAINING_CONFIG['gradient_accumulation_steps'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da78f7da",
   "metadata": {},
   "source": [
    "## 8. Initialize Trainer and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4487aca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collator for language modeling\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,  # We're doing causal LM, not masked LM\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized. Starting training...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e2631c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "train_result = trainer.train()\n",
    "\n",
    "# Print training metrics\n",
    "print(\"\\nTraining completed!\")\n",
    "print(f\"Final train loss: {train_result.training_loss:.4f}\")\n",
    "print(f\"Training runtime: {train_result.metrics.get('train_runtime', 0)/60:.1f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a073d017",
   "metadata": {},
   "source": [
    "## 9. Save Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c75329d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the LoRA adapter weights\n",
    "model.save_pretrained(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "# Save training config for reproducibility\n",
    "config_save = {\n",
    "    \"model_name\": MODEL_NAME,\n",
    "    \"lora_config\": LORA_CONFIG,\n",
    "    \"training_config\": TRAINING_CONFIG,\n",
    "    \"seed\": SEED,\n",
    "    \"max_seq_length\": MAX_SEQ_LENGTH,\n",
    "}\n",
    "\n",
    "with open(f\"{OUTPUT_DIR}/training_config.json\", \"w\") as f:\n",
    "    json.dump(config_save, f, indent=2)\n",
    "\n",
    "print(f\"Model saved to: {OUTPUT_DIR}\")\n",
    "print(f\"Files in output dir: {list(Path(OUTPUT_DIR).glob('*'))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c85856",
   "metadata": {},
   "source": [
    "## 10. Evaluation: Generate Sample Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2378e943",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(model, tokenizer, prompt: str, max_new_tokens: int = 512) -> str:\n",
    "    \"\"\"Generate a response from the fine-tuned model.\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    # Decode only the new tokens\n",
    "    input_length = inputs[\"input_ids\"].shape[1]\n",
    "    response_tokens = outputs[0][input_length:]\n",
    "    response = tokenizer.decode(response_tokens, skip_special_tokens=True)\n",
    "    return response.strip()\n",
    "\n",
    "# Test with a sample prompt\n",
    "sample_prompt = train_data[0][\"text\"].split(\"<end_of_turn>\")[0] + \"<end_of_turn>\\n<start_of_turn>model\\n\"\n",
    "print(\"Test prompt:\")\n",
    "print(sample_prompt[:500] + \"...\\n\")\n",
    "\n",
    "# Generate response\n",
    "response = generate_response(model, tokenizer, sample_prompt)\n",
    "print(\"Generated response:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8e7c5e",
   "metadata": {},
   "source": [
    "## 11. Validate Output Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e96c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_response_format(response: str) -> dict[str, Any]:\n",
    "    \"\"\"Check if response follows expected format.\"\"\"\n",
    "    checks = {\n",
    "        \"has_risk_level\": response.upper().startswith(\"RISK LEVEL:\"),\n",
    "        \"has_clinical_reasoning\": \"CLINICAL REASONING\" in response.upper() or \"CLINICAL ASSESSMENT\" in response.upper(),\n",
    "        \"has_complications\": \"COMPLICATION\" in response.upper(),\n",
    "        \"has_actions\": \"ACTION\" in response.upper() or \"RECOMMENDED\" in response.upper(),\n",
    "        \"has_warning_signs\": \"WARNING\" in response.upper(),\n",
    "        \"risk_label_valid\": any(r in response.upper() for r in [\"LOW\", \"MID\", \"HIGH\"]),\n",
    "    }\n",
    "    checks[\"all_pass\"] = all(checks.values())\n",
    "    return checks\n",
    "\n",
    "# Validate the generated response\n",
    "validation = validate_response_format(response)\n",
    "print(\"Validation results:\")\n",
    "for check, passed in validation.items():\n",
    "    status = \"✓\" if passed else \"✗\"\n",
    "    print(f\"  {status} {check}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fccccb",
   "metadata": {},
   "source": [
    "## 12. Test on Multiple Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d3131f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a few samples from each risk level\n",
    "test_samples = []\n",
    "for risk in [\"LOW\", \"MID\", \"HIGH\"]:\n",
    "    samples = [d for d in eval_data if d.get(\"risk_level\") == risk][:2]\n",
    "    test_samples.extend(samples)\n",
    "\n",
    "print(f\"Testing on {len(test_samples)} samples...\\n\")\n",
    "\n",
    "results = []\n",
    "for i, sample in enumerate(test_samples):\n",
    "    # Extract prompt from text\n",
    "    prompt = sample[\"text\"].split(\"<end_of_turn>\")[0] + \"<end_of_turn>\\n<start_of_turn>model\\n\"\n",
    "    expected_risk = sample.get(\"risk_level\", \"UNKNOWN\")\n",
    "    \n",
    "    # Generate\n",
    "    generated = generate_response(model, tokenizer, prompt, max_new_tokens=400)\n",
    "    validation = validate_response_format(generated)\n",
    "    \n",
    "    # Extract predicted risk\n",
    "    predicted_risk = \"UNKNOWN\"\n",
    "    for r in [\"LOW\", \"MID\", \"HIGH\"]:\n",
    "        if r in generated.upper()[:50]:\n",
    "            predicted_risk = r\n",
    "            break\n",
    "    \n",
    "    results.append({\n",
    "        \"expected\": expected_risk,\n",
    "        \"predicted\": predicted_risk,\n",
    "        \"format_valid\": validation[\"all_pass\"],\n",
    "        \"response_preview\": generated[:200] + \"...\" if len(generated) > 200 else generated,\n",
    "    })\n",
    "    \n",
    "    print(f\"Sample {i+1} | Expected: {expected_risk} | Predicted: {predicted_risk}\")\n",
    "    print(f\"  Format valid: {validation['all_pass']}\")\n",
    "    print(f\"  Preview: {results[-1]['response_preview']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2400ce07",
   "metadata": {},
   "source": [
    "## Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13830c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile results\n",
    "correct_predictions = sum(1 for r in results if r[\"expected\"] == r[\"predicted\"])\n",
    "valid_formats = sum(1 for r in results if r[\"format_valid\"])\n",
    "\n",
    "summary = {\n",
    "    \"total_samples_tested\": len(results),\n",
    "    \"correct_risk_predictions\": correct_predictions,\n",
    "    \"prediction_accuracy\": correct_predictions / len(results) if results else 0,\n",
    "    \"valid_format_count\": valid_formats,\n",
    "    \"format_compliance_rate\": valid_formats / len(results) if results else 0,\n",
    "    \"training_loss\": train_result.training_loss if 'train_result' in locals() else None,\n",
    "    \"model_output_dir\": OUTPUT_DIR,\n",
    "}\n",
    "\n",
    "print(\"=== EXPERIMENT SUMMARY ===\")\n",
    "for key, value in summary.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"{key}: {value:.2%}\")\n",
    "    else:\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "# Save summary\n",
    "with open(f\"{OUTPUT_DIR}/experiment_summary.json\", \"w\") as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "print(f\"\\nSummary saved to: {OUTPUT_DIR}/experiment_summary.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0656341c",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- **If results are good:** Merge LoRA weights with base model for deployment using `model.merge_and_unload()`\n",
    "- **If underfitting:** Increase LoRA rank (r=32), train for more epochs, or increase learning rate\n",
    "- **If overfitting:** Increase dropout, reduce epochs, or add more training data\n",
    "- **Production:** Convert to GGUF or ONNX format for efficient inference\n",
    "- **Evaluation:** Run full evaluation on held-out test set with clinical expert review"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
