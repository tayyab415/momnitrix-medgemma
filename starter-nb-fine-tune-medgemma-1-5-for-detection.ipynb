{"metadata":{"kernelspec":{"name":"","display_name":""},"language_info":{"name":"python"},"jupytext":{"cell_metadata_filter":"-all","main_language":"python","notebook_metadata_filter":"-all"}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"5826ee18","cell_type":"markdown","source":"[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/harpreetsahota204/medgemma_kaggle_competition/blob/main/medgemma_impact_starter.ipynb)\n\n# Winning the MedGemma Impact Challenge with FiftyOne\n\n**The difference between a demo and a winning submission is understanding where your model breaks—and why.**\n\nThis notebook shows you how to use [FiftyOne](https://docs.voxel51.com/) as your workbench for the MedGemma Impact Challenge. \nWe'll go beyond running inference and printing metrics. You'll learn to:\n\n1. **Explore your data** before modeling\n2. **Visualize embeddings** to diagnose learnability  \n3. **Run MedGemma inference** and store predictions alongside ground truth\n4. **Analyze failures** systematically—not just count them\n5. **Fine-tune for localization** using FiftyOne's PyTorch integration\n\nWe'll use the [SLAKE dataset](https://huggingface.co/datasets/Voxel51/SLAKE), a medical VQA benchmark \nwith images from multiple modalities (CT, MRI, X-ray), rich annotations including bounding boxes and \nsegmentation masks, and questions spanning anatomy, abnormalities, and more.\n\n---","metadata":{}},{"id":"f184fdcf","cell_type":"markdown","source":"## Setup & Installation","metadata":{}},{"id":"b7db1955","cell_type":"code","source":"!pip install -U fiftyone huggingface_hub accelerate sentencepiece protobuf torch torchvision umap-learn","metadata":{},"outputs":[],"execution_count":null},{"id":"50f6e801","cell_type":"markdown","source":"### Authenticate with Hugging Face\n\nBoth MedGemma and MedSigLIP are gated models. You'll need to:\n1. Request access on [MedGemma](https://huggingface.co/google/medgemma-1.5-4b-it) and [MedSigLIP](https://huggingface.co/google/medsiglip-448)\n2. Set your HF token","metadata":{}},{"id":"fe350ba9","cell_type":"code","source":"import os\n# os.environ[\"HF_TOKEN\"] = \"your_token_here\"\n\n# Or login via CLI: hf auth login\n\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" #update for your setup","metadata":{},"outputs":[],"execution_count":1},{"id":"45462b53","cell_type":"markdown","source":"---\n## 1. Load the SLAKE Dataset\n\nThe SLAKE dataset is already in [FiftyOne format](https://docs.voxel51.com/user_guide/using_datasets.html) on Hugging Face. \nOne line to load it using the [`load_from_hub()`](https://docs.voxel51.com/api/fiftyone.utils.huggingface.html#fiftyone.utils.huggingface.load_from_hub) function.","metadata":{}},{"id":"6dad7b4b","cell_type":"code","source":"from fiftyone.utils.huggingface import load_from_hub\n\ndataset = load_from_hub(\n    \"Voxel51/SLAKE\",\n    name=\"SLAKE\",\n    overwrite=True,\n    max_samples=50 #taking a small subset of the dataset for this example\n)","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading config file fiftyone.yml from Voxel51/SLAKE\n","Loading dataset\n","Importing samples...\n"," 100% |███████████████████| 50/50 [3.8ms elapsed, 0s remaining, 13.1K samples/s]      \n"]}],"execution_count":2},{"id":"49186060","cell_type":"markdown","source":"### Understanding FiftyOne Datasets\n\nA FiftyOne [Dataset](https://docs.voxel51.com/api/fiftyone.core.dataset.html#fiftyone.core.dataset.Dataset) \nis comprised of [Samples](https://docs.voxel51.com/api/fiftyone.core.sample.html#fiftyone.core.sample.Sample).\n\n**Samples** store all information associated with a particular piece of data in a dataset, including:\n- Basic metadata about the data\n- One or more sets of labels\n- Additional features associated with subsets of the data and/or label sets\n\nThe attributes of a Sample are called [Fields](https://docs.voxel51.com/api/fiftyone.core.fields.html#fiftyone.core.fields.Field), \nwhich store information about the Sample. When a new Field is assigned to a Sample in a Dataset, \nit is automatically added to the dataset's schema and thus accessible on all other samples in the dataset.\n\nLet's look at the schema to understand what we're working with:","metadata":{}},{"id":"53e3ac8d","cell_type":"code","source":"dataset","metadata":{},"outputs":[{"data":{"text/plain":["Name:        SLAKE\n","Media type:  image\n","Num samples: 50\n","Persistent:  False\n","Tags:        []\n","Sample fields:\n","    id:               fiftyone.core.fields.ObjectIdField\n","    filepath:         fiftyone.core.fields.StringField\n","    tags:             fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n","    metadata:         fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n","    created_at:       fiftyone.core.fields.DateTimeField\n","    last_modified_at: fiftyone.core.fields.DateTimeField\n","    detections:       fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n","    segmentation:     fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Segmentation)\n","    location:         fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n","    modality:         fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n","    base_type:        fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n","    answer_type:      fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n","    question_0:       fiftyone.core.fields.StringField\n","    answer_0:         fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n","    question_1:       fiftyone.core.fields.StringField\n","    answer_1:         fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n","    question_2:       fiftyone.core.fields.StringField\n","    answer_2:         fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n","    question_3:       fiftyone.core.fields.StringField\n","    answer_3:         fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n","    question_4:       fiftyone.core.fields.StringField\n","    answer_4:         fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n","    question_5:       fiftyone.core.fields.StringField\n","    answer_5:         fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n","    question_6:       fiftyone.core.fields.StringField\n","    answer_6:         fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n","    question_7:       fiftyone.core.fields.StringField\n","    answer_7:         fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n","    question_8:       fiftyone.core.fields.StringField\n","    answer_8:         fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n","    question_9:       fiftyone.core.fields.StringField\n","    answer_9:         fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n","    question_10:      fiftyone.core.fields.StringField\n","    answer_10:        fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n","    question_11:      fiftyone.core.fields.StringField\n","    answer_11:        fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n","    question_12:      fiftyone.core.fields.StringField\n","    answer_12:        fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n","    question_13:      fiftyone.core.fields.StringField\n","    answer_13:        fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n","    question_14:      fiftyone.core.fields.StringField\n","    answer_14:        fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n","    question_15:      fiftyone.core.fields.StringField\n","    answer_15:        fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n","    question_16:      fiftyone.core.fields.StringField\n","    answer_16:        fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n","    question_17:      fiftyone.core.fields.StringField\n","    answer_17:        fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n","    question_18:      fiftyone.core.fields.StringField\n","    answer_18:        fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n","    question_19:      fiftyone.core.fields.StringField\n","    answer_19:        fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"execution_count":3},{"id":"0d305f0e","cell_type":"markdown","source":"To see the contents of a single Sample and its Fields, you can use the \n[`first()` method](https://docs.voxel51.com/api/fiftyone.core.dataset.html#fiftyone.core.dataset.Dataset.first):","metadata":{}},{"id":"96aed697","cell_type":"code","source":"dataset.first()","metadata":{},"outputs":[{"data":{"text/plain":["<Sample: {\n","    'id': '6830a40b7a3316c43716ba5b',\n","    'media_type': 'image',\n","    'filepath': '/home/harpreet/fiftyone/huggingface/hub/Voxel51/SLAKE/data/source_xmlab132.jpg',\n","    'tags': [],\n","    'metadata': <ImageMetadata: {\n","        'size_bytes': 179352,\n","        'mime_type': 'image/jpeg',\n","        'width': 1024,\n","        'height': 1024,\n","        'num_channels': 3,\n","    }>,\n","    'created_at': datetime.datetime(2026, 1, 19, 21, 42, 13, 156000),\n","    'last_modified_at': datetime.datetime(2026, 1, 19, 21, 42, 13, 156000),\n","    'detections': <Detections: {\n","        'detections': [\n","            <Detection: {\n","                'id': '6830a40a7a3316c437168c02',\n","                'attributes': {},\n","                'tags': [],\n","                'label': 'Cardiomegaly',\n","                'bounding_box': [0.3779296875, 0.4091796875, 0.490234375, 0.3583984375],\n","                'mask': None,\n","                'mask_path': None,\n","                'confidence': None,\n","                'index': None,\n","            }>,\n","        ],\n","    }>,\n","    'segmentation': <Segmentation: {\n","        'id': '6830a40a7a3316c437168c03',\n","        'tags': [],\n","        'mask': None,\n","        'mask_path': '/home/harpreet/fiftyone/huggingface/hub/Voxel51/SLAKE/fields/segmentation/mask_xmlab132.png',\n","    }>,\n","    'location': <Classification: {\n","        'id': '6830a40a7a3316c437168c04',\n","        'tags': [],\n","        'label': 'Lung',\n","        'confidence': None,\n","        'logits': None,\n","    }>,\n","    'modality': <Classification: {\n","        'id': '6830a40a7a3316c437168c05',\n","        'tags': [],\n","        'label': 'X-Ray',\n","        'confidence': None,\n","        'logits': None,\n","    }>,\n","    'base_type': <Classification: {\n","        'id': '6830a40a7a3316c437168c06',\n","        'tags': [],\n","        'label': 'vqa',\n","        'confidence': None,\n","        'logits': None,\n","    }>,\n","    'answer_type': <Classification: {\n","        'id': '6830a40a7a3316c437168c07',\n","        'tags': [],\n","        'label': 'OPEN',\n","        'confidence': None,\n","        'logits': None,\n","    }>,\n","    'question_0': 'What modality is used to take this image?',\n","    'answer_0': <Classification: {\n","        'id': '6830a40a7a3316c437168c08',\n","        'tags': [],\n","        'label': 'X-Ray',\n","        'confidence': None,\n","        'logits': None,\n","    }>,\n","    'question_1': 'Which part of the body does this image belong to?',\n","    'answer_1': <Classification: {\n","        'id': '6830a40a7a3316c437168c09',\n","        'tags': [],\n","        'label': 'Chest',\n","        'confidence': None,\n","        'logits': None,\n","    }>,\n","    'question_2': 'Are there abnormalities in this image?',\n","    'answer_2': <Classification: {\n","        'id': '6830a40a7a3316c437168c0a',\n","        'tags': [],\n","        'label': 'Yes',\n","        'confidence': None,\n","        'logits': None,\n","    }>,\n","    'question_3': 'What is the largest organ in the picture?',\n","    'answer_3': <Classification: {\n","        'id': '6830a40a7a3316c437168c0b',\n","        'tags': [],\n","        'label': 'Lung',\n","        'confidence': None,\n","        'logits': None,\n","    }>,\n","    'question_4': 'What diseases are included in the picture?',\n","    'answer_4': <Classification: {\n","        'id': '6830a40a7a3316c437168c0c',\n","        'tags': [],\n","        'label': 'Cardiomegal',\n","        'confidence': None,\n","        'logits': None,\n","    }>,\n","    'question_5': 'Where is/are the abnormality located?',\n","    'answer_5': <Classification: {\n","        'id': '6830a40a7a3316c437168c0d',\n","        'tags': [],\n","        'label': 'Center',\n","        'confidence': None,\n","        'logits': None,\n","    }>,\n","    'question_6': 'Which organ is abnormal, heart or lung?',\n","    'answer_6': <Classification: {\n","        'id': '6830a40a7a3316c437168c0e',\n","        'tags': [],\n","        'label': 'Heart',\n","        'confidence': None,\n","        'logits': None,\n","    }>,\n","    'question_7': None,\n","    'answer_7': None,\n","    'question_8': None,\n","    'answer_8': None,\n","    'question_9': None,\n","    'answer_9': None,\n","    'question_10': None,\n","    'answer_10': None,\n","    'question_11': None,\n","    'answer_11': None,\n","    'question_12': None,\n","    'answer_12': None,\n","    'question_13': None,\n","    'answer_13': None,\n","    'question_14': None,\n","    'answer_14': None,\n","    'question_15': None,\n","    'answer_15': None,\n","    'question_16': None,\n","    'answer_16': None,\n","    'question_17': None,\n","    'answer_17': None,\n","    'question_18': None,\n","    'answer_18': None,\n","    'question_19': None,\n","    'answer_19': None,\n","}>"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"execution_count":4},{"id":"a59a1208","cell_type":"markdown","source":"### Understanding the SLAKE Schema\n\nThis dataset is **image-centric**: each of the 642 samples represents one medical image,\nwith multiple Q&A pairs attached to it. Let's break down the key fields:\n\n**Metadata fields** (stored as `Classification` objects—access via `.label`):\n- `modality`: Imaging modality (CT, MRI, X-Ray) \n- `location`: Anatomical region (Lung, Brain, Abdomen, etc.)\n- `answer_type`: Question type (OPEN or CLOSED)\n- `base_type`: Task type (vqa)\n\n**Multiple Q&A pairs** (up to 20 per image):\n- `question_0`, `question_1`, ... `question_19`: Question strings\n- `answer_0`, `answer_1`, ... `answer_19`: Answer as `Classification` objects\n\n**Annotations** (where available):\n- `detections`: Bounding boxes with labels (e.g., \"Cardiomegaly\")\n- `segmentation`: Segmentation masks with `mask_path`","metadata":{}},{"id":"6eee2265","cell_type":"markdown","source":"### Accessing Classification Fields\n\nMany fields in this dataset are stored as FiftyOne \n[`Classification`](https://docs.voxel51.com/api/fiftyone.core.labels.html#fiftyone.core.labels.Classification) \nobjects. To get the actual value, access the `.label` attribute:","metadata":{}},{"id":"719f4777","cell_type":"code","source":"sample = dataset.first()\n\n# These are Classification objects - access .label to get the string value\nprint(f\"Modality: {sample.modality.label}\")\nprint(f\"Location: {sample.location.label}\")\nprint(f\"Answer Type: {sample.answer_type.label}\")\n\n# Questions are stored as strings\nprint(f\"\\nQuestion 0: {sample.question_0}\")\n\n# Answers are Classification objects\nprint(f\"Answer 0: {sample.answer_0.label}\")","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Modality: X-Ray\n","Location: Lung\n","Answer Type: OPEN\n","\n","Question 0: What modality is used to take this image?\n","Answer 0: X-Ray\n"]}],"execution_count":5},{"id":"fb4c8d80","cell_type":"markdown","source":"### Slicing Field Values with `ViewField`\n\n**Key Concept:** Methods like `count_values(\"modality.label\")` work because they accept \n**field paths as strings** (using dot notation). However, **slicing/indexing requires \n`ViewField` expressions**.\n\n**String field paths** (dot notation) work for:\n- `count_values(\"modality.label\")`\n- `distinct(\"modality.label\")`\n- `sort_by(\"modality.label\")`\n\n**`ViewField` expressions** are required for:\n- Array indexing: `F(\"bounding_box\")[2]`\n- Array slicing: `F(\"detections\")[1:3]`\n- String slicing: `F(\"text_field\")[:10]`\n\n```python\nfrom fiftyone import ViewField as F\n\n# ❌ This won't work (can't slice string paths)\ndataset.count_values(\"predictions.detections[0].label\")\n\n# ✅ Use ViewField for slicing\nexpr = F(\"predictions.detections\")[0].label\ndataset.count_values(expr)\n\n# ✅ Other examples\nbbox_width = F(\"bounding_box\")[2]\nfirst_three = F(\"detections\")[:3]\n```\n\n**Summary:**\n- **Dot notation strings** = simple field paths\n- **`F(...)` expressions** = when you need indexing/slicing operations on field values","metadata":{}},{"id":"ef6b18e9","cell_type":"markdown","source":"### Exploring Q&A Pairs\n\nEach image has multiple question-answer pairs. Let's look at a single sample:","metadata":{}},{"id":"f4b70ec9","cell_type":"code","source":"sample = dataset.first()\n\n# Print Q&A pairs for this sample\nprint(f\"Sample Q&A pairs:\\n\")\nfor i in range(7):  # First 7 questions (most samples have ~7)\n    q = getattr(sample, f\"question_{i}\")\n    a = getattr(sample, f\"answer_{i}\")\n    if q is not None:\n        print(f\"Q{i}: {q}\")\n        print(f\"A{i}: {a.label if a else 'None'}\")\n        print()","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Sample Q&A pairs:\n","\n","Q0: What modality is used to take this image?\n","A0: X-Ray\n","\n","Q1: Which part of the body does this image belong to?\n","A1: Chest\n","\n","Q2: Are there abnormalities in this image?\n","A2: Yes\n","\n","Q3: What is the largest organ in the picture?\n","A3: Lung\n","\n","Q4: What diseases are included in the picture?\n","A4: Cardiomegal\n","\n","Q5: Where is/are the abnormality located?\n","A5: Center\n","\n","Q6: Which organ is abnormal, heart or lung?\n","A6: Heart\n","\n"]}],"execution_count":6},{"id":"c3d9865d","cell_type":"markdown","source":"---\n## 2. Explore Your Data (Before You Model)\n\nDon't rush to inference. Understanding your data distribution is how you catch problems early.\n\nFiftyOne provides powerful functionality to compute statistics about your dataset using \n[built-in Aggregation methods](https://docs.voxel51.com/user_guide/using_aggregations.html).","metadata":{}},{"id":"ce77df1b","cell_type":"markdown","source":"### What modalities do we have?\n\nUse the [`count_values()` aggregation](https://docs.voxel51.com/api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.count_values) \nto compute the occurrences of field values in a collection.\n\n**Important:** Since `modality` is a Classification field, we need to access \nthe `.label` attribute using dot notation in the field path:","metadata":{}},{"id":"b5fe4b6d","cell_type":"code","source":"dataset.count_values(\"modality.label\")","metadata":{},"outputs":[{"data":{"text/plain":["{'X-Ray': 12, 'CT': 17, 'MRI': 21}"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"execution_count":7},{"id":"2d1ee31d","cell_type":"markdown","source":"### What anatomical locations are covered?\n\nThe `location` field tells us what body part/organ the image focuses on:","metadata":{}},{"id":"42cd168f","cell_type":"code","source":"dataset.count_values(\"location.label\")","metadata":{},"outputs":[{"data":{"text/plain":["{'Brain_Tissue': 15,\n"," 'Lung': 14,\n"," 'Neck': 1,\n"," 'Pelvic Cavity': 2,\n"," 'Brain': 2,\n"," 'Chest_lung': 3,\n"," 'Abdomen': 13}"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"execution_count":8},{"id":"5be3f5c6","cell_type":"markdown","source":"### What types of questions?\n\nThe `answer_type` field indicates whether questions are OPEN (free-form) or CLOSED (yes/no, multiple choice):","metadata":{}},{"id":"d5c5c740","cell_type":"code","source":"dataset.count_values(\"answer_type.label\")","metadata":{},"outputs":[{"data":{"text/plain":["{'OPEN': 28, 'CLOSED': 22}"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"execution_count":9},{"id":"1b0f0101","cell_type":"markdown","source":"### What detection labels exist?\n\nThe `detections` field contains bounding boxes with labels (e.g., anatomical structures, \nabnormalities). Use [`count_values()`](https://docs.voxel51.com/api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.count_values) \non nested fields:","metadata":{}},{"id":"a6afddbc","cell_type":"code","source":"dataset.count_values(\"detections.detections.label\")","metadata":{},"outputs":[{"data":{"text/plain":["{'Left Eye': 1,\n"," 'Spinal Cord': 9,\n"," 'Left Kidney': 4,\n"," 'Left Temporal Lobe': 1,\n"," 'Esophagus': 2,\n"," 'Nodule': 3,\n"," 'Spleen': 7,\n"," 'Mass': 3,\n"," 'Liver': 14,\n"," 'Stomach': 1,\n"," 'Left Lung': 4,\n"," 'Small Bowel': 9,\n"," 'Brain Non-enhancing Tumor': 17,\n"," 'Right Temporal Lobe': 1,\n"," 'Trachea': 1,\n"," 'Pneumonia': 3,\n"," 'Right Lung': 5,\n"," 'Brain Enhancing Tumor': 10,\n"," 'Right Eye': 1,\n"," 'Cardiomegaly': 3,\n"," 'Heart': 3,\n"," 'Rectum': 1,\n"," 'Bladder': 1,\n"," 'Brain Edema': 20,\n"," 'Duodenum': 2,\n"," 'Right Kidney': 3,\n"," 'Colon': 6}"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"execution_count":10},{"id":"abdf9c10","cell_type":"markdown","source":"### Launch the App to explore visually\n\nThe most powerful part of FiftyOne is [the FiftyOne App](https://docs.voxel51.com/user_guide/app.html#using-the-fiftyone-app), \nwhich runs locally on your machine. Filter, sort, and browse your data interactively.","metadata":{}},{"id":"1fa2fdc9","cell_type":"code","source":"import fiftyone as fo\nsession = fo.launch_app(dataset)","metadata":{},"outputs":[],"execution_count":null},{"id":"07995739","cell_type":"markdown","source":"# ![Explore MedGemma](https://raw.githubusercontent.com/harpreetsahota204/medgemma_kaggle_competition/main/gifs/explore_med_gemma.gif)\n","metadata":{}},{"id":"f4c205a3","cell_type":"markdown","source":"**Try these in the App:**\n- In sidebar of the app, under the Labels section, click the dropdown for `modality` and click the check box for CT to filter the samples in the panel to only CT scans\n- Try the same for the to `location` label, for example filter to \"Lung\"` to see lung images\n- Look at samples with detections (bounding boxes) vs without\n- Explore the Q&A pairs in the sample panel\n\nYou'll start to notice patterns: certain anatomical locations have more images, \ncertain modalities are over/under-represented, etc.","metadata":{}},{"id":"ddc486c0","cell_type":"markdown","source":"### Create useful Dataset Views\n\n[Dataset Views](https://docs.voxel51.com/user_guide/using_views.html) let you filter, sort, and \nslice your data without modifying the underlying dataset. Views are powerful because they:\n- Chain multiple operations together\n- Are lazily evaluated for efficiency\n- Can be saved and reloaded\n\nYou can use [`ViewField`](https://docs.voxel51.com/api/fiftyone.core.expressions.html#fiftyone.core.expressions.ViewField) \nand [`ViewExpression`](https://docs.voxel51.com/api/fiftyone.core.expressions.html#fiftyone.core.expressions.ViewExpression) \nclasses to define expressions using native Python operators. Simply wrap the target field in a \n`ViewField` and apply comparison, logic, arithmetic or array operations to it.\n\nLearn more about [creating Views](https://docs.voxel51.com/cheat_sheets/views_cheat_sheet.html) \nand [filtering](https://docs.voxel51.com/cheat_sheets/filtering_cheat_sheet.html) in the cheat sheets.","metadata":{}},{"id":"cd5eef03","cell_type":"code","source":"from fiftyone import ViewField as F\n# CLOSED answer type only (yes/no questions - easier to evaluate)\n# Note: Use \"answer_type.label\" to filter on the Classification's label\nclosed_questions = dataset.match(F(\"answer_type.label\") == \"CLOSED\")\ndataset.save_view(\"closed_questions\", closed_questions)\nprint(f\"Images with CLOSED questions: {len(closed_questions)}\")\n\n# Images with detection annotations (bounding boxes)\nhas_detections = dataset.match(F(\"detections.detections\").length() > 0)\ndataset.save_view(\"has_detections\", has_detections)\nprint(f\"Images with detections: {len(has_detections)}\")\n\n# X-Ray images only\nxray_only = dataset.match(F(\"modality.label\") == \"X-Ray\")\ndataset.save_view(\"xray_only\", xray_only)\nprint(f\"X-Ray images: {len(xray_only)}\")\n\n# CT images only\nct_only = dataset.match(F(\"modality.label\") == \"CT\")\ndataset.save_view(\"ct_only\", ct_only)\nprint(f\"CT images: {len(ct_only)}\")\n\n# Lung images\nlung_images = dataset.match(F(\"location.label\") == \"Lung\")\ndataset.save_view(\"lung_images\", lung_images)\nprint(f\"Lung images: {len(lung_images)}\")","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Images with CLOSED questions: 22\n","Images with detections: 47\n","X-Ray images: 12\n","CT images: 17\n","Lung images: 14\n"]}],"execution_count":12},{"id":"556936b7","cell_type":"markdown","source":"For those familiar with `pandas`, check out this \n[pandas vs FiftyOne cheat sheet](https://docs.voxel51.com/cheat_sheets/pandas_vs_fiftyone.html) \nto learn how to translate common pandas operations into FiftyOne syntax.","metadata":{}},{"id":"267f7220","cell_type":"markdown","source":"---\n## 3. Compute Embeddings with MedSigLIP\n\nBefore running VQA inference, let's see if the embedding space even separates our classes.\nIf MedSigLIP embeddings don't cluster by modality or body part, that's diagnostic information.\n\nYou can visualize [image embeddings](https://docs.voxel51.com/brain.html#visualizing-embeddings) \nusing models from the [FiftyOne Model Zoo](https://docs.voxel51.com/model_zoo/overview.html), \nor custom models which you can integrate as a [Remote Zoo Model](https://docs.voxel51.com/model_zoo/remote.html#remotely-sourced-zoo-models).","metadata":{}},{"id":"e27efb5f","cell_type":"markdown","source":"### Register and load MedSigLIP\n\nMedSigLIP is available as a Remote Zoo Model. First, register the model source:","metadata":{}},{"id":"bb452f84","cell_type":"code","source":"import fiftyone.zoo as foz\n# Register the model source (one time)\nfoz.register_zoo_model_source(\n    \"https://github.com/harpreetsahota204/medsiglip\",\n    overwrite=True\n)\n\n# Download the model (one time)\nfoz.download_zoo_model(\n    \"https://github.com/harpreetsahota204/medsiglip\",\n    model_name=\"google/medsiglip-448\",\n)","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading https://github.com/harpreetsahota204/medsiglip...\n","   22.2Mb [463.1ms elapsed, ? remaining, 47.9Mb/s] \n","Overwriting existing model source '/home/harpreet/fiftyone/__models__/medsiglip'\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"01070d13c5cb4e1a96ac009f97242859","version_major":2,"version_minor":0},"text/plain":["Downloading (incomplete total...): 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4b7e343660244cd3a0e8da3a4b97c379","version_major":2,"version_minor":0},"text/plain":["Fetching 9 files:   0%|          | 0/9 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["(<fiftyone.zoo.models.RemoteZooModel at 0x7625c032ba10>,\n"," '/home/harpreet/fiftyone/__models__/medsiglip/medsiglip-448')"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"execution_count":13},{"id":"6c979196","cell_type":"code","source":"# Load the model\nmedsiglip = foz.load_zoo_model(\"google/medsiglip-448\")","metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"928620e3394540758ab991d62fe2b6bf","version_major":2,"version_minor":0},"text/plain":["Downloading (incomplete total...): 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"559c6f15f5bf4588b0ceba48f80ff84b","version_major":2,"version_minor":0},"text/plain":["Fetching 9 files:   0%|          | 0/9 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e3947bf4355d4b93bf6c48a6939107fe","version_major":2,"version_minor":0},"text/plain":["Loading weights:   0%|          | 0/888 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"execution_count":14},{"id":"23cd9e2c","cell_type":"markdown","source":"### Compute embeddings\n\nUse the [`compute_embeddings()` method](https://docs.voxel51.com/api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.compute_embeddings) \nto compute embeddings for all samples in your dataset:","metadata":{}},{"id":"06e3f16b","cell_type":"code","source":"dataset.compute_embeddings(\n    model=medsiglip,\n    embeddings_field=\"medsiglip_embeddings\",\n)","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":[" 100% |███████████████████| 50/50 [3.9s elapsed, 0s remaining, 19.9 samples/s]      \n"]}],"execution_count":15},{"id":"0b5e4006","cell_type":"markdown","source":"### Visualize in 2D\n\nUse the [`compute_visualization()` method](https://docs.voxel51.com/api/fiftyone.brain.html#fiftyone.brain.compute_visualization) \nto generate low-dimensional representations of the samples in your Dataset. \nThis projects high-dimensional embeddings to 2D/3D for visualization.","metadata":{}},{"id":"7976e71f","cell_type":"code","source":"import fiftyone.brain as fob\n\nresults = fob.compute_visualization(\n    dataset,\n    embeddings=\"medsiglip_embeddings\",\n    method=\"umap\",\n    brain_key=\"medsiglip_viz\",\n    num_dims=2,\n)","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Generating visualization...\n","UMAP( verbose=True)\n","Mon Jan 19 15:43:22 2026 Construct fuzzy simplicial set\n","Mon Jan 19 15:43:23 2026 Finding Nearest Neighbors\n","Mon Jan 19 15:43:26 2026 Finished Nearest Neighbor Search\n","Mon Jan 19 15:43:28 2026 Construct embedding\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ab6db928c0494952a9789dd4016d6331","version_major":2,"version_minor":0},"text/plain":["Epochs completed:   0%|            0/500 [00:00]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\tcompleted  0  /  500 epochs\n","\tcompleted  50  /  500 epochs\n","\tcompleted  100  /  500 epochs\n","\tcompleted  150  /  500 epochs\n","\tcompleted  200  /  500 epochs\n","\tcompleted  250  /  500 epochs\n","\tcompleted  300  /  500 epochs\n","\tcompleted  350  /  500 epochs\n","\tcompleted  400  /  500 epochs\n","\tcompleted  450  /  500 epochs\n","Mon Jan 19 15:43:30 2026 Finished embedding\n"]}],"execution_count":16},{"id":"7d47c157","cell_type":"markdown","source":"### Build a similarity index for later\n\nUse the [`compute_similarity()` method](https://docs.voxel51.com/api/fiftyone.brain.html#fiftyone.brain.compute_similarity) \nto build a similarity index over the images in your dataset. This allows you to \n[sort by similarity](https://docs.voxel51.com/brain.html#sorting-by-similarity) or \n[search with natural language](https://docs.voxel51.com/brain.html#text-similarity) (for models that support this, such as CLIP, SigLIP, or MedSigLIP).","metadata":{}},{"id":"7ef48f84","cell_type":"code","source":"sim_index = fob.compute_similarity(\n    dataset,\n    model=\"google/medsiglip-448\",\n    brain_key=\"medsiglip_similarity\",\n    embeddings=\"medsiglip_embeddings\"\n)","metadata":{},"outputs":[],"execution_count":17},{"id":"6b3778b2","cell_type":"markdown","source":"With embeddings computed, you can perform non-trivial analysis like computing scores for \n[uniqueness](https://docs.voxel51.com/brain.html#image-uniqueness), \n[representativeness](https://docs.voxel51.com/brain.html#image-representativeness), \nand [identifying near duplicates](https://docs.voxel51.com/brain.html#near-duplicates) \nwith simple function calls.\n\n- Near-duplicates: Redundant images that inflate dataset size without adding value\n\n- Uniqueness: How distinct each sample is from others (low = redundant, high = informative)\n\n- Representativeness: How well a sample represents the overall distribution (high = typical, low = outlier)\n\nAs an example, let's compute uniqueness.\n\nIn a nutshell, uniqueness measures how far a sample is from its nearest neighbors in embedding space, with higher values indicating the sample is more isolated/distinct from other samples in the dataset.\n\nIt's computed by finding each sample's K nearest neighbors (K=3), calculating a weighted average of the distances to those neighbors, and normalizing the result to a 0-1 scale.","metadata":{}},{"id":"437a0b59","cell_type":"code","source":"# Compute uniqueness scores\nfob.compute_uniqueness(\n    dataset,\n    embeddings=\"radio_embeddings\",\n    similarity_index=sim_index\n    )","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Retrieving embeddings from similarity index...\n","Computing uniqueness...\n","Uniqueness computation complete\n"]}],"execution_count":18},{"id":"ac4e5076","cell_type":"markdown","source":"**In the App:**\n- Open the [Embeddings panel](https://docs.voxel51.com/user_guide/app.html#embeddings-panel)\n- Color by `modality` — do CT, MRI, X-ray form distinct clusters?\n- Color by `body_part` — do anatomical regions separate?\n- Color by `content_type` — do question types cluster?\n\n**What you're looking for:**\n- Clear separation = model has a chance\n- Everything mixed together = fundamental representation problem\n","metadata":{}},{"id":"b8da34b5","cell_type":"code","source":"# Relaunch app to see embeddings panel\nimport fiftyone as fo\nsession = fo.launch_app(dataset)","metadata":{},"outputs":[{"data":{"text/html":["\n","        <iframe\n","            width=\"100%\"\n","            height=\"800\"\n","            src=\"http://localhost:5151/?notebook=True&subscription=7642949a-22e5-4f5e-99b8-734529a7ab1e\"\n","            frameborder=\"0\"\n","            allowfullscreen\n","            \n","        ></iframe>\n","        "],"text/plain":["<IPython.lib.display.IFrame at 0x76263d82a3d0>"]},"metadata":{},"output_type":"display_data"}],"execution_count":19},{"id":"3bb110a0","cell_type":"markdown","source":"![Explore MedGemma Embeddings](https://raw.githubusercontent.com/harpreetsahota204/medgemma_kaggle_competition/main/gifs/explore_medgemma_embeddings.gif)\n","metadata":{}},{"id":"c30fc6cc","cell_type":"markdown","source":"---\n## 4. Run MedGemma Inference\n\nNow let's run MedGemma 1.5 on the VQA task and store predictions.\n\nFiftyOne is open-source and hackable, with a robust framework for \n[building Plugins](https://docs.voxel51.com/plugins/developing_plugins.html) that extend \nand customize the tool. Browse this [curated collection of plugins](https://docs.voxel51.com/plugins/) \nto see integrations with various computer vision models and AI tools.","metadata":{}},{"id":"b703dd1b","cell_type":"markdown","source":"### Register and load MedGemma","metadata":{}},{"id":"13536799","cell_type":"code","source":"foz.register_zoo_model_source(\n    \"https://github.com/harpreetsahota204/medgemma_1_5\",\n    overwrite=True\n)\n\nfoz.download_zoo_model(\n    \"https://github.com/harpreetsahota204/medgemma_1_5\",\n    model_name=\"google/medgemma-1.5-4b-it\",\n)","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading https://github.com/harpreetsahota204/medgemma_1_5...\n","  152.7Kb [65.3ms elapsed, ? remaining, 2.3Mb/s] \n","Overwriting existing model source '/home/harpreet/fiftyone/__models__/medgemma_1_5'\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"baf7a3897fe340c3a13893b95e98d648","version_major":2,"version_minor":0},"text/plain":["Downloading (incomplete total...): 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8081bbcd056947daa83344d906fd1c97","version_major":2,"version_minor":0},"text/plain":["Fetching 15 files:   0%|          | 0/15 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["(<fiftyone.zoo.models.RemoteZooModel at 0x76263d894a50>,\n"," '/home/harpreet/fiftyone/__models__/medgemma_1_5/medgemma-1.5-4b-it')"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"execution_count":20},{"id":"976a9b04","cell_type":"code","source":"medgemma = foz.load_zoo_model(\"google/medgemma-1.5-4b-it\")","metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4ab299e537894ec6a0c7b6a70dab6742","version_major":2,"version_minor":0},"text/plain":["Downloading (incomplete total...): 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2460f52a3fed41d2a47734a9c2ca86e6","version_major":2,"version_minor":0},"text/plain":["Fetching 15 files:   0%|          | 0/15 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["`torch_dtype` is deprecated! Use `dtype` instead!\n"]},{"name":"stdout","output_type":"stream","text":["Loading MedGemma model from /home/harpreet/fiftyone/__models__/medgemma_1_5/medgemma-1.5-4b-it\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"49ab7e82d7ce4c0e8edff0630a835669","version_major":2,"version_minor":0},"text/plain":["Loading weights:   0%|          | 0/883 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"execution_count":21},{"id":"84b87872","cell_type":"markdown","source":"### Configure for VQA","metadata":{}},{"id":"86603789","cell_type":"code","source":"# Set operation mode\nmedgemma.operation = \"classify\"\n\n# Set a custom system prompt\nmedgemma.system_prompt = \"\"\"You are an expert radiologist, histopathologist, ophthalmologist, and dermatologist.\n\nYour expert opinion is needed for answering questions about medical images.\n\nReport your answer as JSON array in this format: \n\n```json\n{\n    \"classifications\": [\n        {\n            \"label\": \"descriptive medical condition or relevant label\"\n            ...,\n        }\n    ]\n}\n```\n\nAlways return your response as valid JSON wrapped in ```json blocks.  You must produce only a single word answer. Do not report your confidence.\n\"\"\"","metadata":{},"outputs":[],"execution_count":22},{"id":"4b9c3158","cell_type":"markdown","source":"### Running Inference on Multi-Question Samples\n\nSince each image has multiple Q&A pairs (`question_0`/`answer_0` through `question_19`/`answer_19`),\nwe have a few options for running inference:\n\n1. **Pick one question per image** (simplest) - use `prompt_field=\"question_0\"`\n2. **Run on all questions** - loop through question fields\n3. **Flatten the dataset** - create a new sample per Q&A pair\n\nLet's start simple by running on the first question of each image:","metadata":{}},{"id":"4dc8a044","cell_type":"code","source":"dataset.apply_model(\n    medgemma,\n    label_field=\"pred_answer_0\",\n    prompt_field=\"question_0\",  # Use the first question for each image\n    batch_size=32,\n    num_workers=4,\n)","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"]},{"name":"stdout","output_type":"stream","text":[" 100% |███████████████████| 50/50 [8.6s elapsed, 0s remaining, 5.8 samples/s]   \n"]}],"execution_count":23},{"id":"cb323b9f","cell_type":"markdown","source":"### Inspect predictions","metadata":{}},{"id":"aecbb399","cell_type":"code","source":"dataset.first()['pred_answer_0']","metadata":{},"outputs":[{"data":{"text/plain":["<Classifications: {\n","    'classifications': [\n","        <Classification: {\n","            'id': '696ea5962b435087800e5e3b',\n","            'tags': [],\n","            'label': 'Chest X-ray',\n","            'confidence': None,\n","            'logits': None,\n","        }>,\n","    ],\n","    'logits': None,\n","}>"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"execution_count":24},{"id":"695c286b","cell_type":"code","source":"# Look at a few samples\nfor sample in dataset.take(5):\n    print(f\"Image: {sample.filepath.split('/')[-1]}\")\n    print(f\"Modality: {sample.modality.label}\")\n    print(f\"Q: {sample.question_0}\")\n    print(f\"GT: {sample.answer_0.label if sample.answer_0 else 'None'}\")\n    print(f\"Pred: {sample.pred_answer_0.classifications[0].label if sample.pred_answer_0 else 'None'}\")\n    print(\"-\" * 50)","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Image: source_xmlab314.jpg\n","Modality: X-Ray\n","Q: Is this image taken via X-Ray?\n","GT: Yes\n","Pred: X-Ray\n","--------------------------------------------------\n","Image: source_xmlab42.jpg\n","Modality: MRI\n","Q: What modality is used to take this image?\n","GT: MRI\n","Pred: MRI\n","--------------------------------------------------\n","Image: source_xmlab361.jpg\n","Modality: X-Ray\n","Q: Is this image taken via X-Ray?\n","GT: Yes\n","Pred: X-Ray\n","--------------------------------------------------\n","Image: source_xmlab478.jpg\n","Modality: MRI\n","Q: What modality is used to take this image?\n","GT: MRI\n","Pred: MRI\n","--------------------------------------------------\n","Image: source_xmlab516.jpg\n","Modality: MRI\n","Q: How was this image taken?\n","GT: MRI\n","Pred: MRI\n","--------------------------------------------------\n"]}],"execution_count":25},{"id":"c2c43ea2","cell_type":"markdown","source":"---\n## 5. Evaluate Performance\n\nLet's compute accuracy—but more importantly, let's slice it to find patterns.\n\nFiftyOne provides [evaluation methods](https://docs.voxel51.com/user_guide/evaluation.html) \nfor various task types including [detection](https://docs.voxel51.com/user_guide/evaluation.html#detections), [classification](https://docs.voxel51.com/user_guide/evaluation.html#classifications), and [segmentation](https://docs.voxel51.com/user_guide/evaluation.html#semantic-segmentations).\n\n##### We need to make a conversion from Classifications → ⁠Classification\n\n\nThe implementation of MedGemma outputs a FiftyOne *Classifications* object (notice it's plural), but to run the evaluation for classification we need a FiftyOne *Classification* (singluar)\n\nFiftyOne's `evaluate_classifications()` only works with **single-label** classification fields (`Classification`), not multilabel containers (`Classifications`).\n\n**What you need to do:**\n\n1. Choose one label per sample (e.g., first label, highest confidence)\n2. Store it as a `Classification` field\n3. Pass that field to `evaluate_classifications()`\n\n**Read more in the docs:**\n\n- [Classification evaluation overview](https://docs.voxel51.com/user_guide/evaluation.html#classifications)\n- [Simple evaluation example](https://docs.voxel51.com/user_guide/evaluation.html#id4)\n- [Binary evaluation example](https://docs.voxel51.com/user_guide/evaluation.html#binary-evaluation)\n- [Classification evaluation tutorial](https://docs.voxel51.com/tutorials/evaluate_classifications.html#Evaluating-model-with-FiftyOne)\n","metadata":{}},{"id":"de69c3ef","cell_type":"code","source":"import fiftyone as fo\n\n# assume dataset has a multilabel field \"multi\" of type fo.Classifications\n# and we want a single-label field \"single\" of type fo.Classification\n\nfor sample in dataset:\n    cls_list = sample[\"pred_answer_0\"].classifications if sample[\"pred_answer_0\"] is not None else []\n\n    if cls_list:\n        # choose one classification; here we take the first\n        chosen = cls_list[0]\n        sample[\"pred_answer_0_as_cls\"] = fo.Classification(\n            label=chosen.label,\n        )\n    else:\n        sample[\"pred_answer_0_as_cls\"] = None\n\n    sample.save()","metadata":{},"outputs":[],"execution_count":26},{"id":"09752ed7","cell_type":"code","source":"# Evaluate the predictions in the `predictions` field with respect to the\n# labels in the `ground_truth` field\nclassify_results = dataset.evaluate_classifications(\n    \"pred_answer_0_as_cls\",\n    gt_field=\"answer_0\",\n    eval_key=\"eval_ans_0\",\n)","metadata":{},"outputs":[],"execution_count":27},{"id":"08b71b54","cell_type":"code","source":"# Print a classification report\nclassify_results.print_report()","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["                  precision    recall  f1-score   support\n","\n","           Axial       0.00      0.00      0.00         0\n","              CT       0.75      0.38      0.50         8\n","         CT scan       0.00      0.00      0.00         0\n","     Chest X-ray       0.00      0.00      0.00         0\n","             MRI       0.74      1.00      0.85        14\n","        MRI scan       0.00      0.00      0.00         0\n","              No       1.00      0.29      0.44         7\n","              PA       0.00      0.00      0.00         0\n","           Right       0.00      0.00      0.00         1\n","Transverse Plane       0.00      0.00      0.00         2\n","           X-Ray       0.20      0.12      0.15         8\n","             Yes       0.00      0.00      0.00        10\n","   coronal plane       0.00      0.00      0.00         0\n","     lower right       0.00      0.00      0.00         0\n","\n","        accuracy                           0.40        50\n","       macro avg       0.19      0.13      0.14        50\n","    weighted avg       0.50      0.40      0.40        50\n","\n"]}],"execution_count":28},{"id":"2f826bd7","cell_type":"markdown","source":"You can also open the evaluation panel in the app for a more interactive evaluation experience.\n\nYou can use [Scenario Analysis](https://docs.voxel51.com/user_guide/app.html#scenario-analysis) \nfor a deep dive into model behavior across different scenarios. This helps uncover edge cases, \nidentify annotation errors, and understand performance variations in different contexts.\n\n![Eval MedGemma Classifications](https://raw.githubusercontent.com/harpreetsahota204/medgemma_kaggle_competition/main/gifs/medgemma_eval.gif)\n\n\n### Visual Question Answering\n\nYou can also use MedGemma for visual question answering to get a more open-ended answer:","metadata":{}},{"id":"da21ac1a","cell_type":"code","source":"medgemma.operation=\"vqa\" #change operation\n\nmedgemma.system_prompt = None #reset system prompt, use default system prompt for vqa\n\nprint(medgemma.system_prompt) #print the default vqa system prompt","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["You are an expert radiologist, histopathologist, ophthalmologist, and dermatologist. You are asked to provide leverage your expertise to answers to medical questions.\n","\n","You may be provided with a simple query, patient history with a complex query, asked to provide a medical diagnosis, or any variety of medical question.\n","\n"]}],"execution_count":29},{"id":"94e79c99","cell_type":"code","source":"dataset.apply_model(\n    medgemma,\n    label_field=\"free_text_answer_0\",\n    prompt_field=\"question_0\",  # Use the first question for each image\n    batch_size=32,\n    num_workers=4,\n)","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":[" 100% |███████████████████| 50/50 [24.4s elapsed, 0s remaining, 2.1 samples/s]   \n"]}],"execution_count":30},{"id":"27504225","cell_type":"code","source":"dataset.first()['free_text_answer_0']","metadata":{},"outputs":[{"data":{"text/plain":["'This image is a **chest X-ray (CXR)**.'"]},"execution_count":31,"metadata":{},"output_type":"execute_result"}],"execution_count":31},{"id":"83638f85","cell_type":"markdown","source":"### Running on All Questions (Optional)\n\nIf you want to evaluate on all Q&A pairs, you can loop through the question fields.\nThis stores predictions for each question in separate fields:","metadata":{}},{"id":"d643f5fa","cell_type":"code","source":"\n# # Run inference on all questions (takes longer)\n# for i in range(20):  # Up to 20 questions per image\n#     q_field = f\"question_{i}\"\n#     pred_field = f\"free_text_answer_{i}\"\n#     \n#     # Only run if this question exists in any sample\n#     if dataset.count(q_field) > 0:\n#         print(f\"Running inference on {q_field}...\")\n#         dataset.apply_model(\n#             medgemma,\n#             label_field=pred_field,\n#             prompt_field=q_field,\n#             batch_size=32,\n#             num_workers=4,\n#         )","metadata":{},"outputs":[],"execution_count":32},{"id":"78f6dc48","cell_type":"markdown","source":"### Add correctness field\n\n\nSince MedGemma produces verbose answers in VQA mode, we use Gemma 3 270m as a semantic judge to determine if the predicted answer is correct rather than relying on exact string matching.\n\nUse [`values()`](https://docs.voxel51.com/api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.values) \nto efficiently extract field values across all samples, and \n[`set_values()`](https://docs.voxel51.com/api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.set_values) \nto add computed fields back to the dataset.","metadata":{}},{"id":"c01c1ac5","cell_type":"code","source":"from transformers import pipeline\nfrom tqdm import tqdm\nimport torch\n\n# Load LLM judge\njudge = pipeline(\n    \"text-generation\",\n    model=\"google/gemma-3-270m-it\",\n    device=\"cuda\",\n    dtype=\"auto\"\n)\n\n# Get data\ngt_values = dataset.values(\"free_text_answer_0\")\npred_values = dataset.values(\"pred_answer_0\")\nquestions = dataset.values(\"question_0\")\n\nSYSTEM_PROMPT = \"\"\"You are an expert medical evaluator. Your task is to determine if a predicted answer correctly answers a question, given the ground truth answer. The predicted answer may be more verbose or phrased differently, but should be semantically equivalent to the ground truth.\n\nRespond with ONLY \"CORRECT\" or \"INCORRECT\" - no other text.\"\"\"\n\ndef is_correct(question, gt, pred):\n    if not pred or not gt:\n        return False\n    \n    messages = [\n        {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": SYSTEM_PROMPT}]},\n        {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": f\"\"\"Question: {question}\nGround Truth Answer: {gt}\nPredicted Answer: {pred}\n\nIs the Predicted Answer CORRECT or INCORRECT?\"\"\"}]}\n    ]\n    \n    output = judge(messages, max_new_tokens=16, do_sample=False)\n    return \"CORRECT\" in output[0][\"generated_text\"][-1][\"content\"].upper()\n\n# Evaluate and save\nresults = [is_correct(q, gt, p) for q, gt, p in tqdm(zip(questions, gt_values, pred_values), total=len(questions))]\ndataset.set_values(\"is_correct_0\", results)\ndataset.save()\n\nprint(f\"{sum(results)}/{len(results)} answers judged as correct\")\n","metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6272cfc1726c418daba4b68a8cd1e4ab","version_major":2,"version_minor":0},"text/plain":["Loading weights:   0%|          | 0/236 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/50 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"," 20%|██        | 10/50 [00:00<00:03, 13.27it/s]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","100%|██████████| 50/50 [00:03<00:00, 13.44it/s]\n"]},{"name":"stdout","output_type":"stream","text":["50/50 answers judged as correct\n"]}],"execution_count":33},{"id":"774e144e","cell_type":"markdown","source":"### Overall accuracy (on question_0, LLM-judged)\n","metadata":{}},{"id":"e5470284","cell_type":"code","source":"# Clean up judge pipeline to free GPU memory for subsequent operations\nfrom fiftyone import ViewField as F\ndel judge\nimport gc\ngc.collect()\ntorch.cuda.empty_cache()\n\n# Calculate overall accuracy using LLM-judged correctness\ncorrect = dataset.match(F(\"is_correct_0\") == True)\ntotal = len(dataset)\naccuracy = len(correct) / total\n\nprint(f\"Overall Accuracy (Q0, LLM-judged): {accuracy:.2%} ({len(correct)}/{total})\")","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Overall Accuracy (Q0, LLM-judged): 100.00% (50/50)\n"]}],"execution_count":34},{"id":"6b8a8643","cell_type":"markdown","source":"### Accuracy by answer type\n\nCLOSED questions (yes/no) should be easier than OPEN (free-form) ones.","metadata":{}},{"id":"6b763893","cell_type":"code","source":"print(\"Accuracy by Answer Type:\")\nfor atype in dataset.distinct(\"answer_type.label\"):\n    view = dataset.match(F(\"answer_type.label\") == atype)\n    correct_view = view.match(F(\"is_correct_0\") == True)\n    acc = len(correct_view) / len(view) if len(view) > 0 else 0\n    print(f\"  {atype}: {acc:.2%} ({len(correct_view)}/{len(view)})\")","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy by Answer Type:\n","  CLOSED: 100.00% (22/22)\n","  OPEN: 100.00% (28/28)\n"]}],"execution_count":35},{"id":"14d1344d","cell_type":"markdown","source":"### Accuracy by modality\n\nDoes MedGemma perform differently on CT vs MRI vs X-Ray?","metadata":{}},{"id":"9e2218aa","cell_type":"code","source":"print(\"\\nAccuracy by Modality:\")\nfor modality in dataset.distinct(\"modality.label\"):\n    view = dataset.match(F(\"modality.label\") == modality)\n    correct_view = view.match(F(\"is_correct_0\") == True)\n    acc = len(correct_view) / len(view) if len(view) > 0 else 0\n    print(f\"  {modality}: {acc:.2%} ({len(correct_view)}/{len(view)})\")","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Accuracy by Modality:\n","  CT: 100.00% (17/17)\n","  MRI: 100.00% (21/21)\n","  X-Ray: 100.00% (12/12)\n"]}],"execution_count":36},{"id":"6c44cec9","cell_type":"markdown","source":"### Accuracy by anatomical location","metadata":{}},{"id":"9446dcc7","cell_type":"code","source":"print(\"\\nAccuracy by Location:\")\nresults = []\nfor location in dataset.distinct(\"location.label\"):\n    view = dataset.match(F(\"location.label\") == location)\n    correct_view = view.match(F(\"is_correct_0\") == True)\n    acc = len(correct_view) / len(view) if len(view) > 0 else 0\n    results.append((location, acc, len(view)))\n\n# Sort by accuracy\nfor location, acc, n in sorted(results, key=lambda x: x[1]):\n    print(f\"  {location}: {acc:.2%} (n={n})\")","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Accuracy by Location:\n","  Abdomen: 100.00% (n=13)\n","  Brain: 100.00% (n=2)\n","  Brain_Tissue: 100.00% (n=15)\n","  Chest_lung: 100.00% (n=3)\n","  Lung: 100.00% (n=14)\n","  Neck: 100.00% (n=1)\n","  Pelvic Cavity: 100.00% (n=2)\n"]}],"execution_count":37},{"id":"4b15028e","cell_type":"markdown","source":"**This is where it gets interesting.** \n\nYou might find things like:\n- \"MedGemma struggles on Brain MRI images\"  \n- \"Abnormality detection is worse on Abdomen CT than Lung X-Ray\"\n- \"OPEN questions have much lower accuracy than CLOSED questions\"\n\nThese are *actionable insights*, not just numbers.","metadata":{}},{"id":"b19c4059","cell_type":"markdown","source":"## Detection with MedGemma\n\nYou can use MedGemma to localize anatomical structures and pathologies in medical images. The model outputs bounding boxes in FiftyOne's Detections format.","metadata":{}},{"id":"61229050","cell_type":"code","source":"# Set detection mode\nmedgemma.operation = \"detect\"\n\n# Get labels to detect (e.g., from ground truth)\nlabels = dataset.distinct(\"detections.detections.label\")\nlabels_str = \", \".join(labels)\n\n# Prompt for localization\nmedgemma.prompt = f\"\"\"Locate the following in this scan: {labels_str}. \nOutput the final answer in the format \"Final Answer: X\" where X is a JSON list of objects. \nThe object needs a \"box_2d\" and \"label\" key. \nIf the object is not present in the scan, skip it and don't output anything for that object.\nAnswer:\"\"\"\n\n# Apply detection\ndataset.apply_model(\n    medgemma,\n    label_field=\"pred_detection\",\n    batch_size=32,\n    num_workers=4,\n)","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":[" 100% |███████████████████| 50/50 [4.7m elapsed, 0s remaining, 0.2 samples/s]   \n"]}],"execution_count":38},{"id":"caa937ea","cell_type":"markdown","source":"We can then use [FiftyOne's evaluation API](https://docs.voxel51.com/user_guide/evaluation.html) to see how well the initial results. You can [`evaluate_detections()` method](https://docs.voxel51.com/user_guide/evaluation.html#detections) to evaluate the predictions of an object detection model stored in a [`Detections`](https://docs.voxel51.com/api/fiftyone.core.labels.html#fiftyone.core.labels.Detections), [`Polylines`](https://docs.voxel51.com/api/fiftyone.core.labels.html#fiftyone.core.labels.Polylines), or [`Keypoints`](https://docs.voxel51.com/api/fiftyone.core.labels.html#fiftyone.core.labels.Keypoints) field of your dataset or of a temporal detection model stored in a [`TemporalDetections`](https://docs.voxel51.com/api/fiftyone.core.labels.html#fiftyone.core.labels.TemporalDetection) field of your dataset.","metadata":{}},{"id":"f4d70489","cell_type":"code","source":"results = dataset.evaluate_detections(\n    \"pred_detection\",        \n    gt_field=\"detections\",  \n    eval_key=\"initial_detection_eval\",\n    tolerance=2\n)","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Evaluating detections...\n"," 100% |███████████████████| 50/50 [251.9ms elapsed, 0s remaining, 198.5 samples/s]      \n"]}],"execution_count":39},{"id":"ba306862","cell_type":"markdown","source":"The `evaluate_detections()` method returns a [`DetectionResults` instance](https://docs.voxel51.com/api/fiftyone.utils.eval.detection.html#fiftyone.utils.eval.detection.DetectionResults) that provides a variety of methods for generating various aggregate evaluation reports about your model.\n\nIn addition, when you specify an `eval_key` parameter, a number of helpful fields will be populated on each sample and its predicted/ground truth objects that you can leverage via the FiftyOne App to interactively explore the strengths and weaknesses of your model on individual samples.\n\nYou can print the report to get a high-level picture of the model performance:","metadata":{}},{"id":"4c7d894e","cell_type":"code","source":"results.print_report()","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["                           precision    recall  f1-score   support\n","\n","                  Bladder       0.00      0.00      0.00         1\n","              Brain Edema       0.00      0.00      0.00        20\n","    Brain Enhancing Tumor       0.00      0.00      0.00        10\n","Brain Non-enhancing Tumor       0.00      0.00      0.00        17\n","             Cardiomegaly       0.00      0.00      0.00         3\n","                Clavicles       0.00      0.00      0.00         0\n","                    Colon       0.00      0.00      0.00         6\n","                 Duodenum       0.00      0.00      0.00         2\n","                Esophagus       0.00      0.00      0.00         2\n","                    Heart       0.00      0.00      0.00         3\n","                 Left Eye       0.00      0.00      0.00         1\n","              Left Kidney       0.00      0.00      0.00         4\n","                Left Lung       0.00      0.00      0.00         4\n","       Left Temporal Lobe       0.00      0.00      0.00         1\n","          Lines/Catheters       0.00      0.00      0.00         0\n","                    Liver       0.11      0.14      0.12        14\n","                     Mass       0.00      0.00      0.00         3\n","                   Nodule       0.00      0.00      0.00         3\n","                Pneumonia       0.00      0.00      0.00         3\n","                   Rectum       0.00      0.00      0.00         1\n","                     Ribs       0.00      0.00      0.00         0\n","                Right Eye       0.00      0.00      0.00         1\n","             Right Kidney       0.00      0.00      0.00         3\n","               Right Lung       0.00      0.00      0.00         5\n","      Right Temporal Lobe       0.00      0.00      0.00         1\n","                 Scapulae       0.00      0.00      0.00         0\n","              Small Bowel       0.00      0.00      0.00         9\n","              Spinal Cord       0.00      0.00      0.00         9\n","                   Spleen       0.00      0.00      0.00         7\n","                  Stomach       0.00      0.00      0.00         1\n","                  Trachea       0.00      0.00      0.00         1\n","                Vertebrae       0.00      0.00      0.00         0\n","\n","                micro avg       0.01      0.01      0.01       135\n","                macro avg       0.00      0.00      0.00       135\n","             weighted avg       0.01      0.01      0.01       135\n","\n"]}],"execution_count":40},{"id":"ddda8c24","cell_type":"markdown","source":"You can inspect the quality of the detections also use the model evaluation panel in the app:\n\n\n![Eval MedGemma Classifications](https://raw.githubusercontent.com/harpreetsahota204/medgemma_kaggle_competition/main/gifs/medgemma_eval_detections.gif)\n\n\n\nThe results look...not great.\n\nBut, this means we have a starting point. Now that we know the model can predict bounding boxes we can fine-tune it on our dataset!\n","metadata":{}},{"id":"724416ca","cell_type":"markdown","source":"If you're running this notebook end to end, then it's a good idea to clear up some GPU memory:","metadata":{}},{"id":"d271f7cb","cell_type":"code","source":"del medgemma\ndel medsiglip\n\nimport gc\ngc.collect()\ntorch.cuda.empty_cache()","metadata":{},"outputs":[],"execution_count":null},{"id":"edc5bd88","cell_type":"markdown","source":"---\n## 7. Fine-Tuning MedGemma for Localization\n\nYou've explored the data, identified failure patterns, and have hypotheses about what to fix.\nNow let's fine-tune MedGemma to output bounding box coordinates for localization tasks.\n\nThis section demonstrates converting datasets to PyTorch format for training.\n\nWe'll follow these steps:\n1. Define a [`GetItem`](https://docs.voxel51.com/api/fiftyone.utils.torch.html#fiftyone.utils.torch.GetItem) subclass to extract and transform data from FiftyOne\n2. Create train/val splits and flatten detections using [`to_patches()`](https://docs.voxel51.com/api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.to_patches)\n3. Convert to PyTorch datasets using [`to_torch()`](https://docs.voxel51.com/api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.to_torch)\n4. Set up QLoRA fine-tuning with the TRL library's `SFTTrainer`","metadata":{}},{"id":"52f74ac2","cell_type":"markdown","source":"### Install fine-tuning dependencies","metadata":{}},{"id":"29e1e592","cell_type":"code","source":"!pip install --upgrade --quiet bitsandbytes peft trl","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n","\u001b[0m"]}],"execution_count":41},{"id":"7ec2e337","cell_type":"markdown","source":"### Step 1: Define the GetItem subclass\n\nFiftyOne's [`GetItem`](https://docs.voxel51.com/api/fiftyone.utils.torch.html#fiftyone.utils.torch.GetItem) \nclass is the bridge between FiftyOne and PyTorch. It tells FiftyOne:\n\n1. **What fields to extract** from each sample (via `required_keys`)\n2. **How to transform them** into your desired format (via `__call__`)\n\nThe `field_mapping` parameter is important when working with patches. In a patches view,\nthe detection data lives in the original field name (e.g., \"detections\"), but we want \nto access it with a generic name in our code.\n\n`field_mapping={\"detection\": \"detections\"}` means:\n- In our code, we write `d[\"detection\"]`\n- FiftyOne knows to pull from the \"detections\" field\n\nThis makes our `GetItem` reusable across datasets with different field names.","metadata":{}},{"id":"6b8eb7a1","cell_type":"code","source":"from typing import Any\nfrom PIL import Image\nfrom fiftyone.utils.torch import GetItem\n\n# System prompt for localization task\nLOCALIZATION_SYSTEM_PROMPT = \"\"\"Instructions:\nThe following user query will require outputting bounding boxes. The format of bounding boxes coordinates is [y0, x0, y1, x1] where (y0, x0) must be top-left corner and (y1, x1) the bottom-right corner. This implies that x0 < x1 and y0 < y1. Always normalize the x and y coordinates the range [0, 1000], meaning that a bounding box starting at 15% of the image width would be associated with an x coordinate of 150. You MUST output a single parseable json list of objects enclosed into ```json...``` brackets, for instance ```json[{\"box_2d\": [800, 3, 840, 471], \"label\": \"car\"}, {\"box_2d\": [400, 22, 600, 73], \"label\": \"dog\"}]``` is a valid output. Now answer to the user query.\n\nRemember \"left\" refers to the patient's left side where the heart is and sometimes underneath an L in the upper right corner of the image.\"\"\"\n\n\nclass LocalizationGetItem(GetItem):\n    \"\"\"\n    Extracts and transforms detection data for MedGemma localization fine-tuning.\n    \n    Each patch sample (after to_patches()) contains:\n    - filepath: path to the full image\n    - detection: the Detection object (bbox, label, etc.)\n    - metadata: image dimensions\n    \n    We transform this into MedGemma's expected message format with:\n    - System prompt explaining the bbox output format\n    - User message with the localization query\n    - Assistant message with the target bbox in JSON format\n    \"\"\"\n    \n    def __init__(self, field_mapping=None):\n        # Must call super().__init__() with field_mapping - this sets up\n        # the internal mapping that FiftyOne uses to pull the right fields\n        super().__init__(field_mapping=field_mapping)\n    \n    @property\n    def required_keys(self):\n        # These are the keys we'll access in __call__.\n        # 'detection' is a virtual key that gets mapped to the real field\n        # via field_mapping. 'filepath' and 'metadata' are standard fields\n        # that exist on all FiftyOne samples.\n        return [\"filepath\", \"detection\", \"metadata\"]\n    \n    def __call__(self, d):\n        \"\"\"\n        Transform a FiftyOne sample dict into MedGemma fine-tuning format.\n        \n        This is where the FiftyOne → MedGemma conversion happens:\n        - FiftyOne bbox format [x, y, w, h] in [0,1] \n        - MedGemma format [y0, x0, y1, x1] normalized to [0, 1000]\n        \"\"\"\n        filepath = d[\"filepath\"]\n        detection = d[\"detection\"]\n        \n        # Get the label from the detection\n        label = detection.label\n        \n        # --- Bounding Box Conversion ---\n        # FiftyOne stores bboxes as [x, y, width, height] with values in [0, 1]\n        # MedGemma expects [y0, x0, y1, x1] normalized to [0, 1000]\n        rx, ry, rw, rh = detection.bounding_box\n        \n        # Convert to [y0, x0, y1, x1] format, scaled to [0, 1000]\n        x0 = int(rx * 1000)\n        y0 = int(ry * 1000)\n        x1 = int((rx + rw) * 1000)\n        y1 = int((ry + rh) * 1000)\n        \n        # Format as [y0, x0, y1, x1] per the prompt instructions\n        bbox_normalized = [y0, x0, y1, x1]\n        \n        # --- Construct Messages ---\n        # Format the target response as JSON\n        target_json = f'```json[{{\"box_2d\": {bbox_normalized}, \"label\": \"{label}\"}}]```'\n        \n        # Build the message payload in chat format\n        messages = [\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\"type\": \"image\"},\n                    {\"type\": \"text\", \"text\": f\"{LOCALIZATION_SYSTEM_PROMPT}\\n\\nLocate the {label} in this medical image.\"},\n                ],\n            },\n            {\n                \"role\": \"assistant\",\n                \"content\": [\n                    {\"type\": \"text\", \"text\": target_json},\n                ],\n            },\n        ]\n        \n        return {\n            \"filepath\": filepath,\n            \"image\": Image.open(filepath).convert(\"RGB\"),\n            \"messages\": messages,\n            \"label\": label,\n        }","metadata":{"lines_to_next_cell":1},"outputs":[],"execution_count":3},{"id":"1e60a36d","cell_type":"markdown","source":"### Step 2: Create train/val split and flatten detections\n\nSince our dataset doesn't have existing train/val [tags](https://docs.voxel51.com/user_guide/basics.html#tags), \nwe'll create them using [`random_split()`](https://docs.voxel51.com/api/fiftyone.utils.random.html#fiftyone.utils.random.random_split).\n\nThen we use [`to_patches()`](https://docs.voxel51.com/api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.to_patches) \nto flatten the dataset so each detection becomes its own sample.\n\n**Key insight:** `to_patches(field)` creates a view where each detection in that field becomes \nits own sample. If you have 100 images with 5 detections each, `to_patches` gives you 500 patch samples. \nThis is perfect for instance-level training.","metadata":{}},{"id":"376188c1","cell_type":"code","source":"import fiftyone.utils.random as four\nfrom fiftyone import ViewField as F\n\n# Filter to samples that have detections\nhas_detections_view = dataset.match(F(\"detections\") != None)\nprint(f\"Samples with detections: {len(has_detections_view)}\")\n\n# Create train/val split (80/20)\nfour.random_split(has_detections_view, {\"train\": 0.8, \"val\": 0.2})","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Samples with detections: 50\n"]}],"execution_count":4},{"id":"20e62fad","cell_type":"code","source":"# Filter by split tags using match_tags()\n# https://docs.voxel51.com/api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.match_tags\ntrain_view = has_detections_view.match_tags(\"train\")\nval_view = has_detections_view.match_tags(\"val\")\n\nprint(f\"Samples - train: {len(train_view)}, val: {len(val_view)}\")","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Samples - train: 40, val: 10\n"]}],"execution_count":5},{"id":"758b73b4","cell_type":"code","source":"# Flatten using to_patches() - each detection becomes its own sample\ntrain_patches = train_view.to_patches(\"detections\")\nval_patches = val_view.to_patches(\"detections\")\n\nprint(f\"Patches - train: {len(train_patches)}, val: {len(val_patches)}\")","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Patches - train: 115, val: 20\n"]}],"execution_count":6},{"id":"8499de29","cell_type":"markdown","source":"### Step 3: Convert to PyTorch datasets\n\nUse [`to_torch()`](https://docs.voxel51.com/api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.to_torch) \nwith our `GetItem` class to create PyTorch-compatible datasets.\n\nIn the patches view, each sample's detection data lives in the original field (e.g., \"detections\"). \nThe `field_mapping` lets us access it with a generic name in our `GetItem` code, making the class \nreusable across different datasets.","metadata":{}},{"id":"cddcc44d","cell_type":"code","source":"# Set up field mapping - in patches view, each sample's detection data \n# lives in the original field \"detections\"\nfield_mapping = {\"detection\": \"detections\"}\n\n# Create GetItem instances\ntrain_getter = LocalizationGetItem(field_mapping=field_mapping)\nval_getter = LocalizationGetItem(field_mapping=field_mapping)\n\n# Convert to PyTorch datasets\ntrain_dataset = train_patches.to_torch(train_getter)\nval_dataset = val_patches.to_torch(val_getter)\n\nprint(f\"Train dataset size: {len(train_dataset)}\")\nprint(f\"Val dataset size: {len(val_dataset)}\")","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Train dataset size: 115\n","Val dataset size: 20\n"]}],"execution_count":7},{"id":"959634c5","cell_type":"code","source":"# Verify the data format\nsample = train_dataset[0]\nprint(\"Sample keys:\", sample.keys())\nprint(\"Label:\", sample[\"label\"])\nprint(\"Messages structure:\")\nfor msg in sample[\"messages\"]:\n    print(f\"  Role: {msg['role']}\")","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Sample keys: dict_keys(['filepath', 'image', 'messages', 'label'])\n","Label: Cardiomegaly\n","Messages structure:\n","  Role: user\n","  Role: assistant\n"]}],"execution_count":8},{"id":"d4286f6b","cell_type":"markdown","source":"### Step 4: Load MedGemma with QLoRA configuration\n\nWe use 4-bit quantization (QLoRA) to reduce memory requirements while maintaining\nfine-tuning capability. This allows fine-tuning on consumer GPUs.","metadata":{}},{"id":"d4553265","cell_type":"code","source":"\nimport torch\nfrom transformers import AutoProcessor, AutoModelForImageTextToText, BitsAndBytesConfig\n\nmodel_id = \"google/medgemma-1.5-4b-it\"\n\nmodel_kwargs = dict(\n    attn_implementation=\"eager\",\n    torch_dtype=torch.bfloat16,\n    device_map={\"\": 0},\n    quantization_config=BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_use_double_quant=True,\n        bnb_4bit_quant_type=\"nf4\",\n        bnb_4bit_compute_dtype=torch.bfloat16,\n        bnb_4bit_quant_storage=torch.bfloat16,\n    ),\n)\n\nmodel = AutoModelForImageTextToText.from_pretrained(model_id, **model_kwargs)\nprocessor = AutoProcessor.from_pretrained(model_id)\nprocessor.tokenizer.padding_side = \"right\"","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["`torch_dtype` is deprecated! Use `dtype` instead!\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"543b6429462b408ab874392aea1994ac","version_major":2,"version_minor":0},"text/plain":["Loading weights:   0%|          | 0/883 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"]}],"execution_count":9},{"id":"85093c9d","cell_type":"markdown","source":"### Step 5: Configure LoRA\n\nLoRA (Low-Rank Adaptation) allows efficient fine-tuning by only training \nsmall adapter matrices instead of all model weights.","metadata":{}},{"id":"bb60f41a","cell_type":"code","source":"from peft import LoraConfig\n\npeft_config = LoraConfig(\n    lora_alpha=16,\n    lora_dropout=0.05,\n    r=16,\n    bias=\"none\",\n    target_modules=\"all-linear\",\n    task_type=\"CAUSAL_LM\",\n    modules_to_save=[\n        \"lm_head\",\n        \"embed_tokens\",\n    ],\n)","metadata":{"lines_to_next_cell":1},"outputs":[],"execution_count":10},{"id":"09aae91b","cell_type":"markdown","source":"### Step 6: Define the collate function\n\nThe collate function processes batches by:\n1. Applying the chat template to format messages\n2. Processing images and text together\n3. Creating labels with proper masking for padding and image tokens","metadata":{}},{"id":"daf8d8aa","cell_type":"code","source":"def collate_fn(examples: list[dict[str, Any]]):\n    texts = []\n    images = []\n    \n    for example in examples:\n        # Convert image to RGB and wrap in list (processor expects list of images per sample)\n        images.append([example[\"image\"].convert(\"RGB\")])\n        \n        # Apply chat template to format the conversation\n        texts.append(processor.apply_chat_template(\n            example[\"messages\"], \n            add_generation_prompt=False, \n            tokenize=False\n        ).strip())\n    \n    # Tokenize texts and process images\n    batch = processor(text=texts, images=images, return_tensors=\"pt\", padding=True)\n    \n    # Create labels from input_ids\n    # We mask padding tokens and image tokens so they don't contribute to loss\n    labels = batch[\"input_ids\"].clone()\n    \n    # Get the image token ID to mask it\n    image_token_id = processor.tokenizer.convert_tokens_to_ids(\n        processor.tokenizer.special_tokens_map[\"boi_token\"]\n    )\n    \n    # Mask tokens that should not be used in loss computation\n    labels[labels == processor.tokenizer.pad_token_id] = -100\n    labels[labels == image_token_id] = -100\n    labels[labels == 262144] = -100  # Additional image token ID\n    \n    batch[\"labels\"] = labels\n    return batch","metadata":{"lines_to_next_cell":1},"outputs":[],"execution_count":11},{"id":"16d684dc","cell_type":"markdown","source":"### Step 7: Configure training\n\nWe use TRL's `SFTConfig` and `SFTTrainer` for a clean training setup with\nall the best practices built in.","metadata":{}},{"id":"9bd6ede8","cell_type":"code","source":"from trl import SFTConfig, SFTTrainer\n\nnum_train_epochs = 1  # Adjust based on your needs\nlearning_rate = 2e-4\n\ntraining_args = SFTConfig(\n    output_dir=\"medgemma-localization-lora\",         # Directory to save the model\n    num_train_epochs=num_train_epochs,               # Number of training epochs\n    per_device_train_batch_size=4,                   # Batch size per device during training\n    per_device_eval_batch_size=4,                    # Batch size per device during evaluation\n    gradient_accumulation_steps=4,                   # Number of steps before performing a backward/update pass\n    gradient_checkpointing=True,                     # Enable gradient checkpointing to reduce memory usage\n    optim=\"adamw_torch_fused\",                       # Use fused AdamW optimizer for better performance\n    logging_steps=50,                                # Number of steps between logs\n    save_strategy=\"epoch\",                           # Save checkpoint every epoch\n    eval_strategy=\"steps\",                           # Evaluate every `eval_steps`\n    eval_steps=50,                                   # Number of steps between evaluations\n    learning_rate=learning_rate,                     # Learning rate\n    bf16=True,                                       # Use bfloat16 precision\n    max_grad_norm=0.3,                               # Max gradient norm\n    warmup_steps=5,                               # Warmup steps\n    lr_scheduler_type=\"linear\",                      # Use linear learning rate scheduler\n    push_to_hub=False,                               # Set to True to push model to Hub\n    report_to=\"tensorboard\",                         # Report metrics to tensorboard\n    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n    dataset_kwargs={\"skip_prepare_dataset\": True},   # We preprocess manually\n    remove_unused_columns=False,                     # Keep columns for data collator\n    label_names=[\"labels\"],                          # Input keys that correspond to labels\n)","metadata":{},"outputs":[],"execution_count":12},{"id":"14dc20d7","cell_type":"markdown","source":"### Step 8: Create trainer and train!","metadata":{}},{"id":"e5838650","cell_type":"code","source":"# Workaround for MedGemma 1.5's SiglipVisionTransformer\nfrom transformers.models.siglip.modeling_siglip import SiglipVisionTransformer\nSiglipVisionTransformer.get_input_embeddings = lambda self: None\n\ntrainer = SFTTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    peft_config=peft_config,\n    processing_class=processor,\n    data_collator=collate_fn,\n)","metadata":{},"outputs":[],"execution_count":null},{"id":"a5e94c12","cell_type":"code","source":"# Start training\ntrainer.train()","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 1, 'bos_token_id': 2, 'pad_token_id': 0}.\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='2' max='8' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2/8 : < :, Epoch 0.14/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"execution_count":null},{"id":"20526cc0","cell_type":"code","source":"# Save the fine-tuned model\ntrainer.save_model()\n\n# Optional: Push to Hugging Face Hub\n# trainer.push_to_hub()","metadata":{},"outputs":[],"execution_count":null},{"id":"45ea4646","cell_type":"markdown","source":"### Clean up GPU memory","metadata":{}},{"id":"13634f23","cell_type":"code","source":"del model\ndel trainer\n\nimport gc\ngc.collect()\ntorch.cuda.empty_cache()","metadata":{},"outputs":[],"execution_count":null},{"id":"068b7a4f","cell_type":"markdown","source":"## Evaluating Your Fine-Tuned Model\n\nOf course, the above is just a blueprint for what to do. For the best results, you need to figure out the right data to train on as well as the training recipe.\n\nOnce you've fine-tuned MedGemma for localization, go back through the earlier \nsections of this notebook to evaluate how well your model performs:\n\n1. **Load your fine-tuned model** and run inference on the validation set. To do this, you will need to [fork my implementation of MedGemma 1.5](https://github.com/harpreetsahota204/medgemma_1_5) as a remote zoo model and update the [model maifest](https://github.com/harpreetsahota204/medgemma_1_5/blob/main/manifest.json) to download your weights. You may also need to make changes to the [zoo.py](https://github.com/harpreetsahota204/medgemma_1_5/blob/main/zoo.py) to merge your LORA with the original model. This is an exercise left to you.\n2. **Store predictions** in FiftyOne alongside the ground truth\n3. **Use the evaluation techniques** from Sections 5 and 6:\n   - Compute accuracy by modality, body part, and content type\n   - Analyze errors using the App and similarity search\n   - Tag patterns in failures\n\nYou can use FiftyOne's [`evaluate_detections()` method](https://docs.voxel51.com/user_guide/evaluation.html#detections) \nto evaluate object detection predictions, computing metrics like mAP and per-class performance.\n\nThis iterative workflow—explore, model, evaluate, fine-tune—is how you systematically\nimprove your model's performance on specific failure modes.","metadata":{}},{"id":"a3c6b0ad","cell_type":"markdown","source":"---\n## Bringing It All Together\n\nHere's what you've learned to do:\n\n| Step | What You Did | Why It Matters |\n|------|-------------|----------------|\n| Load & Explore | Understood data distribution before modeling | Caught potential issues early |\n| Embeddings | Visualized MedSigLIP clusters | Diagnosed whether classes are separable |\n| Inference | Ran MedGemma, stored predictions with data | Everything in one place for analysis |\n| Evaluation | Sliced accuracy by modality, location, etc. | Found *where* the model fails |\n| Error Analysis | Visualized failures, tagged patterns | Understood *why* it fails |\n| Fine-Tuning | Used GetItem + SFTTrainer for localization | Improved model on specific failure modes |\n\n**The workflow you built here works for any dataset, any model, any challenge.**\n\nWhether you're doing:\n- Chest X-ray report generation\n- Dermatology classification  \n- CT severity assessment\n- Histopathology analysis\n\nThe pattern is the same:\n1. Organize your data in FiftyOne\n2. Understand it before modeling\n3. Run inference, store predictions\n4. Slice, visualize, debug\n5. Fine-tune and iterate\n\n**Now go win that challenge.** 🏆","metadata":{}},{"id":"5460be53","cell_type":"markdown","source":"---\n## Resources\n\n### FiftyOne Documentation\n- [FiftyOne Documentation](https://docs.voxel51.com/)\n- [FiftyOne Datasets](https://docs.voxel51.com/user_guide/using_datasets.html)\n- [FiftyOne Views Cheat Sheet](https://docs.voxel51.com/cheat_sheets/views_cheat_sheet.html)\n- [FiftyOne Filtering Cheat Sheet](https://docs.voxel51.com/cheat_sheets/filtering_cheat_sheet.html)\n- [FiftyOne PyTorch Integration](https://docs.voxel51.com/integrations/pytorch.html)\n- [FiftyOne Brain](https://docs.voxel51.com/brain.html) (embeddings, similarity, visualization)\n- [FiftyOne Evaluation](https://docs.voxel51.com/user_guide/evaluation.html)\n- [FiftyOne Model Zoo](https://docs.voxel51.com/model_zoo/overview.html)\n- [FiftyOne Plugins](https://docs.voxel51.com/plugins/)\n\n### Dataset & Models\n- [SLAKE Dataset on HuggingFace](https://huggingface.co/datasets/Voxel51/SLAKE)\n- [MedGemma Model Card](https://huggingface.co/google/medgemma-1.5-4b-it)\n- [MedSigLIP Model Card](https://huggingface.co/google/medsiglip-448)\n\n### Fine-Tuning\n- [TRL SFTTrainer Documentation](https://huggingface.co/docs/trl/sft_trainer)\n- [PEFT LoRA Documentation](https://huggingface.co/docs/peft/main/en/conceptual_guides/lora)\n\n### Competition\n- [MedGemma Impact Challenge](https://www.kaggle.com/competitions/med-gemma-impact-challenge)","metadata":{}}]}