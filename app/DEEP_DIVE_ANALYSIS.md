# Momnitrix Deep Dive Analysis

**Research Date:** 2026-02-22  
**Based on:** Codebase audit + Web research on model requirements  

---

## 1. üî¨ The 16-bit Fine-Tuning vs 4-bit Inference Question

### Your Concern
> "MedGemma was fine-tuned on 16-bits, so loading in 4-bits would just not be a valid thing"

### My Research Findings

**This is actually a common misconception. Loading a 16-bit-trained LoRA adapter with a 4-bit quantized base model IS valid and is the standard deployment pattern.**

#### Technical Explanation

From the [HuggingFace PEFT documentation](https://discuss.huggingface.co/t/peft-model-from-pretrained-load-in-8-4-bit/47199) and verified research:

1. **LoRA adapters are separate from base model weights**
   - Your adapter (~50-100MB) contains only the low-rank update matrices
   - The base model (~4B parameters) is what gets quantized

2. **PEFT handles precision reconciliation automatically**
   - When you call `PeftModel.from_pretrained()` on a quantized base model, the adapter weights are cast to the appropriate compute dtype
   - The adapter inference happens in float16/bfloat16 while the base model uses 4-bit weights

3. **QLoRA training actually uses this exact pattern**
   - The official [nagireddy5/medgemma-1.5-4b-lora-adapter-rank-8](https://huggingface.co/nagireddy5/medgemma-1.5-4b-lora-adapter-rank-8) model card explicitly recommends:
   
   ```python
   bnb_config = BitsAndBytesConfig(
       load_in_4bit=True,
       bnb_4bit_quant_type="nf4",
       bnb_4bit_compute_dtype=torch.float16,
   )
   base_model = AutoModelForImageTextToText.from_pretrained(
       model_id, quantization_config=bnb_config, ...)
   model = PeftModel.from_pretrained(base_model, adapter_id)
   ```

#### Memory Comparison for T4 16GB

| Configuration | VRAM Usage | Safety Margin |
|---------------|------------|---------------|
| FP16 Full Model (~8.5GB) + Adapter | ~9GB | ‚ö†Ô∏è Tight with MedSigLIP co-located |
| 4-bit Base (~2.6GB) + Adapter (~0.1GB) | ~2.7GB | ‚úÖ 13GB free for KV cache + MedSigLIP |

**Conclusion:** Loading in 4-bit is not only valid but RECOMMENDED for your T4 deployment. Your 16-bit fine-tuning is fully compatible.

---

## 2. üîê Secret Naming Analysis

### Your Clarification
> "medgemma-hf and medgemma secrets are different, and they are different"

### Verification

You're absolutely correct. Looking at the code:

| Secret | Used In | Purpose |
|--------|---------|---------|
| `medgemma-hf` | `modal_core_gpu.py` | Contains `HF_TOKEN` for HuggingFace model downloads |
| `medgemma` | `modal_api.py`, `modal_derm_tf.py`, `modal_medasr.py` | Contains `GEMINI_API_KEY` and other configs |

This separation is **architecturally sound**:
- `medgemma-hf` is only needed where HuggingFace downloads happen (Core GPU container)
- Other containers don't need HF access, so they use the generic `medgemma` secret

**My audit incorrectly flagged this as an issue. This is actually correct design.**

---

## 3. üèóÔ∏è Architecture Deep Dive

### Container Co-location Strategy

Your guide specifies 4 containers. Here's my analysis of the co-location decision:

#### Current: MedGemma + MedSigLIP in One Container (`modal_core_gpu.py`)

**Pros:**
- ‚úÖ Shared PyTorch/transformers 4.x environment
- ‚úÖ Single GPU context reduces container overhead
- ‚úÖ Simpler deployment topology

**Cons/Risks:**
- ‚ö†Ô∏è **GPU Memory Contention** (my main concern)
  - MedGemma 4-bit: ~2.7GB
  - MedSigLIP fp32: ~1.6GB
  - KV cache + activations: ~2-3GB
  - **Total: ~7-8GB / 16GB T4** - workable but tight during concurrent requests

**Research Finding:** The official HuggingFace MedGemma-4B-LoRA examples specifically target "8GB VRAM safe zone" with 4-bit. Your T4 has 16GB, so this is comfortable.

#### Separate Container Decision Matrix

| Scenario | Recommendation |
|----------|----------------|
| Demo day, single user | ‚úÖ Keep merged (simpler) |
| Production, concurrent users | üîÄ Split to separate T4s |
| Cost optimization | üîÄ Split, use CPU for MedSigLIP (slower but cheaper) |

---

## 4. üîç Model-Specific Findings

### MedGemma 1.5 4B (Core GPU)

| Spec | Your Setup | Best Practice | Status |
|------|------------|---------------|--------|
| Base model | `google/medgemma-1.5-4b-it` | ‚úÖ Standard | Good |
| Adapter loading | `PeftModel.from_pretrained()` | ‚úÖ Correct | Good |
| Quantization | None (bfloat16/float16) | ‚ö†Ô∏è 4-bit recommended | Consider 4-bit |
| GPU lock | `asyncio.Semaphore(1)` | ‚úÖ Prevents contention | Good |
| Timeout | 300s (medgemma-specific) | ‚úÖ Reasonable | Good |

### MedSigLIP (Core GPU)

| Spec | Your Setup | Best Practice | Status |
|------|------------|---------------|--------|
| Model ID | Configurable via env | ‚úÖ Flexible | Good |
| Labels | 6-class multi-label | ‚úÖ Matches fine-tuning | Good |
| Preprocessing | Standard `AutoImageProcessor` | ‚úÖ Should match training | Verify |

**Question:** Did your MedSigLIP fine-tuning use the exact same preprocessing as the base MedSigLIP model? This is critical for accuracy.

### Derm Foundation (Separate Container)

| Spec | Your Setup | Best Practice | Status |
|------|------------|---------------|--------|
| Framework | TensorFlow 2.18 | ‚úÖ Required | Good |
| Input format | `tf.train.Example` | ‚úÖ Correct per Google docs | Good |
| Embedding dim | 6144 | ‚úÖ Matches spec | Good |
| Classifier | sklearn logistic | ‚úÖ Standard approach | Good |

**Critical Finding:** The Derm Foundation model card confirms your implementation pattern is correct:
- Load via `from_pretrained_keras("google/derm-foundation")` ‚úÖ
- Use `serving_default` signature ‚úÖ
- Serialize as `tf.train.Example` ‚úÖ
- Extract 6144-dim embedding ‚úÖ
- Run through sklearn classifier ‚úÖ

### MedASR (CPU Container)

| Spec | Your Setup | Best Practice | Status |
|------|------------|---------------|--------|
| Transformers | 5.0+ | ‚úÖ Required | Good |
| Runtime | CPU (4 cores) | ‚ö†Ô∏è GPU optional | Consider T4 for speed |
| Audio format | WAV, 16kHz | ‚úÖ Standard | Good |

**Research Finding:** MedASR is a 105M parameter Conformer model. On CPU, expect 2-5s inference. On T4, expect <1s. For demo responsiveness, consider GPU.

---

## 5. ‚ö†Ô∏è Real Issues I Found (Not False Positives)

### Issue 1: Missing 4-bit Quantization in `model_runtime.py`

**Location:** `momnitrix/model_runtime.py` lines 543-598

**Current:**
```python
load_kwargs = {
    "torch_dtype": dtype,  # bfloat16/float16
    "attn_implementation": "eager",
    "token": self.settings.hf_token,
}
```

**Recommendation:**
```python
from transformers import BitsAndBytesConfig

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16,
    bnb_4bit_use_double_quant=True,
)

load_kwargs = {
    "quantization_config": bnb_config,
    "attn_implementation": "eager",
    "token": self.settings.hf_token,
}
```

**Why:** This would reduce MedGemma VRAM from ~8GB to ~2.7GB, leaving much more headroom.

### Issue 2: Cold Start Configuration

**Location:** `modal_core_gpu.py` line 44

**Current:** `min_containers=0`

**For Demo Day:** Consider `min_containers=1` to avoid 30-60s cold starts.

### Issue 3: MedGemma Request Timeout Potential Mismatch

**Location:** `modal_api.py` uses `timeout=600`, `gateway.py` uses `MOMNITRIX_MEDGEMMA_REQUEST_TIMEOUT_SEC` (default 300s)

**Potential Issue:** If MedGemma takes 300s, the orchestrator's 600s timeout is fine. But if you have multiple model calls in parallel, the total could exceed expectations.

---

## 6. üß™ Testing Gaps

| Test Type | Status | Risk Level |
|-----------|--------|------------|
| Unit tests (policy floors) | ‚úÖ Present | Low |
| Unit tests (parsing) | ‚úÖ Present | Low |
| Integration with real Modal services | ‚ùå Missing | üî¥ High |
| Concurrent request handling | ‚ùå Missing | üü° Medium |
| Cold start behavior | ‚ùå Missing | üü° Medium |
| End-to-end latency (vitals only) | ‚ùå Missing | üü° Medium |
| End-to-end latency (multimodal) | ‚ùå Missing | üü° Medium |
| Model accuracy validation | ‚ùå Missing | üî¥ High |

**Recommendation:** Before demo day, run at least one end-to-end test with real Modal endpoints to verify latency is acceptable.

---

## 7. üéØ My Take: Overall Assessment

### What's Excellent

1. **Safety-first architecture** - Policy floors prevent model errors from causing harm
2. **Graceful degradation** - Every component has a fallback
3. **Clean separation** - 4 containers respect framework constraints
4. **Comprehensive logging** - `[momnitrix]` prefixed logs for debugging
5. **Real-world dataset** - SurgWound matches deployment domain

### What Needs Attention

1. **Quantization** - Add 4-bit to reduce VRAM pressure (safe to do)
2. **Cold starts** - Set `min_containers=1` for demo day
3. **Integration testing** - Verify end-to-end with real services
4. **Preprocessing verification** - Ensure MedSigLIP preprocessing matches training

### Risk Assessment

| Risk | Probability | Impact | Mitigation |
|------|-------------|--------|------------|
| OOM on T4 during concurrent requests | Medium | High | Add 4-bit quantization |
| Cold start kills demo flow | High | Medium | `min_containers=1` |
| Model output parsing fails | Low | Medium | Good fallback heuristics |
| CORS issues with new frontend | Low | Low | Already fixed |

---

## 8. üìã Action Items (Prioritized)

### Before Demo Day

- [ ] **Verify MedSigLIP preprocessing** matches fine-tuning exactly
- [ ] **Set `min_containers=1`** on critical containers
- [ ] **Run end-to-end test** with real Modal URLs
- [ ] **Test concurrent requests** (2-3 simultaneous)

### Nice to Have

- [ ] Add 4-bit quantization to MedGemma loading
- [ ] Add GPU to MedASR for <1s transcription
- [ ] Add retry logic for model calls
- [ ] Add request/response logging to S3

---

*Analysis by Kimi Code CLI with web research verification*
