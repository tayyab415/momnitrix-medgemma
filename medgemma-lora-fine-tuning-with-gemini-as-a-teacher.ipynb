{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f463cb7",
   "metadata": {
    "papermill": {
     "duration": 0.005266,
     "end_time": "2026-02-11T10:24:33.815895",
     "exception": false,
     "start_time": "2026-02-11T10:24:33.810629",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# MedGemma LoRA Fine-Tuning — Patient Visit Summaries\n",
    "\n",
    "Fine-tune MedGemma 4B on patient-friendly visit summaries using Gemini Flash as teacher.\n",
    "\n",
    "**Runtime:** T4 GPU on Google Colab (~2-3 hours for full training)\n",
    "\n",
    "**What this does:**\n",
    "1. Generate synthetic training data from MTSamples using Gemini Flash\n",
    "2. QLoRA fine-tune MedGemma 4B to produce patient summaries with a short prompt\n",
    "3. Export adapter weights (~28MB) to use locally with MLX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58a3bfd0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-11T10:24:33.825257Z",
     "iopub.status.busy": "2026-02-11T10:24:33.824870Z",
     "iopub.status.idle": "2026-02-11T10:25:08.780984Z",
     "shell.execute_reply": "2026-02-11T10:25:08.779611Z"
    },
    "papermill": {
     "duration": 34.965057,
     "end_time": "2026-02-11T10:25:08.784832",
     "exception": false,
     "start_time": "2026-02-11T10:24:33.819775",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q \\\n",
    "    transformers>=4.46.0 \\\n",
    "    peft>=0.13.0 \\\n",
    "    trl>=0.12.0 \\\n",
    "    bitsandbytes>=0.44.0 \\\n",
    "    datasets \\\n",
    "    accelerate \\\n",
    "    google-genai \\\n",
    "    huggingface_hub \\\n",
    "    nest_asyncio \\\n",
    "    flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "555a65e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-11T10:25:08.795390Z",
     "iopub.status.busy": "2026-02-11T10:25:08.795006Z",
     "iopub.status.idle": "2026-02-11T10:25:11.034106Z",
     "shell.execute_reply": "2026-02-11T10:25:11.032742Z"
    },
    "papermill": {
     "duration": 2.246027,
     "end_time": "2026-02-11T10:25:11.035950",
     "exception": true,
     "start_time": "2026-02-11T10:25:08.789923",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "BackendError",
     "evalue": "Unexpected response from the service. Response: {'errors': ['No user secrets exist for kernel id 109225408 and label HF_TOKEN.'], 'error': {'code': 5}, 'wasSuccessful': False}.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBackendError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_17/3334439926.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mkaggle_secrets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mUserSecretsClient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0msecrets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUserSecretsClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mHF_TOKEN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msecrets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_secret\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'HF_TOKEN'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mGEMINI_API_KEY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msecrets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_secret\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GEMINI_API_KEY'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mWORK_DIR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/kaggle/working'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/kaggle_secrets.py\u001b[0m in \u001b[0;36mget_secret\u001b[0;34m(self, label)\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;34m'Label'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         }\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mresponse_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweb_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_post_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_body\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGET_USER_SECRET_BY_LABEL_ENDPOINT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'secret'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresponse_json\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             raise BackendError(\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/kaggle_web_client.py\u001b[0m in \u001b[0;36mmake_post_request\u001b[0;34m(self, data, endpoint, timeout)\u001b[0m\n\u001b[1;32m     47\u001b[0m                 \u001b[0mresponse_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresponse_json\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'wasSuccessful'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'result'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresponse_json\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m                     raise BackendError(\n\u001b[0m\u001b[1;32m     50\u001b[0m                         f'Unexpected response from the service. Response: {response_json}.')\n\u001b[1;32m     51\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse_json\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'result'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBackendError\u001b[0m: Unexpected response from the service. Response: {'errors': ['No user secrets exist for kernel id 109225408 and label HF_TOKEN.'], 'error': {'code': 5}, 'wasSuccessful': False}."
     ]
    }
   ],
   "source": [
    "# Configuration — auto-detects Kaggle vs Colab\n",
    "import os\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "ON_KAGGLE = os.path.exists('/kaggle')\n",
    "\n",
    "# --- Load secrets ---\n",
    "if ON_KAGGLE:\n",
    "    from kaggle_secrets import UserSecretsClient\n",
    "    secrets = UserSecretsClient()\n",
    "    HF_TOKEN = secrets.get_secret('HF_TOKEN')\n",
    "    GEMINI_API_KEY = secrets.get_secret('GEMINI_API_KEY')\n",
    "    WORK_DIR = '/kaggle/working'\n",
    "    print('Running on Kaggle')\n",
    "else:\n",
    "    try:\n",
    "        from google.colab import userdata\n",
    "        HF_TOKEN = userdata.get('HF_TOKEN')\n",
    "        GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')\n",
    "    except Exception:\n",
    "        HF_TOKEN = ''\n",
    "        GEMINI_API_KEY = ''\n",
    "    WORK_DIR = '/content'\n",
    "    print('Running on Colab')\n",
    "\n",
    "os.environ['HF_TOKEN'] = HF_TOKEN\n",
    "os.environ['GEMINI_API_KEY'] = GEMINI_API_KEY\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "# --- Auto-detect GPU and set optimal config ---\n",
    "gpu_name = torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'cpu'\n",
    "vram_gb = torch.cuda.get_device_properties(0).total_mem / 1e9 if torch.cuda.is_available() else 0\n",
    "\n",
    "if 'A100' in gpu_name:\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    N_TRAIN_SAMPLES = 1500\n",
    "    BATCH_SIZE = 4\n",
    "    GRAD_ACCUM_STEPS = 2\n",
    "    MAX_SEQ_LENGTH = 2048\n",
    "    USE_FLASH_ATTN = True\n",
    "    print(f'A100 detected ({vram_gb:.0f}GB) — full speed config')\n",
    "elif 'L4' in gpu_name or 'A10' in gpu_name or 'P100' in gpu_name:\n",
    "    N_TRAIN_SAMPLES = 1500\n",
    "    BATCH_SIZE = 2\n",
    "    GRAD_ACCUM_STEPS = 4\n",
    "    MAX_SEQ_LENGTH = 2048\n",
    "    USE_FLASH_ATTN = 'P100' not in gpu_name  # P100 doesn't support flash attn\n",
    "    print(f'{gpu_name} detected ({vram_gb:.0f}GB) — mid-tier config')\n",
    "else:\n",
    "    N_TRAIN_SAMPLES = 200\n",
    "    BATCH_SIZE = 1\n",
    "    GRAD_ACCUM_STEPS = 8\n",
    "    MAX_SEQ_LENGTH = 1536\n",
    "    USE_FLASH_ATTN = False\n",
    "    print(f'{gpu_name} detected ({vram_gb:.0f}GB) — conservative config')\n",
    "\n",
    "# Shared config\n",
    "LORA_RANK = 8\n",
    "LORA_ALPHA = 16\n",
    "NUM_TRAIN_EPOCHS = 3\n",
    "LEARNING_RATE = 5e-5\n",
    "\n",
    "MODEL_ID = 'google/medgemma-4b-it'\n",
    "OUTPUT_DIR = f'{WORK_DIR}/medgemma-lora-patient-summary'\n",
    "DATA_DIR = f'{WORK_DIR}/training_data'\n",
    "\n",
    "print(f'batch={BATCH_SIZE} x grad_accum={GRAD_ACCUM_STEPS} = effective {BATCH_SIZE * GRAD_ACCUM_STEPS}, '\n",
    "      f'max_len={MAX_SEQ_LENGTH}, samples={N_TRAIN_SAMPLES}, flash_attn={USE_FLASH_ATTN}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c234c87a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Step 1: Generate Training Data with Gemini Flash\n",
    "\n",
    "**Skip this section** if you already generated data in a previous session — run the cell below to load from Google Drive instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6aba1f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load existing training data (Google Drive on Colab, skip on Kaggle)\n",
    "import shutil\n",
    "\n",
    "if ON_KAGGLE:\n",
    "    print('On Kaggle — skipping Drive load, will generate fresh data below')\n",
    "else:\n",
    "    from google.colab import drive\n",
    "    DRIVE_DATA_DIR = '/content/drive/MyDrive/medasr-mlx/training_data'\n",
    "    drive.mount('/content/drive', force_remount=False)\n",
    "\n",
    "    if Path(f'{DRIVE_DATA_DIR}/train.jsonl').exists():\n",
    "        Path(DATA_DIR).mkdir(parents=True, exist_ok=True)\n",
    "        for split in ['train', 'valid', 'test']:\n",
    "            src = f'{DRIVE_DATA_DIR}/{split}.jsonl'\n",
    "            dst = f'{DATA_DIR}/{split}.jsonl'\n",
    "            if Path(src).exists():\n",
    "                shutil.copy2(src, dst)\n",
    "        n_train = sum(1 for _ in open(f'{DATA_DIR}/train.jsonl'))\n",
    "        n_valid = sum(1 for _ in open(f'{DATA_DIR}/valid.jsonl'))\n",
    "        print(f'Loaded from Drive: {n_train} train, {n_valid} valid')\n",
    "        print('>>> Skip the generation cells below and go straight to Step 2')\n",
    "    else:\n",
    "        print(f'No data found at {DRIVE_DATA_DIR} — run generation cells below')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc0ccd4",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset\n",
    "from google import genai\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "# Teacher prompt for Gemini Flash\n",
    "TEACHER_PROMPT = \"\"\"You are a helpful medical communication assistant. A patient has just \\\n",
    "recorded their doctor visit. You will receive the transcript and produce \\\n",
    "a clear, friendly summary that helps the patient understand what happened.\n",
    "\n",
    "Rules:\n",
    "- Write at a 6th-grade reading level\n",
    "- Explain every medical term in parentheses the first time it appears\n",
    "- Use \"your doctor\" instead of doctor names\n",
    "- Organize into these exact sections:\n",
    "  1. VISIT SUMMARY — What happened in 2-3 sentences\n",
    "  2. KEY FINDINGS — What the doctor found, explained simply\n",
    "  3. DIAGNOSIS — What the doctor thinks is going on\n",
    "  4. ACTION ITEMS — What I need to do next (medications, follow-ups, tests, etc.)\n",
    "  5. QUESTIONS TO ASK — Things I might want to clarify at my next visit\n",
    "- Include ALL action items (medications, follow-ups, tests, lifestyle changes)\n",
    "- If something is unclear from the transcript, say so honestly\n",
    "- Do NOT invent information not in the transcript\n",
    "- Be warm and reassuring but accurate\n",
    "\n",
    "Here is the transcript:\n",
    "\n",
    "---\n",
    "{transcript}\n",
    "---\n",
    "\n",
    "Please write the patient-friendly summary now.\"\"\"\n",
    "\n",
    "SHORT_USER_PROMPT = \"Summarize this doctor visit for me in plain language:\\n\\n{transcript}\"\n",
    "\n",
    "TARGET_SPECIALTIES = [\n",
    "    ' Cardiovascular / Pulmonary', ' Orthopedic', ' Neurology',\n",
    "    ' Gastroenterology', ' General Medicine', ' Radiology',\n",
    "    ' Emergency Room Reports', ' Consult - History and Phy.',\n",
    "]\n",
    "\n",
    "# Load and filter MTSamples\n",
    "print('Loading MTSamples...')\n",
    "ds = load_dataset('harishnair04/mtsamples', split='train')\n",
    "filtered = [\n",
    "    row for row in ds\n",
    "    if row['medical_specialty'] in TARGET_SPECIALTIES\n",
    "    and row.get('transcription')\n",
    "    and len(row['transcription'].strip()) > 200\n",
    "]\n",
    "samples = random.sample(filtered, min(N_TRAIN_SAMPLES, len(filtered)))\n",
    "print(f'{len(samples)} samples selected from {len(filtered)} available')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f26d14",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate summaries with Gemini Flash — 10 concurrent requests\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()  # needed for Kaggle (Colab has native await support)\n",
    "\n",
    "client = genai.Client(api_key=GEMINI_API_KEY)\n",
    "CONCURRENCY = 10\n",
    "\n",
    "async def generate_one(idx, row):\n",
    "    \"\"\"Generate a single summary. Returns (idx, pair_dict) or (idx, None) on failure.\"\"\"\n",
    "    transcript = row['transcription'].strip()\n",
    "    if len(transcript) > 4000:\n",
    "        transcript = transcript[:4000] + '...'\n",
    "    specialty = row['medical_specialty'].strip()\n",
    "\n",
    "    try:\n",
    "        t0 = time.time()\n",
    "        response = await client.aio.models.generate_content(\n",
    "            model='gemini-2.0-flash',\n",
    "            contents=TEACHER_PROMPT.format(transcript=transcript),\n",
    "            config={'temperature': 0.2, 'max_output_tokens': 4096},\n",
    "        )\n",
    "        summary = response.text\n",
    "        elapsed = time.time() - t0\n",
    "    except Exception as e:\n",
    "        print(f'  [{idx+1}] {specialty} ERROR: {e}')\n",
    "        return idx, None\n",
    "\n",
    "    if len(summary) < 100 or len(summary) > 5000:\n",
    "        print(f'  [{idx+1}] {specialty} SKIP ({len(summary)} chars)')\n",
    "        return idx, None\n",
    "\n",
    "    print(f'  [{idx+1}] {specialty} {elapsed:.1f}s, {len(summary)} chars')\n",
    "    return idx, {\n",
    "        'messages': [\n",
    "            {'role': 'user', 'content': SHORT_USER_PROMPT.format(transcript=transcript)},\n",
    "            {'role': 'assistant', 'content': summary},\n",
    "        ]\n",
    "    }\n",
    "\n",
    "async def generate_all(samples):\n",
    "    sem = asyncio.Semaphore(CONCURRENCY)\n",
    "\n",
    "    async def bounded(idx, row):\n",
    "        async with sem:\n",
    "            return await generate_one(idx, row)\n",
    "\n",
    "    tasks = [bounded(i, row) for i, row in enumerate(samples)]\n",
    "    return await asyncio.gather(*tasks)\n",
    "\n",
    "t_start = time.time()\n",
    "print(f'Generating {len(samples)} summaries ({CONCURRENCY} concurrent)...')\n",
    "results = asyncio.get_event_loop().run_until_complete(generate_all(samples))\n",
    "\n",
    "train_pairs = [pair for _, pair in results if pair is not None]\n",
    "errors = sum(1 for _, pair in results if pair is None)\n",
    "total_elapsed = time.time() - t_start\n",
    "\n",
    "print(f'\\nDone: {len(train_pairs)} pairs from {len(samples)} samples '\n",
    "      f'({errors} errors) in {total_elapsed:.0f}s '\n",
    "      f'({total_elapsed/len(samples):.1f}s/sample avg)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebf9be5",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save as train/valid/test JSONL splits (local + Google Drive backup on Colab)\n",
    "Path(DATA_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "random.shuffle(train_pairs)\n",
    "n = len(train_pairs)\n",
    "\n",
    "# Guarantee at least 1 sample per split\n",
    "n_test = max(1, int(n * 0.1))\n",
    "n_valid = max(1, int(n * 0.1))\n",
    "n_train = n - n_valid - n_test\n",
    "\n",
    "if n_train < 1:\n",
    "    raise ValueError(f'Only {n} training pairs generated — need at least 3. Check Gemini errors above.')\n",
    "\n",
    "splits = {\n",
    "    'train': train_pairs[:n_train],\n",
    "    'valid': train_pairs[n_train:n_train + n_valid],\n",
    "    'test': train_pairs[n_train + n_valid:],\n",
    "}\n",
    "\n",
    "for name, data in splits.items():\n",
    "    path = f'{DATA_DIR}/{name}.jsonl'\n",
    "    with open(path, 'w') as f:\n",
    "        for item in data:\n",
    "            f.write(json.dumps(item) + '\\n')\n",
    "    print(f'{name}: {len(data)} samples -> {path}')\n",
    "\n",
    "# Backup to Google Drive (Colab only)\n",
    "if not ON_KAGGLE:\n",
    "    DRIVE_DATA_DIR = '/content/drive/MyDrive/medasr-mlx/training_data'\n",
    "    try:\n",
    "        from google.colab import drive\n",
    "        drive.mount('/content/drive', force_remount=False)\n",
    "        Path(DRIVE_DATA_DIR).mkdir(parents=True, exist_ok=True)\n",
    "        for name in splits:\n",
    "            shutil.copy2(f'{DATA_DIR}/{name}.jsonl', f'{DRIVE_DATA_DIR}/{name}.jsonl')\n",
    "        print(f'\\nBacked up to Google Drive: {DRIVE_DATA_DIR}')\n",
    "    except Exception as e:\n",
    "        print(f'\\nDrive backup skipped: {e}')\n",
    "else:\n",
    "    print(f'\\nData saved to {DATA_DIR} (Kaggle working dir — persists in output)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813fede7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Step 2: Load MedGemma with 4-bit Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8f95c7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "model_kwargs = dict(\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto',\n",
    "    token=HF_TOKEN,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "if USE_FLASH_ATTN:\n",
    "    model_kwargs['attn_implementation'] = 'flash_attention_2'\n",
    "\n",
    "print(f'Loading {MODEL_ID}' + (' with Flash Attention 2' if USE_FLASH_ATTN else '') + '...')\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, token=HF_TOKEN)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_ID, **model_kwargs)\n",
    "model.config.use_cache = False\n",
    "\n",
    "vram_before = torch.cuda.memory_allocated() / 1e9\n",
    "print(f'Full VLM loaded. GPU memory: {vram_before:.1f} GB')\n",
    "\n",
    "# --- Strip vision encoder (SigLIP) and multi-modal projector ---\n",
    "vision_attrs = ['vision_tower', 'multi_modal_projector']\n",
    "stripped = []\n",
    "for attr in vision_attrs:\n",
    "    if hasattr(model, attr):\n",
    "        delattr(model, attr)\n",
    "        stripped.append(attr)\n",
    "    if hasattr(model, 'model') and hasattr(model.model, attr):\n",
    "        delattr(model.model, attr)\n",
    "        stripped.append(f'model.{attr}')\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "vram_after = torch.cuda.memory_allocated() / 1e9\n",
    "print(f'Stripped {stripped} — freed {vram_before - vram_after:.1f} GB')\n",
    "print(f'Text-only model. GPU memory: {vram_after:.1f} GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606a9575",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Quick sanity check — generate with full prompt before training\n",
    "test_input = tokenizer.apply_chat_template(\n",
    "    [{'role': 'user', 'content': 'What is costochondritis? Explain in simple terms.'}],\n",
    "    tokenize=False, add_generation_prompt=True,\n",
    ")\n",
    "inputs = tokenizer(test_input, return_tensors='pt').to(model.device)\n",
    "with torch.no_grad():\n",
    "    out = model.generate(**inputs, max_new_tokens=100, temperature=0.2, do_sample=True)\n",
    "print(tokenizer.decode(out[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bc22ff",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Step 3: Configure LoRA and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac86c1c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=LORA_RANK,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=['q_proj', 'v_proj', 'k_proj', 'o_proj',\n",
    "                    'gate_proj', 'up_proj', 'down_proj'],\n",
    "    bias='none',\n",
    "    task_type='CAUSAL_LM',\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78936a10",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datasets\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "import torch\n",
    "import functools\n",
    "import json\n",
    "\n",
    "# --- Diagnostics: check what's actually in the files ---\n",
    "for split_name in ['train', 'valid', 'test']:\n",
    "    fpath = f'{DATA_DIR}/{split_name}.jsonl'\n",
    "    try:\n",
    "        with open(fpath) as f:\n",
    "            lines = [l for l in f if l.strip()]\n",
    "        print(f'{split_name}.jsonl: {len(lines)} lines')\n",
    "    except FileNotFoundError:\n",
    "        print(f'{split_name}.jsonl: FILE NOT FOUND')\n",
    "\n",
    "# --- Clear datasets cache to avoid stale results ---\n",
    "import shutil\n",
    "cache_dir = os.path.expanduser('~/.cache/huggingface/datasets')\n",
    "json_caches = [d for d in os.listdir(cache_dir) if d.startswith('json')] if os.path.exists(cache_dir) else []\n",
    "for d in json_caches:\n",
    "    shutil.rmtree(os.path.join(cache_dir, d), ignore_errors=True)\n",
    "if json_caches:\n",
    "    print(f'Cleared {len(json_caches)} cached json datasets')\n",
    "\n",
    "# --- Load train data ---\n",
    "train_path = f'{DATA_DIR}/train.jsonl'\n",
    "with open(train_path) as f:\n",
    "    train_records = [json.loads(l) for l in f if l.strip()]\n",
    "\n",
    "if len(train_records) == 0:\n",
    "    raise ValueError(f'{train_path} is empty! Re-run data generation (cells 5-7) or check your Drive backup.')\n",
    "\n",
    "train_ds = datasets.Dataset.from_list(train_records)\n",
    "\n",
    "# --- Load or create valid data ---\n",
    "valid_path = f'{DATA_DIR}/valid.jsonl'\n",
    "try:\n",
    "    with open(valid_path) as f:\n",
    "        valid_records = [json.loads(l) for l in f if l.strip()]\n",
    "except FileNotFoundError:\n",
    "    valid_records = []\n",
    "\n",
    "if len(valid_records) == 0:\n",
    "    print('WARNING: valid set is empty — splitting 10% off train')\n",
    "    split = train_ds.train_test_split(test_size=0.1, seed=42)\n",
    "    train_ds = split['train']\n",
    "    valid_ds = split['test']\n",
    "else:\n",
    "    valid_ds = datasets.Dataset.from_list(valid_records)\n",
    "\n",
    "print(f'Train: {len(train_ds)}, Valid: {len(valid_ds)}')\n",
    "\n",
    "# Patch model forward to inject token_type_ids (all 0 = text tokens)\n",
    "# Gemma 3 VLM requires this during training, even with vision stripped.\n",
    "_original_forward = model.forward.__wrapped__ if hasattr(model.forward, '__wrapped__') else model.forward\n",
    "\n",
    "@functools.wraps(_original_forward)\n",
    "def _forward_with_token_type_ids(*args, **kwargs):\n",
    "    if 'token_type_ids' not in kwargs or kwargs['token_type_ids'] is None:\n",
    "        input_ids = kwargs.get('input_ids')\n",
    "        if input_ids is None and len(args) > 0:\n",
    "            input_ids = args[0]\n",
    "        if input_ids is not None:\n",
    "            kwargs['token_type_ids'] = torch.zeros_like(input_ids)\n",
    "    return _original_forward(*args, **kwargs)\n",
    "\n",
    "model.forward = _forward_with_token_type_ids\n",
    "print('Patched model.forward to inject token_type_ids')\n",
    "\n",
    "# Training config\n",
    "training_args = SFTConfig(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=NUM_TRAIN_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRAD_ACCUM_STEPS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    lr_scheduler_type='cosine',\n",
    "    warmup_ratio=0.05,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=10,\n",
    "    eval_strategy='steps',\n",
    "    eval_steps=50,\n",
    "    save_strategy='steps',\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    bf16=True,\n",
    "    max_length=MAX_SEQ_LENGTH,\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={'use_reentrant': False},\n",
    "    optim='adamw_torch_fused',\n",
    "    report_to='none',\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=valid_ds,\n",
    "    processing_class=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3c735a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train!\n",
    "print('Starting training...')\n",
    "trainer.train()\n",
    "print(f'\\nTraining complete. GPU memory: {torch.cuda.memory_allocated() / 1e9:.1f} GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc81d698",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the LoRA adapter\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "# Check adapter size\n",
    "import os\n",
    "total = sum(os.path.getsize(os.path.join(OUTPUT_DIR, f)) for f in os.listdir(OUTPUT_DIR))\n",
    "print(f'Adapter saved to {OUTPUT_DIR} ({total / 1e6:.1f} MB)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08883db3",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Step 4: Test the Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a143811",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compare base vs fine-tuned on a held-out transcript\n",
    "test_transcript = \"\"\"SUBJECTIVE: The patient is a 45-year-old male who presents with chest pain \\\n",
    "that started two days ago. He describes it as a sharp, intermittent pain in \\\n",
    "the left side of his chest. The pain worsens with deep breathing and movement. \\\n",
    "He denies shortness of breath, palpitations, or radiation to the arm or jaw. \\\n",
    "No fever, cough, or recent illness. He has a history of hypertension controlled \\\n",
    "with lisinopril 10mg daily. No family history of heart disease.\n",
    "\n",
    "OBJECTIVE: Blood pressure 138/88, heart rate 76, respiratory rate 16, \\\n",
    "temperature 98.6. Chest wall is tender to palpation over the left costochondral \\\n",
    "junction. Heart sounds regular, no murmurs. Lungs clear bilaterally. \\\n",
    "ECG shows normal sinus rhythm, no ST changes.\n",
    "\n",
    "ASSESSMENT: Costochondritis, likely musculoskeletal etiology. \\\n",
    "Hypertension, stable on current medication.\n",
    "\n",
    "PLAN: Ibuprofen 400mg three times daily with food for 7 days. \\\n",
    "Apply ice to affected area. Avoid heavy lifting for one week. \\\n",
    "Follow up in 2 weeks if symptoms persist. Continue lisinopril.\"\"\"\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    [{'role': 'user', 'content': f'Summarize this doctor visit for me in plain language:\\n\\n{test_transcript}'}],\n",
    "    tokenize=False, add_generation_prompt=True,\n",
    ")\n",
    "inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
    "\n",
    "# --- Base model (LoRA disabled) ---\n",
    "with model.disable_adapter():\n",
    "    with torch.no_grad():\n",
    "        out_base = model.generate(**inputs, max_new_tokens=512, temperature=0.2, do_sample=True)\n",
    "base_response = tokenizer.decode(out_base[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "\n",
    "# --- Fine-tuned model (LoRA enabled) ---\n",
    "with torch.no_grad():\n",
    "    out_ft = model.generate(**inputs, max_new_tokens=512, temperature=0.2, do_sample=True)\n",
    "ft_response = tokenizer.decode(out_ft[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "\n",
    "# --- Print side by side ---\n",
    "print('=' * 80)\n",
    "print('BASE MODEL (no LoRA)')\n",
    "print('=' * 80)\n",
    "print(base_response)\n",
    "print()\n",
    "print('=' * 80)\n",
    "print('FINE-TUNED MODEL (LoRA)')\n",
    "print('=' * 80)\n",
    "print(ft_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf55b5e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Step 5: Export for Local MLX Use\n",
    "\n",
    "Two options to use the adapter locally:\n",
    "\n",
    "**Option A:** Convert PEFT adapter to MLX format (recommended)\n",
    "\n",
    "**Option B:** Download PEFT adapter and convert locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e943f675",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Option A: Convert to MLX-compatible safetensors right here\n",
    "# This creates adapter weights that mlx-lm can load\n",
    "\n",
    "from safetensors.torch import load_file, save_file\n",
    "import re\n",
    "\n",
    "adapter_weights = load_file(f'{OUTPUT_DIR}/adapter_model.safetensors')\n",
    "\n",
    "# Remap PEFT keys to mlx-lm format:\n",
    "# base_model.model.model.layers.X.self_attn.q_proj.lora_A.weight -> model.layers.X.self_attn.q_proj.lora_a\n",
    "mlx_weights = {}\n",
    "for key, tensor in adapter_weights.items():\n",
    "    # Strip PEFT prefix\n",
    "    new_key = key.replace('base_model.model.', '')\n",
    "    # Rename lora_A.weight -> lora_a, lora_B.weight -> lora_b\n",
    "    new_key = new_key.replace('.lora_A.weight', '.lora_a')\n",
    "    new_key = new_key.replace('.lora_B.weight', '.lora_b')\n",
    "    # Transpose: PEFT stores (out, rank) for A and (rank, in) for B\n",
    "    # MLX LoRA expects (in, rank) for A and (rank, out) for B\n",
    "    mlx_weights[new_key] = tensor.T.contiguous()\n",
    "\n",
    "mlx_adapter_dir = f'{OUTPUT_DIR}/mlx_adapter'\n",
    "os.makedirs(mlx_adapter_dir, exist_ok=True)\n",
    "save_file(mlx_weights, f'{mlx_adapter_dir}/adapters.safetensors')\n",
    "\n",
    "# Write mlx-lm compatible adapter_config.json\n",
    "import json\n",
    "mlx_config = {\n",
    "    'num_layers': 16,\n",
    "    'lora_parameters': {\n",
    "        'rank': LORA_RANK,\n",
    "        'alpha': float(LORA_ALPHA),\n",
    "        'dropout': 0.0,\n",
    "        'scale': float(LORA_ALPHA) / LORA_RANK,\n",
    "        'keys': ['self_attn.q_proj', 'self_attn.v_proj', 'self_attn.k_proj',\n",
    "                 'self_attn.o_proj', 'mlp.gate_proj', 'mlp.up_proj', 'mlp.down_proj'],\n",
    "    },\n",
    "}\n",
    "with open(f'{mlx_adapter_dir}/adapter_config.json', 'w') as f:\n",
    "    json.dump(mlx_config, f, indent=2)\n",
    "\n",
    "print(f'MLX adapter saved to {mlx_adapter_dir}')\n",
    "print('Keys:', list(mlx_weights.keys())[:5], '...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d734d9",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Download the MLX adapter\n",
    "mlx_zip = f'{WORK_DIR}/mlx_adapter.zip'\n",
    "!zip -r {mlx_zip} {mlx_adapter_dir}\n",
    "\n",
    "if ON_KAGGLE:\n",
    "    print(f'\\nMLX adapter zipped at {mlx_zip}')\n",
    "    print('Find it in the Output tab → /kaggle/working/mlx_adapter.zip')\n",
    "    print('Then on your Mac:')\n",
    "    print('  unzip mlx_adapter.zip -d artifacts/medgemma-lora-colab/')\n",
    "else:\n",
    "    from google.colab import files\n",
    "    files.download(mlx_zip)\n",
    "    print('\\nDownload the zip, then on your Mac:')\n",
    "    print('  unzip mlx_adapter.zip -d artifacts/medgemma-lora-colab/')\n",
    "    print('  python main.py  # will auto-detect adapter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148fb727",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Optional: push adapter to HuggingFace Hub\n",
    "# from huggingface_hub import HfApi\n",
    "# api = HfApi(token=HF_TOKEN)\n",
    "# api.upload_folder(\n",
    "#     folder_path=mlx_adapter_dir,\n",
    "#     repo_id='YOUR_USERNAME/medgemma-patient-summary-lora',\n",
    "#     repo_type='model',\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 14898831,
     "sourceId": 118534,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31259,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 41.272455,
   "end_time": "2026-02-11T10:25:11.861934",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-02-11T10:24:30.589479",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
