{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n# MedGemma 1.5 Fine-tuning for COVID-19 Cough Detection\nThis notebook demonstrates the complete workflow for fine-tuning the MedGemma 1.5 (4B) model to detect COVID-19 from cough audio recordings.\n\n## Workflow:\n1.  **Setup**: Install dependencies.\n2.  **Data Prep**: Convert audio to Mel Spectrograms.\n3.  **EDA**: Visualize data distribution.\n4.  **Training**: Fine-tune using LoRA.\n5.  **Evaluation**: Compare performance.\n","metadata":{}},{"cell_type":"markdown","source":"## 1. Environment Setup","metadata":{}},{"cell_type":"code","source":"\n!pip install -q torch torchaudio transformers librosa matplotlib pillow pandas scikit-learn accelerate peft bitsandbytes\n!pip install -q imageio-ffmpeg\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T14:55:59.720029Z","iopub.execute_input":"2026-02-16T14:55:59.720243Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"\nimport os\nimport sys\nimport glob\nimport json\nimport math\nimport argparse\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom tqdm.notebook import tqdm\nimport torch\nimport librosa\nfrom sklearn.model_selection import train_test_split\nfrom transformers import AutoProcessor, AutoModelForVision2Seq, TrainingArguments, Trainer, BitsAndBytesConfig\nfrom peft import LoraConfig, get_peft_model, TaskType\nfrom sklearn.metrics import roc_auc_score, f1_score, confusion_matrix, accuracy_score\n\nprint(f\"PyTorch Version: {torch.__version__}\")\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n## 2. Authentication\nYou need to authenticate with Hugging Face to access the gated MedGemma model.\n","metadata":{}},{"cell_type":"code","source":"\nfrom google.colab import userdata\nfrom huggingface_hub import login\n\ntry:\n    # Try to get token from Colab secrets\n    hf_token = userdata.get('HF_TOKEN')\n    login(token=hf_token)\nexcept:\n    # Interactive login\n    login()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n## 3. Data Preparation\nUpload your dataset. This script assumes:\n- `data/` directory exists.\n- Audio files and metadata JSONs are in `data/input`.\n- Or you can upload pre-processed images to `data/processed_images`.\n\nThe code below defines the preprocessing logic (Audio -> Mel Spectrogram).\n","metadata":{}},{"cell_type":"code","source":"\n# Audio to Mel Spectrogram Conversion Logic\ndef get_mel_spectrogram(file_path, target_sr=16000, n_mels=128, image_size=(224, 224)):\n    try:\n        # Load audio using librosa (returns numpy array)\n        y, sr = librosa.load(file_path, sr=target_sr)\n        \n        # Compute Mel Spectrogram\n        mel_spec = librosa.feature.melspectrogram(\n            y=y, \n            sr=sr, \n            n_mels=n_mels\n        )\n        \n        # Convert to dB\n        mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n        \n        # Normalize to 0-255\n        mel_spec_norm = (mel_spec_db - mel_spec_db.min()) / (mel_spec_db.max() - mel_spec_db.min()) * 255\n        mel_spec_norm = mel_spec_norm.astype(np.uint8)\n        \n        # Convert to PIL Image\n        img = Image.fromarray(mel_spec_norm, mode='L') # Grayscale\n        img = img.convert(\"RGB\") # Convert to RGB\n        \n        # Resize to target size\n        img = img.resize(image_size)\n        \n        return img\n    except Exception as e:\n        print(f\"Error processing {file_path}: {e}\")\n        return None\n\ndef preprocess_dataset(input_dir, output_dir):\n    os.makedirs(output_dir, exist_ok=True)\n    json_files = glob.glob(os.path.join(input_dir, '*.json'))\n    print(f\"Found {len(json_files)} files.\")\n    \n    processed_data = []\n    for json_file in tqdm(json_files):\n        file_id = os.path.splitext(os.path.basename(json_file))[0]\n        try:\n            with open(json_file, 'r') as f:\n                metadata = json.load(f)\n        except: continue\n            \n        status = metadata.get('status')\n        if not status: continue\n            \n        # Find audio\n        audio_path = None\n        for ext in ['.webm', '.ogg', '.wav']:\n            p = os.path.join(input_dir, file_id + ext)\n            if os.path.exists(p):\n                audio_path = p\n                break\n        \n        if not audio_path: continue\n        \n        # Convert\n        img = get_mel_spectrogram(audio_path)\n        if img:\n            out_path = os.path.join(output_dir, f\"{file_id}.png\")\n            img.save(out_path)\n            processed_data.append({\n                'image_path': out_path,\n                'label': status\n            })\n            \n    df = pd.DataFrame(processed_data)\n    csv_path = os.path.join(os.path.dirname(output_dir), 'processed_metadata.csv')\n    df.to_csv(csv_path, index=False)\n    print(f\"Saved metadata to {csv_path}\")\n    return csv_path\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Configuration for Data\ninput_data_dir = \"data/input\" # Update this path\noutput_image_dir = \"data/processed_images\"\n\n# Uncomment to run preprocessing if you have data uploaded\n# preprocess_dataset(input_data_dir, output_image_dir)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4. Create Splits","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport os\nimport argparse\n\ndef main():\n    # parser = argparse.ArgumentParser() (Replaced by SplitConfig)\n    # parser.add_argument(\"--metadata\", type=str, default=\"data/processed_metadata.csv\")\n    # parser.add_argument(\"--output_dir\", type=str, default=\"data\")\n    # parser.add_argument(\"--test_size\", type=float, default=0.2)\n    # parser.add_argument(\"--seed\", type=int, default=42)\n    args = SplitConfig()\n\n    if not os.path.exists(args.metadata):\n        print(f\"Metadata file not found: {args.metadata}\")\n        return\n\n    df = pd.read_csv(args.metadata)\n    print(f\"Total samples: {len(df)}\")\n    print(\"Label distribution:\")\n    print(df['label'].value_counts())\n\n    # Stratified split to maintain label distribution\n    train_df, test_df = train_test_split(\n        df, \n        test_size=args.test_size, \n        random_state=args.seed, \n        stratify=df['label']\n    )\n\n    print(f\"Train samples: {len(train_df)}\")\n    print(f\"Test samples: {len(test_df)}\")\n\n    train_path = os.path.join(args.output_dir, \"train_metadata.csv\")\n    test_path = os.path.join(args.output_dir, \"test_metadata.csv\")\n\n    train_df.to_csv(train_path, index=False)\n    test_df.to_csv(test_path, index=False)\n\n    print(f\"Saved splits to {train_path} and {test_path}\")\n\n# Run the main logic\nif True:\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nclass SplitConfig:\n    metadata = \"data/processed_metadata.csv\"\n    output_dir = \"data\"\n    test_size = 0.2\n    seed = 42\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 5. Exploratory Data Analysis (EDA)","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nimport argparse\nfrom PIL import Image\nimport math\n\ndef main():\n    # parser = argparse.ArgumentParser() (Replaced by EDAConfig)\n    # parser.add_argument(\"--metadata\", type=str, default=\"data/processed_metadata.csv\")\n    # parser.add_argument(\"--output_dir\", type=str, default=\"data/eda_results\")\n    args = EDAConfig()\n\n    if not os.path.exists(args.metadata):\n        print(f\"Metadata file not found: {args.metadata}\")\n        return\n\n    os.makedirs(args.output_dir, exist_ok=True)\n\n    df = pd.read_csv(args.metadata)\n    print(f\"Loaded {len(df)} records.\")\n\n    # 1. Label Distribution\n    plt.figure(figsize=(10, 6))\n    counts = df['label'].value_counts()\n    counts.plot(kind='bar', color=['skyblue', 'orange', 'red'])\n    plt.title('Label Distribution')\n    plt.xlabel('Label')\n    plt.ylabel('Count')\n    plt.xticks(rotation=45)\n    for i, v in enumerate(counts):\n        plt.text(i, v, str(v), ha='center', va='bottom')\n    \n    dist_plot_path = os.path.join(args.output_dir, \"label_distribution.png\")\n    plt.tight_layout()\n    plt.savefig(dist_plot_path)\n    print(f\"Saved distribution plot to {dist_plot_path}\")\n    plt.close()\n\n    # 2. Sample Images Visualization\n    unique_labels = df['label'].unique()\n    samples_per_class = 3\n    \n    # Calculate grid size\n    n_cols = samples_per_class\n    n_rows = len(unique_labels)\n    \n    fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols * 4, n_rows * 4))\n    if n_rows == 1: axes = [axes] # Handle single row case\n    \n    for i, label in enumerate(unique_labels):\n        subset = df[df['label'] == label]\n        # Sample with replacement if not enough data, though unlikely here\n        samples = subset.sample(min(samples_per_class, len(subset)), random_state=42)\n        \n        for j, (_, row) in enumerate(samples.iterrows()):\n            img_path = row['image_path']\n            ax = axes[i][j] if n_rows > 1 else axes[j]\n            \n            try:\n                img = Image.open(img_path)\n                ax.imshow(img)\n                ax.set_title(f\"{label}\\n{os.path.basename(img_path)}\")\n                ax.axis('off')\n            except Exception as e:\n                print(f\"Error loading {img_path}: {e}\")\n                ax.text(0.5, 0.5, \"Error loading image\", ha='center', va='center')\n                ax.axis('off')\n\n    plt.tight_layout()\n    samples_plot_path = os.path.join(args.output_dir, \"sample_spectrograms.png\")\n    plt.savefig(samples_plot_path)\n    print(f\"Saved sample images plot to {samples_plot_path}\")\n    plt.close()\n\n    print(\"\\n--- Summary ---\")\n    print(counts)\n\n# Run the main logic\nif True:\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nclass EDAConfig:\n    metadata = \"data/processed_metadata.csv\"\n    output_dir = \"data/eda_results\"\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 6. Fine-tuning MedGemma","metadata":{}},{"cell_type":"code","source":"\nclass TrainConfig:\n    data_dir = \"data\"\n    model_name = \"google/medgemma-1.5-4b-it\"\n    output_dir = \"output\"\n    epochs = 3\n    max_steps = 100 # Adjusted for Colab demo\n    batch_size = 2\n    bnb_4bit = True # Enable 4-bit for Colab T4\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport argparse\nimport pandas as pd\nfrom PIL import Image\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\ntry:\n    from transformers import AutoProcessor, Gemma3ForConditionalGeneration as AutoModelForVision2Seq, TrainingArguments, Trainer\n    try:\n        from transformers import BitsAndBytesConfig\n    except ImportError:\n        BitsAndBytesConfig = None\nexcept ImportError:\n    # If explicit import fails (unlikely given check), fallback to AutoModelForImageTextToText if possible or generic\n    from transformers import AutoProcessor, AutoModelForImageTextToText as AutoModelForVision2Seq, TrainingArguments, Trainer, BitsAndBytesConfig\nfrom peft import LoraConfig, get_peft_model, TaskType\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nfrom dotenv import load_dotenv\n\n# Load environment variables\nload_dotenv()\n\nclass CoughDataset(Dataset):\n    def __init__(self, metadata_file, image_dir, processor, prompt=\"<image>Classify this cough sound.\"):\n        self.df = pd.read_csv(metadata_file)\n        self.image_dir = image_dir\n        self.processor = processor\n        self.prompt = prompt\n        \n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        image_path = row['image_path']\n        label_text = str(row['label']) # Target text\n        \n        try:\n            image = Image.open(image_path).convert(\"RGB\")\n        except Exception as e:\n            print(f\"Error loading image {image_path}: {e}\")\n            image = Image.new('RGB', (224, 224)) # Dummy black image\n            \n        # Prepare inputs for VLM\n        # For PaliGemma, prompt should be \"detect cough\" or similar, or just a question.\n        # We process input (text + image) and target (label_text)\n        \n        inputs = self.processor(\n            text=self.prompt, \n            images=image, \n            return_tensors=\"pt\",\n            padding=\"max_length\", # Ensure consistent size if needed, usually collator handles it\n            truncation=True\n        )\n        \n        # Process target text for labels\n        # We need to tokenize the label text. \n        # Note: AutoProcessor for PaliGemma usually has a tokenizer.\n        # If the processor handles suffixes, we can use that, but often manual tokenization is safer for \"labels\"\n        \n        # Tokenize target\n        # For PaliGemma, the model expects standard causal LM labels\n        # But we need to make sure the PROMPT is not in the loss if we want purely response training, \n        # or standard CausalLM training on the whole sequence.\n        # Simpler approach: Prepend Image Tokens + Prompt, Append Label.\n        \n        # Using processor's suffix argument is cleaner if available, but let's manual it for generic VLM\n        # Actually, PaliGemma processor call: \n        # inputs = processor(text=prompts, images=images, suffix=labels, return_tensors=\"pt\", padding=True)\n        \n        # Let's try to tokenise label separately if suffix is not supported or to be safe.\n        # But wait, we can't create full inputs easily without processor.\n        # Let's assume standard processor usage for labels: similar to inputs but with target text.\n        \n        # A common pattern for VLM finetuning:\n        # Inputs: image + \"Question: ... Answer:\"\n        # Labels: \"target\"\n        \n        # Actually, let's use the processor to tokenize the label text (as a target)\n        # Note: variable length labels might need padding in collator.\n        \n        # Let's use a simpler strategy:\n        # Tokenize prompt + label. \n        # However, dealing with image tokens in standard tokenizer is tricky.\n        \n        # Strategy: Use `suffix` if using PaliGemmaProcessor.\n        if \"PaliGemma\" in type(self.processor).__name__ or True: # Assume VLM processor handles it\n             # Attempt to use 'suffix' for labels if supported, otherwise just return text and let collation handle it?\n             # No, dataset must return tensors.\n             \n             # Re-do with suffix\n             # We need to do this carefully.\n             pass\n\n        # Optimized VLM Dataset Return:\n        # We return the raw text and image, and let a custom data collator handle the tokenization?\n        # No, slow.\n        \n        # Let's try using the processor to generate labels.\n        # labels = self.processor.tokenizer(text=label_text, return_tensors=\"pt\").input_ids.squeeze(0)\n        \n        # Return dict\n        input_ids = inputs.input_ids.squeeze(0)\n        attention_mask = inputs.attention_mask.squeeze(0)\n        pixel_values = inputs.pixel_values.squeeze(0)\n        \n        # Create labels: same as input_ids but mask out the prompt? \n        # VLM fine-tuning typically provides the full sequence (Prompt + Answer) and masks the prompt in labels.\n        # But Processor usually creates [PAD] [IMG] ... [TXT].\n        \n        # Let's use a simplified approach: just generate the full sequence input_ids for (Prompt + Label) \n        # and create labels from it.\n        # But handling image token insertion is processor's job.\n        \n        # Better approach: \n        # use processor(text=prompt, images=image, suffix=label_text, ...) if supported.\n        # If not, use processor(text=prompt + \" \" + label_text, images=image)\n        \n        full_text = f\"{self.prompt} {label_text}\"\n        inputs_full = self.processor(text=full_text, images=image, return_tensors=\"pt\", padding=\"max_length\", max_length=512, truncation=True)\n        \n        input_ids = inputs_full.input_ids.squeeze(0)\n        attention_mask = inputs_full.attention_mask.squeeze(0)\n        pixel_values = inputs_full.pixel_values.squeeze(0)\n        # Extract token_type_ids if available (Gemma3 needs it)\n        token_type_ids = inputs_full.get(\"token_type_ids\")\n        if token_type_ids is not None:\n             token_type_ids = token_type_ids.squeeze(0)\n        \n        # Labels: copy input_ids\n        labels = input_ids.clone()\n        # Mask padding in labels\n        labels[attention_mask == 0] = -100\n        \n        item = {\n            \"input_ids\": input_ids,\n            \"attention_mask\": attention_mask,\n            \"pixel_values\": pixel_values,\n            \"labels\": labels\n        }\n        \n        if token_type_ids is not None:\n            item[\"token_type_ids\"] = token_type_ids\n            \n        return item\n\ndef main():\n    # parser = argparse.ArgumentParser() (Replaced by TrainConfig)\n    # parser.add_argument(\"--data_dir\", type=str, default=\"data\")\n    # parser.add_argument(\"--model_name\", type=str, default=\"google/medgemma-1.5-4b-it\") # Or google/paligemma-3b-pt-224\n    # parser.add_argument(\"--output_dir\", type=str, default=\"output\")\n    # parser.add_argument(\"--epochs\", type=int, default=3)\n    # parser.add_argument(\"--max_steps\", type=int, default=-1, help=\"If > 0: set total number of training steps to perform. Overrides epochs.\")\n    # parser.add_argument(\"--batch_size\", type=int, default=2) # 4b model might need small batch size\n    # parser.add_argument(\"--bnb_4bit\", action=\"store_true\", help=\"Use 4-bit quantization\")\n    args = TrainConfig()\n    \n    # Check for hardware\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    print(f\"Using device: {device}\")\n    \n    # Prepare data paths\n    metadata_path = os.path.join(args.data_dir, \"train_metadata.csv\")\n    image_dir = os.path.join(args.data_dir, \"processed_images\")\n    \n    if not os.path.exists(metadata_path):\n        print(\"Processed data not found. Please run preprocess_audio.py first.\")\n        return\n\n    # Load Processor\n    try:\n        processor = AutoProcessor.from_pretrained(args.model_name)\n    except Exception as e:\n        print(f\"Error loading processor: {e}\")\n        return\n\n    dataset = CoughDataset(metadata_path, image_dir, processor=processor, prompt=f\"{processor.boi_token}Classify this cough sound as healthy or COVID-19.\")\n    \n    # Split dataset\n    train_size = int(0.9 * len(dataset))\n    val_size = len(dataset) - train_size\n    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n    \n    print(f\"Train size: {len(train_dataset)}, Val size: {len(val_dataset)}\")\n    \n    # Load Model\n    # Quantization config\n    bnb_config = None\n    if args.bnb_4bit:\n        bnb_config = BitsAndBytesConfig(\n            load_in_4bit=True,\n            bnb_4bit_quant_type=\"nf4\",\n            bnb_4bit_compute_dtype=torch.float16\n        )\n\n    model = AutoModelForVision2Seq.from_pretrained(\n        args.model_name,\n        device_map=\"auto\" if device == \"cuda\" else None, # Disable device_map on CPU to avoid meta device issues\n        quantization_config=bnb_config,\n        torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n        trust_remote_code=True\n    )\n    \n    # Apply LoRA\n    # Target modules for LoRA in PaliGemma/Gemma usually: q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj\n    peft_config = LoraConfig(\n        # task_type=TaskType.CAUSAL_LM, # Removing task_type to avoid prepare_inputs_for_generation check on base model\n        inference_mode=False, \n        r=16, \n        lora_alpha=32, \n        lora_dropout=0.05,\n        target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]\n    )\n    model = get_peft_model(model, peft_config)\n    model.print_trainable_parameters()\n    \n    # Training Arguments\n    training_args = TrainingArguments(\n        output_dir=args.output_dir,\n        per_device_train_batch_size=args.batch_size,\n        gradient_accumulation_steps=4, # To simulate larger batch\n        num_train_epochs=args.epochs,\n        max_steps=args.max_steps,\n        learning_rate=2e-4,\n        logging_steps=10,\n        save_strategy=\"epoch\",\n        eval_strategy=\"epoch\",\n        fp16=(device == \"cuda\"),\n        remove_unused_columns=False,\n        save_total_limit=2\n    )\n    \n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=val_dataset,\n        # Default collator usually works for dict of tensors, but checking if padding needed\n        # We handled padding in dataset to max_length, so robust.\n    )\n    \n    print(\"Starting training...\")\n    trainer.train()\n    \n    print(\"Training complete. Saving model...\")\n    model.save_pretrained(os.path.join(args.output_dir, \"final_model\"))\n    processor.save_pretrained(os.path.join(args.output_dir, \"final_model\"))\n\n# Run the main logic\nif True:\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 7. Evaluation","metadata":{}},{"cell_type":"code","source":"\nclass EvalConfig:\n    model_path = \"output/final_model\"\n    test_data = \"data/test_metadata.csv\"\n    output_file = \"evaluation_results.json\"\n    bnb_4bit = True\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom transformers import AutoProcessor\nfrom PIL import Image\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import roc_auc_score, f1_score, confusion_matrix, classification_report, accuracy_score\nimport argparse\nimport os\nfrom tqdm import tqdm\nfrom dotenv import load_dotenv\n\n# Load environment variables\nload_dotenv()\n\n# Fallback import for older transformers versions\n# Fallback import for older transformers versions\ntry:\n    from transformers import AutoProcessor, Gemma3ForConditionalGeneration as AutoModelForVision2Seq\nexcept ImportError:\n    from transformers import AutoProcessor, AutoModelForImageTextToText as AutoModelForVision2Seq\n\ndef evaluate(model, processor, test_loader, device):\n    model.eval()\n    all_labels = []\n    all_preds = []\n    all_probs = []\n    \n    # Target class for binary classification logic (adjust as needed)\n    # Dataset has: healthy, symptomatic, COVID-19\n    # We essentially want to detect COVID-19 vs Rest (or Symptomatic vs Healthy?)\n    # User objective implies COVID detection.\n    # Let's treat \"COVID-19\" as Positive (1) and others as Negative (0) for AUC-ROC\n    \n    target_class = \"COVID-19\"\n    \n    print(f\"Evaluating for Target Class: {target_class}\")\n    \n    with torch.no_grad():\n        for batch in tqdm(test_loader, desc=\"Evaluating\"):\n            # Batch preparation\n            # Note: Custom loop might be needed if not using Trainer's prediction loop\n            pass\n\n    # Since VLM generation is slow and complex to batched score without a specific head,\n    # we will use a generation-based approach for the \"answer\" and map it to labels.\n    # OR we can check the probability of the string \"COVID-19\" vs \"healthy\".\n    \n    # Efficient approach for 4B model:\n    # 1. Generate text.\n    # 2. Parse text (\"detected covid\", \"healthy\", etc.)\n    # 3. For AUC-ROC, we need probabilities. \n    #    We can look at the logits of the first token of the answer \"COVID-19\" vs \"healthy\".\n    \n    return {}\n\ndef main():\n    # parser = argparse.ArgumentParser() (Replaced by EvalConfig)\n    # parser.add_argument(\"--model_path\", type=str, required=True, help=\"Path to model or huggingface repo\")\n    # parser.add_argument(\"--test_data\", type=str, default=\"data/test_metadata.csv\")\n    # parser.add_argument(\"--output_file\", type=str, default=\"evaluation_results.json\")\n    # parser.add_argument(\"--bnb_4bit\", action=\"store_true\", help=\"Use 4-bit quantization\")\n    args = EvalConfig()\n    \n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    print(f\"Using device: {device}\")\n    \n    # Load metadata\n    df = pd.read_csv(args.test_data)\n    print(f\"Loaded {len(df)} test samples.\")\n    \n    # Load Model & Processor\n    print(f\"Loading model from {args.model_path}...\")\n    try:\n        processor = AutoProcessor.from_pretrained(args.model_path, trust_remote_code=True)\n        # Handle quantization if requested (crucial for 4B on consumer GPU)\n        quantization_config = None\n        if args.bnb_4bit:\n            from transformers import BitsAndBytesConfig\n            quantization_config = BitsAndBytesConfig(\n                load_in_4bit=True,\n                bnb_4bit_quant_type=\"nf4\",\n                bnb_4bit_compute_dtype=torch.float16\n            )\n        \n        model = AutoModelForVision2Seq.from_pretrained(\n            args.model_path,\n            device_map=\"auto\",\n            quantization_config=quantization_config,\n            torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n            trust_remote_code=True\n        )\n    except Exception as e:\n        print(f\"Error loading model: {e}\")\n        return\n\n    # Evaluation Loop\n    predictions = []\n    ground_truth = []\n    # For AUC, we need scores. \n    # Strategy: Compute perplexity or logits for \"COVID-19\" vs \"healthy\".\n    # Simplified: Generate response and check containment. (Gives binary output, AUC not ideal but possible if binary)\n    # Better: Get probability of token \"COVID\" vs \"healthy\".\n    \n    # Let's stick to generation for F1/Accuracy/Sensitivity/Specificity first as it's robust.\n    # AUC needs scores. If we can't get scores easily from VLM API, we might skip or approximate.\n    # Approximation: 1.0 if correct class generated, 0.0 otherwise? No, that's just accuracy.\n    \n    # Let's perform generation.\n    \n    print(\"Starting generation...\")\n    # Iterate sample by sample (slow but safe for VLM memory)\n    # DEBUG: Limit to 5 samples to test speed/correctness\n    df_subset = df.head(5)\n    \n    for _, row in tqdm(df_subset.iterrows(), total=len(df_subset)):\n        image_path = row['image_path']\n        true_label = row['label'] # \"COVID-19\", \"healthy\", \"symptomatic\"\n        \n        # Determine binary ground truth for COVID detection\n        is_covid = 1 if true_label == \"COVID-19\" else 0\n        ground_truth.append(is_covid)\n        \n        try:\n            image = Image.open(image_path).convert(\"RGB\")\n            # Use processor.boi_token if available (Gemma3), otherwise <image>\n            boi = getattr(processor, \"boi_token\", \"<image>\")\n            prompt = f\"{boi}Classify this cough sound as healthy or COVID-19.\"\n            inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(model.device)\n            \n            # Generate\n            with torch.no_grad():\n                outputs = model.generate(**inputs, max_new_tokens=20)\n                \n            decoded = processor.batch_decode(outputs, skip_special_tokens=True)[0]\n            # Output will contain the prompt too usually? \n            # MedGemma might handle it differently.\n            # Parse output.\n            \n            # Heuristic parsing\n            response = decoded.lower()\n            if \"covid-19\" in response or \"positive\" in response:\n                pred_score = 1 # Predicted COVID\n            elif \"healthy\" in response or \"negative\" in response:\n                pred_score = 0\n            else:\n                pred_score = 0 # Default to negative if unclear? Or fail?\n            \n            predictions.append(pred_score)\n            \n        except Exception as e:\n            print(f\"Error processing {image_path}: {e}\")\n            predictions.append(0) # Fallback\n\n    # Metrics\n    print(\"\\n--- Evaluation Report ---\")\n    \n    # Binary Classification Metrics\n    y_true = np.array(ground_truth)\n    y_pred = np.array(predictions)\n    \n    acc = accuracy_score(y_true, y_pred)\n    f1 = f1_score(y_true, y_pred, zero_division=0)\n    \n    # Confusion Matrix\n    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n    \n    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n    \n    # AUC-ROC\n    # Note: Using binary predictions (0/1) for AUC is suboptimal (gives trapezoidal approx). \n    # But sufficient for high-level \"quantitative\" comparison if scores are unavailable.\n    roc_auc = roc_auc_score(y_true, y_pred)\n    \n    print(f\"Accuracy: {acc:.4f}\")\n    print(f\"F1-Score: {f1:.4f}\")\n    print(f\"Sensitivity (Recall): {sensitivity:.4f}\")\n    print(f\"Specificity: {specificity:.4f}\")\n    print(f\"AUC-ROC: {roc_auc:.4f}\")\n    \n    report = {\n        \"accuracy\": acc,\n        \"f1_score\": f1,\n        \"sensitivity\": sensitivity,\n        \"specificity\": specificity,\n        \"auc_roc\": roc_auc,\n        \"confusion_matrix\": {\"tn\": int(tn), \"fp\": int(fp), \"fn\": int(fn), \"tp\": int(tp)}\n    }\n    \n    # Save results\n    import json\n    with open(args.output_file, 'w') as f:\n        json.dump(report, f, indent=4)\n    print(f\"Results saved to {args.output_file}\")\n\n# Run the main logic\nif True:\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 8. Results Comparison","metadata":{}},{"cell_type":"code","source":"import json\nimport pandas as pd\nimport argparse\nimport os\n\ndef load_results(filepath):\n    if not os.path.exists(filepath):\n        print(f\"File not found: {filepath}\")\n        return None\n    with open(filepath, 'r') as f:\n        return json.load(f)\n\ndef main():\n    # parser = argparse.ArgumentParser() (Replaced by CompareConfig)\n    # parser.add_argument(\"--baseline\", type=str, default=\"baseline_results.json\")\n    # parser.add_argument(\"--finetuned\", type=str, default=\"finetuned_results.json\")\n    # parser.add_argument(\"--output_md\", type=str, default=\"comparison_report.md\")\n    args = CompareConfig()\n\n    baseline = load_results(args.baseline)\n    finetuned = load_results(args.finetuned)\n\n    if not baseline:\n        print(\"Baseline results missing.\")\n        return\n    \n    if not finetuned:\n         print(\"Finetuned results missing (training might be incomplete).\")\n         return\n\n    # Metrics to compare\n    metrics = [\"accuracy\", \"f1_score\", \"sensitivity\", \"specificity\", \"auc_roc\"]\n    \n    data = []\n    for m in metrics:\n        b_val = baseline.get(m, 0)\n        f_val = finetuned.get(m, 0)\n        diff = f_val - b_val\n        data.append({\n            \"Metric\": m, \n            \"Baseline\": b_val, \n            \"Finetuned\": f_val, \n            \"Difference\": diff\n        })\n        \n    df = pd.DataFrame(data)\n    \n    # Generate Markdown Report\n    md = \"# Model Comparison Report\\n\\n\"\n    md += \"## Metrics Comparison\\n\\n\"\n    md += \"| Metric | Baseline | Fine-tuned | Difference |\\n\"\n    md += \"| :--- | :--- | :--- | :--- |\\n\"\n    \n    for _, row in df.iterrows():\n        diff_str = f\"{row['Difference']:.4f}\"\n        if row['Difference'] > 0:\n            diff_str = f\"+{diff_str}\"\n        \n        md += f\"| **{row['Metric']}** | {row['Baseline']:.4f} | {row['Finetuned']:.4f} | {diff_str} |\\n\"\n        \n    md += \"\\n## Confusion Matrix Comparison\\n\\n\"\n    \n    def format_cm(cm):\n        return (f\"TN: {cm.get('tn',0)}, FP: {cm.get('fp',0)}, \"\n                f\"FN: {cm.get('fn',0)}, TP: {cm.get('tp',0)}\")\n\n    md += f\"- **Baseline**: {format_cm(baseline.get('confusion_matrix', {}))}\\n\"\n    md += f\"- **Fine-tuned**: {format_cm(finetuned.get('confusion_matrix', {}))}\\n\"\n    \n    md += \"\\n## Conclusion\\n\\n\"\n    if finetuned.get('sensitivity', 0) > baseline.get('sensitivity', 0):\n        md += \"The fine-tuned model shows improvement in Sensitivity, indicating better detection of COVID-19 cases.\\n\"\n    else:\n        md += \"The fine-tuned model did not improve Sensitivity. Further tuning or more data might be needed.\\n\"\n\n    with open(args.output_md, 'w') as f:\n        f.write(md)\n        \n    print(f\"Comparison report saved to {args.output_md}\")\n    print(df)\n\n# Run the main logic\nif True:\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nclass CompareConfig:\n    baseline = \"baseline_results.json\" \n    finetuned = \"evaluation_results.json\"\n    output_md = \"comparison_report.md\"\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}