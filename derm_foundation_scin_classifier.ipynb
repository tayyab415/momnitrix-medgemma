{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Experiment: Derm Foundation SCIN Classifier (Implementation Plan Version)\n",
        "\n",
        "Objective:\n",
        "- Train a deployable multi-label skin-condition classifier on top of Derm Foundation embeddings.\n",
        "- Export artifacts required by the Momnetrix/MamaGuard pipeline.\n",
        "\n",
        "What this notebook does:\n",
        "- Loads SCIN metadata (labels + image paths).\n",
        "- Loads precomputed Derm Foundation embeddings (`scin_dataset_precomputed_embeddings.npz`).\n",
        "- Trains a 10-condition multi-label sklearn classifier.\n",
        "- Evaluates with hamming loss + per-label ROC-AUC.\n",
        "- Exports deployment artifacts: `derm_classifier.pkl`, `derm_labels.json`, `derm_config.json`, `sample_prediction.json`.\n",
        "\n",
        "Deployment context:\n",
        "- Runtime endpoint receives a photo.\n",
        "- Derm Foundation (TensorFlow/Keras) creates a 6144-d embedding.\n",
        "- This classifier maps embedding -> `{condition: probability}`.\n",
        "- Gemini then applies pregnancy-specific context and escalation guidance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import ast\n",
        "import json\n",
        "import pickle\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "from typing import Any\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import hamming_loss, roc_auc_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "SEED = 42\n",
        "\n",
        "# Top-10 conditions from the implementation plan / audited notebook.\n",
        "CONDITIONS_TO_PREDICT = [\n",
        "    \"Eczema\",\n",
        "    \"Allergic Contact Dermatitis\",\n",
        "    \"Insect Bite\",\n",
        "    \"Urticaria\",\n",
        "    \"Psoriasis\",\n",
        "    \"Folliculitis\",\n",
        "    \"Irritant Contact Dermatitis\",\n",
        "    \"Tinea\",\n",
        "    \"Herpes Zoster\",\n",
        "    \"Drug Rash\",\n",
        "]\n",
        "\n",
        "# Paths can be changed for local vs Colab usage.\n",
        "WORK_DIR = Path(\".\")\n",
        "ARTIFACT_DIR = WORK_DIR / \"artifacts\" / \"derm\"\n",
        "CACHE_DIR = WORK_DIR / \"data\" / \"scin_cache\"\n",
        "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
        "ARTIFACT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print({\n",
        "    \"seed\": SEED,\n",
        "    \"artifact_dir\": str(ARTIFACT_DIR.resolve()),\n",
        "    \"cache_dir\": str(CACHE_DIR.resolve()),\n",
        "    \"num_conditions\": len(CONDITIONS_TO_PREDICT),\n",
        "})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Loading Strategy (Local or Colab)\n",
        "\n",
        "- This notebook avoids `google.colab.*` imports so it runs locally and in Colab.\n",
        "- We use:\n",
        "  - `huggingface_hub` for precomputed embeddings (`google/derm-foundation` repo).\n",
        "  - public SCIN CSVs (`scin_cases.csv`, `scin_labels.csv`) via direct URL.\n",
        "- If direct URL fetching fails in your environment, download the 2 CSVs manually and point paths in the next cell.\n",
        "\n",
        "Expected columns used:\n",
        "- `case_id`\n",
        "- `image_1_path`, `image_2_path`, `image_3_path`\n",
        "- `dermatologist_gradable_for_skin_condition_1`\n",
        "- `dermatologist_skin_condition_on_label_name`\n",
        "- `dermatologist_skin_condition_confidence`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ScinSources:\n",
        "    cases_csv_url: str = \"https://storage.googleapis.com/dx-scin-public-data/dataset/scin_cases.csv\"\n",
        "    labels_csv_url: str = \"https://storage.googleapis.com/dx-scin-public-data/dataset/scin_labels.csv\"\n",
        "    hf_repo_id: str = \"google/derm-foundation\"\n",
        "    hf_embeddings_filename: str = \"scin_dataset_precomputed_embeddings.npz\"\n",
        "\n",
        "\n",
        "SOURCES = ScinSources()\n",
        "\n",
        "\n",
        "def _parse_listlike(value: Any) -> list[Any]:\n",
        "    if pd.isna(value):\n",
        "        return []\n",
        "    if isinstance(value, list):\n",
        "        return value\n",
        "    if isinstance(value, str):\n",
        "        return list(ast.literal_eval(value))\n",
        "    return []\n",
        "\n",
        "\n",
        "def load_scin_dataframe(sources: ScinSources) -> pd.DataFrame:\n",
        "    cases_df = pd.read_csv(sources.cases_csv_url, dtype={\"case_id\": str})\n",
        "    labels_df = pd.read_csv(sources.labels_csv_url, dtype={\"case_id\": str})\n",
        "    merged = pd.merge(cases_df, labels_df, on=\"case_id\")\n",
        "    merged[\"case_id\"] = merged[\"case_id\"].astype(str)\n",
        "    merged = merged.set_index(\"case_id\")\n",
        "    return merged\n",
        "\n",
        "\n",
        "def load_embeddings(sources: ScinSources, cache_dir: Path) -> dict[str, np.ndarray]:\n",
        "    file_path = hf_hub_download(\n",
        "        repo_id=sources.hf_repo_id,\n",
        "        filename=sources.hf_embeddings_filename,\n",
        "        local_dir=str(cache_dir),\n",
        "    )\n",
        "    data = np.load(file_path, allow_pickle=True)\n",
        "    return {k: v for k, v in data.items()}\n",
        "\n",
        "\n",
        "def prepare_xy(\n",
        "    scin_df: pd.DataFrame,\n",
        "    embeddings: dict[str, np.ndarray],\n",
        "    conditions_to_predict: list[str],\n",
        "    min_confidence: int = 0,\n",
        ") -> tuple[np.ndarray, np.ndarray, MultiLabelBinarizer, dict[str, int]]:\n",
        "    X: list[np.ndarray] = []\n",
        "    y_labels: list[list[str]] = []\n",
        "\n",
        "    stats = {\n",
        "        \"rows_total\": 0,\n",
        "        \"rows_poor_quality\": 0,\n",
        "        \"missing_embedding\": 0,\n",
        "        \"label_not_tracked\": 0,\n",
        "        \"label_low_confidence\": 0,\n",
        "    }\n",
        "\n",
        "    for row in scin_df.itertuples():\n",
        "        stats[\"rows_total\"] += 1\n",
        "\n",
        "        if row.dermatologist_gradable_for_skin_condition_1 != \"DEFAULT_YES_IMAGE_QUALITY_SUFFICIENT\":\n",
        "            stats[\"rows_poor_quality\"] += 1\n",
        "            continue\n",
        "\n",
        "        labels = _parse_listlike(row.dermatologist_skin_condition_on_label_name)\n",
        "        confidences = _parse_listlike(row.dermatologist_skin_condition_confidence)\n",
        "\n",
        "        row_labels: list[str] = []\n",
        "        for label, conf in zip(labels, confidences):\n",
        "            if label not in conditions_to_predict:\n",
        "                stats[\"label_not_tracked\"] += 1\n",
        "                continue\n",
        "            if conf < min_confidence:\n",
        "                stats[\"label_low_confidence\"] += 1\n",
        "                continue\n",
        "            row_labels.append(label)\n",
        "\n",
        "        image_paths = [row.image_1_path, row.image_2_path, row.image_3_path]\n",
        "        for image_path in image_paths:\n",
        "            if pd.isna(image_path):\n",
        "                continue\n",
        "            if image_path not in embeddings:\n",
        "                stats[\"missing_embedding\"] += 1\n",
        "                continue\n",
        "            X.append(embeddings[image_path])\n",
        "            y_labels.append(row_labels)\n",
        "\n",
        "    mlb = MultiLabelBinarizer(classes=conditions_to_predict)\n",
        "    y = mlb.fit_transform(y_labels)\n",
        "    X_np = np.asarray(X)\n",
        "\n",
        "    return X_np, y, mlb, stats\n",
        "\n",
        "\n",
        "scin_df = load_scin_dataframe(SOURCES)\n",
        "embeddings = load_embeddings(SOURCES, CACHE_DIR)\n",
        "X, y, mlb, prep_stats = prepare_xy(\n",
        "    scin_df=scin_df,\n",
        "    embeddings=embeddings,\n",
        "    conditions_to_predict=CONDITIONS_TO_PREDICT,\n",
        "    min_confidence=0,\n",
        ")\n",
        "\n",
        "print(\"Dataset prep stats:\", prep_stats)\n",
        "print(\"X shape:\", X.shape, \"y shape:\", y.shape)\n",
        "print(\"Class order:\", list(mlb.classes_))\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X,\n",
        "    y,\n",
        "    test_size=0.2,\n",
        "    random_state=SEED,\n",
        ")\n",
        "\n",
        "classifier = MultiOutputClassifier(\n",
        "    LogisticRegression(max_iter=300, random_state=SEED)\n",
        ")\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "# Convert sklearn's list-of-arrays into matrix [n_samples, n_labels]\n",
        "proba_cols = [classifier.estimators_[i].predict_proba(X_test)[:, 1] for i in range(len(mlb.classes_))]\n",
        "y_pred_proba = np.column_stack(proba_cols)\n",
        "y_pred_binary = (y_pred_proba >= 0.5).astype(int)\n",
        "\n",
        "print(\"Hamming loss:\", round(float(hamming_loss(y_test, y_pred_binary)), 4))\n",
        "\n",
        "auc_by_label: dict[str, float] = {}\n",
        "for idx, label in enumerate(mlb.classes_):\n",
        "    if len(np.unique(y_test[:, idx])) < 2:\n",
        "        auc_by_label[label] = float(\"nan\")\n",
        "        continue\n",
        "    auc_by_label[label] = float(roc_auc_score(y_test[:, idx], y_pred_proba[:, idx]))\n",
        "\n",
        "print(\"ROC-AUC by label:\")\n",
        "for label in mlb.classes_:\n",
        "    print(f\"  {label}: {auc_by_label[label]:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Export + Inference Contract\n",
        "\n",
        "This is the deployment-facing output section.\n",
        "\n",
        "Artifacts produced:\n",
        "- `derm_classifier.pkl`: trained sklearn classifier head.\n",
        "- `derm_labels.json`: exact label order expected by classifier output.\n",
        "- `derm_config.json`: thresholds and metadata.\n",
        "- `sample_prediction.json`: reference API-shaped output for integration tests.\n",
        "\n",
        "Runtime contract (for your Modal endpoint):\n",
        "1. Endpoint receives image bytes.\n",
        "2. Derm Foundation model converts image -> 6144-d embedding.\n",
        "3. Load `derm_classifier.pkl` and `derm_labels.json` at startup.\n",
        "4. Return sorted `{condition: probability}` and summary fields (`top_k`, `max_confidence`, `low_confidence`).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1) Save classifier\n",
        "classifier_path = ARTIFACT_DIR / \"derm_classifier.pkl\"\n",
        "with classifier_path.open(\"wb\") as f:\n",
        "    pickle.dump(classifier, f)\n",
        "\n",
        "# 2) Save label order (critical for mapping index -> condition)\n",
        "labels_path = ARTIFACT_DIR / \"derm_labels.json\"\n",
        "labels_payload = {\"labels\": list(mlb.classes_)}\n",
        "labels_path.write_text(json.dumps(labels_payload, indent=2), encoding=\"utf-8\")\n",
        "\n",
        "# 3) Save config\n",
        "config = {\n",
        "    \"model_name\": \"google/derm-foundation\",\n",
        "    \"classifier_type\": \"sklearn.multioutput.MultiOutputClassifier(LogisticRegression)\",\n",
        "    \"embedding_dim\": int(X.shape[1]),\n",
        "    \"threshold\": 0.5,\n",
        "    \"top_k\": 5,\n",
        "    \"seed\": SEED,\n",
        "}\n",
        "config_path = ARTIFACT_DIR / \"derm_config.json\"\n",
        "config_path.write_text(json.dumps(config, indent=2), encoding=\"utf-8\")\n",
        "\n",
        "\n",
        "def predict_embedding_proba(\n",
        "    embedding: np.ndarray,\n",
        "    clf: MultiOutputClassifier,\n",
        "    labels: list[str],\n",
        "    threshold: float = 0.5,\n",
        "    top_k: int = 5,\n",
        ") -> dict[str, Any]:\n",
        "    if embedding.ndim == 1:\n",
        "        embedding = np.expand_dims(embedding, axis=0)\n",
        "\n",
        "    probs = [clf.estimators_[i].predict_proba(embedding)[:, 1][0] for i in range(len(labels))]\n",
        "    scores = {label: float(prob) for label, prob in zip(labels, probs)}\n",
        "    sorted_scores = dict(sorted(scores.items(), key=lambda kv: kv[1], reverse=True))\n",
        "\n",
        "    top = list(sorted_scores.items())[:top_k]\n",
        "    max_confidence = top[0][1] if top else 0.0\n",
        "\n",
        "    return {\n",
        "        \"scores\": sorted_scores,\n",
        "        \"top_k\": [{\"condition\": k, \"score\": round(v, 4)} for k, v in top],\n",
        "        \"max_confidence\": round(float(max_confidence), 4),\n",
        "        \"low_confidence\": bool(max_confidence < threshold),\n",
        "        \"threshold\": threshold,\n",
        "    }\n",
        "\n",
        "\n",
        "# 4) Build one sample prediction payload for integration tests\n",
        "sample_payload = predict_embedding_proba(\n",
        "    embedding=X_test[0],\n",
        "    clf=classifier,\n",
        "    labels=list(mlb.classes_),\n",
        "    threshold=config[\"threshold\"],\n",
        "    top_k=config[\"top_k\"],\n",
        ")\n",
        "\n",
        "sample_prediction = {\n",
        "    \"model\": \"derm-foundation-classifier-v1\",\n",
        "    \"case\": \"scin_test_sample_0\",\n",
        "    \"prediction\": sample_payload,\n",
        "}\n",
        "sample_path = ARTIFACT_DIR / \"sample_prediction.json\"\n",
        "sample_path.write_text(json.dumps(sample_prediction, indent=2), encoding=\"utf-8\")\n",
        "\n",
        "print(\"Saved artifacts:\")\n",
        "print(\"-\", classifier_path)\n",
        "print(\"-\", labels_path)\n",
        "print(\"-\", config_path)\n",
        "print(\"-\", sample_path)\n",
        "print(\"\\nSample top-k:\", sample_prediction[\"prediction\"][\"top_k\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next Steps (Handoff)\n",
        "\n",
        "1. Run this notebook fully and confirm artifacts exist in `artifacts/derm/`.\n",
        "2. In Modal endpoint code, load `derm_classifier.pkl` and `derm_labels.json` at startup.\n",
        "3. Ensure Derm Foundation inference returns a `(6144,)` embedding for every image.\n",
        "4. Feed embedding into classifier and return the `sample_prediction.json` schema.\n",
        "5. Pass prediction + gestational age + vitals + meds + original photo to Gemini for pregnancy-contextual guidance.\n",
        "6. Persist final output in Visit Prep Summary.\n",
        "\n",
        "Notes:\n",
        "- This classifier is a screening head, not a diagnosis engine.\n",
        "- Keep label order fixed; changing order without retraining breaks outputs.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
